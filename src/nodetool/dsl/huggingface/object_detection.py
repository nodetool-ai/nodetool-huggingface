# This file is auto-generated by nodetool.dsl.codegen.
# Please do not edit this file manually.

# Instead, edit the node class in the source module and run the following commands to regenerate the DSL:
# nodetool package scan
# nodetool codegen

from pydantic import BaseModel, Field
import typing
from typing import Any
import nodetool.metadata.types
import nodetool.metadata.types as types
from nodetool.dsl.graph import GraphNode, SingleOutputGraphNode

import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.huggingface.object_detection
from nodetool.workflows.base_node import BaseNode


class ObjectDetection(
    SingleOutputGraphNode[list[types.ObjectDetectionResult]],
    GraphNode[list[types.ObjectDetectionResult]],
):
    """

    Detects and localizes objects in images with bounding boxes and confidence scores.
    image, object-detection, bounding-boxes, huggingface, computer-vision

    Use cases:
    - Count and identify objects in photographs and videos
    - Locate specific items in complex scenes for robotics
    - Analyze security camera footage for monitoring systems
    - Detect tables and structures in documents
    - Build automated inventory and inspection systems
    """

    model: types.HFObjectDetection | OutputHandle[types.HFObjectDetection] = (
        connect_field(
            default=types.HFObjectDetection(
                type="hf.object_detection",
                repo_id="facebook/detr-resnet-50",
                path=None,
                variant=None,
                allow_patterns=None,
                ignore_patterns=None,
            ),
            description="The object detection model. DETR models offer high accuracy; YOLOS variants are faster. Specialized models exist for tables and fashion items.",
        )
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The image to detect objects in. Supports common formats like JPEG, PNG.",
    )
    threshold: float | OutputHandle[float] = connect_field(
        default=0.9,
        description="Minimum confidence score (0-1) for detected objects. Higher values return fewer but more certain detections.",
    )
    top_k: int | OutputHandle[int] = connect_field(
        default=5,
        description="Maximum number of detected objects to return, sorted by confidence.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.huggingface.object_detection.ObjectDetection

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.huggingface.object_detection
from nodetool.workflows.base_node import BaseNode


class VisualizeObjectDetection(
    SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]
):
    """

    Renders object detection results as labeled bounding boxes overlaid on the original image.
    image, object-detection, bounding-boxes, visualization, annotation

    Use cases:
    - Visualize and verify object detection model outputs
    - Create annotated images for documentation and presentations
    - Debug and analyze detection accuracy and coverage
    - Generate labeled images for training data review
    """

    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The original image to draw detection boxes on.",
    )
    objects: (
        list[types.ObjectDetectionResult]
        | OutputHandle[list[types.ObjectDetectionResult]]
    ) = connect_field(
        default={},
        description="List of detected objects from ObjectDetection or ZeroShotObjectDetection nodes.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.huggingface.object_detection.VisualizeObjectDetection

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.huggingface.object_detection
from nodetool.workflows.base_node import BaseNode


class ZeroShotObjectDetection(
    SingleOutputGraphNode[list[types.ObjectDetectionResult]],
    GraphNode[list[types.ObjectDetectionResult]],
):
    """

    Detects objects in images using custom labels without requiring task-specific training.
    image, object-detection, bounding-boxes, zero-shot, flexible

    Use cases:
    - Detect custom objects without training a specialized model
    - Search for specific items described in natural language
    - Build flexible object detection systems with dynamic categories
    - Prototype detection applications with arbitrary object classes
    """

    model: (
        types.HFZeroShotObjectDetection | OutputHandle[types.HFZeroShotObjectDetection]
    ) = connect_field(
        default=types.HFZeroShotObjectDetection(
            type="hf.zero_shot_object_detection",
            repo_id="google/owlv2-base-patch16",
            path=None,
            variant=None,
            allow_patterns=None,
            ignore_patterns=None,
        ),
        description="The zero-shot detection model. OWL-ViT/OWLv2 models use CLIP for flexible label matching; Grounding-DINO offers strong performance.",
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The image to detect objects in.",
    )
    threshold: float | OutputHandle[float] = connect_field(
        default=0.1,
        description="Minimum confidence score (0-1) for detections. Lower values find more objects but may include false positives.",
    )
    top_k: int | OutputHandle[int] = connect_field(
        default=5, description="Maximum number of detected objects to return per label."
    )
    candidate_labels: str | OutputHandle[str] = connect_field(
        default="",
        description="Comma-separated list of object labels to detect (e.g., 'cat,dog,person,car'). Use descriptive phrases for better results.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.huggingface.object_detection.ZeroShotObjectDetection

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()
