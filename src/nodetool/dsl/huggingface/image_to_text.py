# This file is auto-generated by nodetool.dsl.codegen.
# Please do not edit this file manually.

# Instead, edit the node class in the source module and run the following commands to regenerate the DSL:
# nodetool package scan
# nodetool codegen

from pydantic import BaseModel, Field
import typing
from typing import Any
import nodetool.metadata.types
import nodetool.metadata.types as types
from nodetool.dsl.graph import GraphNode, SingleOutputGraphNode

import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.huggingface.image_to_text
from nodetool.workflows.base_node import BaseNode


class ImageToText(SingleOutputGraphNode[str], GraphNode[str]):
    """

    Generates textual descriptions and captions from images using vision-language models.
    image, captioning, OCR, image-to-text, accessibility

    Use cases:
    - Automatically generate captions for photos and artwork
    - Extract visible text from images (OCR-style functionality)
    - Create alt-text descriptions for web accessibility
    - Build image search engines with text-based queries
    - Generate descriptions for visually impaired users
    """

    model: types.HFImageToText | OutputHandle[types.HFImageToText] = connect_field(
        default=types.HFImageToText(
            type="hf.image_to_text",
            repo_id="Salesforce/blip-image-captioning-base",
            path=None,
            variant=None,
            allow_patterns=["*.safetensors", "*.json", "*.txt", "*.model"],
            ignore_patterns=None,
        ),
        description="The image captioning model. BLIP models offer good quality; BLIP2 provides enhanced understanding; GIT is faster.",
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The image to generate text from. Supports JPEG, PNG, WebP and other common formats.",
    )
    max_new_tokens: int | OutputHandle[int] = connect_field(
        default=1024,
        description="Maximum length of the generated caption in tokens. Higher values allow longer descriptions.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.huggingface.image_to_text.ImageToText

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.huggingface.image_to_text
from nodetool.workflows.base_node import BaseNode


class LoadImageToTextModel(
    SingleOutputGraphNode[types.HFImageToText], GraphNode[types.HFImageToText]
):
    """

    Loads and validates a Hugging Face image-to-text model for use in downstream nodes.
    model-loader, captioning, OCR, image-to-text

    Use cases:
    - Pre-load image captioning models before running pipelines
    - Validate model availability and compatibility
    - Configure model settings for ImageToText processing
    """

    repo_id: str | OutputHandle[str] = connect_field(
        default="Salesforce/blip-image-captioning-base",
        description="The Hugging Face repository ID for the image-to-text model (e.g., BLIP, GIT, ViT-GPT2).",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.huggingface.image_to_text.LoadImageToTextModel

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()
