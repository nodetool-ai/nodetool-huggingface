# This file is auto-generated by nodetool.dsl.codegen.
# Please do not edit this file manually.

# Instead, edit the node class in the source module and run the following commands to regenerate the DSL:
# nodetool package scan
# nodetool codegen

from pydantic import BaseModel, Field
import typing
from typing import Any
import nodetool.metadata.types
import nodetool.metadata.types as types
from nodetool.dsl.graph import GraphNode, SingleOutputGraphNode

import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.huggingface.text_to_audio
from nodetool.workflows.base_node import BaseNode


class AudioLDM(SingleOutputGraphNode[types.AudioRef], GraphNode[types.AudioRef]):
    """

    Generates audio from text prompts using the AudioLDM latent diffusion model.
    audio, generation, AI, text-to-audio, sound-effects

    Use cases:
    - Create custom music clips from text descriptions
    - Generate sound effects for videos, games, and media
    - Produce background audio for creative projects
    - Explore AI-generated soundscapes and ambient audio
    """

    prompt: str | OutputHandle[str] = connect_field(
        default="Techno music with a strong, upbeat tempo and high melodic riffs",
        description="Text description of the audio you want to generate.",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=10,
        description="Denoising steps. More steps = better quality but slower. 10-50 is typical.",
    )
    audio_length_in_s: float | OutputHandle[float] = connect_field(
        default=5.0, description="Duration of the generated audio in seconds (1-30)."
    )
    seed: int | OutputHandle[int] = connect_field(
        default=0,
        description="Random seed for reproducible generation. Use -1 for random.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.huggingface.text_to_audio.AudioLDM

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.huggingface.text_to_audio
from nodetool.workflows.base_node import BaseNode


class AudioLDM2(SingleOutputGraphNode[types.AudioRef], GraphNode[types.AudioRef]):
    """

    Generates audio from text prompts using the improved AudioLDM2 model.
    audio, generation, AI, text-to-audio, sound-effects, sound-design

    Use cases:
    - Create realistic sound effects from text descriptions
    - Generate background audio for videos and games
    - Produce environmental soundscapes for multimedia
    - Explore creative AI-generated audio for sound design
    """

    prompt: str | OutputHandle[str] = connect_field(
        default="The sound of a hammer hitting a wooden surface.",
        description="Text description of the audio you want to generate.",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="Low quality.",
        description="Describe what to avoid in the generated audio (e.g., 'noise, distortion').",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=200,
        description="Denoising steps. 200 is recommended for quality; lower for speed.",
    )
    audio_length_in_s: float | OutputHandle[float] = connect_field(
        default=10.0, description="Duration of the generated audio in seconds (1-30)."
    )
    num_waveforms_per_prompt: int | OutputHandle[int] = connect_field(
        default=3,
        description="Number of audio variations to generate. Best result is returned.",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=0,
        description="Random seed for reproducible generation. Use -1 for random.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.huggingface.text_to_audio.AudioLDM2

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.huggingface.text_to_audio
from nodetool.workflows.base_node import BaseNode


class DanceDiffusion(SingleOutputGraphNode[types.AudioRef], GraphNode[types.AudioRef]):
    """

    Generates AI-composed music using the DanceDiffusion unconditional audio model.
    audio, generation, AI, music, text-to-audio, unconditional

    Use cases:
    - Create AI-generated music samples and loops
    - Produce background music for videos and games
    - Generate experimental audio content
    - Explore AI-composed musical ideas and patterns
    """

    audio_length_in_s: float | OutputHandle[float] = connect_field(
        default=4.0, description="Duration of the generated audio in seconds (1-30)."
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=50,
        description="Denoising steps. More steps = better quality but slower. 50-200 is typical.",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=0,
        description="Random seed for reproducible generation. Use -1 for random.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.huggingface.text_to_audio.DanceDiffusion

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.huggingface.text_to_audio
from nodetool.workflows.base_node import BaseNode


class MusicGen(SingleOutputGraphNode[types.AudioRef], GraphNode[types.AudioRef]):
    """

    Generates music and audio from text descriptions using Meta's MusicGen models.
    audio, music, generation, huggingface, text-to-audio, soundtrack

    Use cases:
    - Create custom background music for videos, games, and podcasts
    - Generate sound effects from textual descriptions
    - Prototype musical ideas and compositions quickly
    - Produce royalty-free audio content for creative projects
    - Build AI-powered music generation applications
    """

    model: types.HFTextToAudio | OutputHandle[types.HFTextToAudio] = connect_field(
        default=types.HFTextToAudio(
            type="hf.text_to_audio",
            repo_id="",
            path=None,
            variant=None,
            allow_patterns=None,
            ignore_patterns=None,
        ),
        description="The MusicGen model variant. Small is fastest; Large offers best quality; Melody can condition on audio input; Stereo produces 2-channel output.",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="Describe the music you want to generate (e.g., 'upbeat jazz piano with drums' or 'calm ambient soundscape').",
    )
    max_new_tokens: int | OutputHandle[int] = connect_field(
        default=1024,
        description="Controls audio length: ~256 tokens â‰ˆ 5 seconds. Higher values produce longer audio.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.huggingface.text_to_audio.MusicGen

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.huggingface.text_to_audio
from nodetool.workflows.base_node import BaseNode


class MusicLDM(SingleOutputGraphNode[types.AudioRef], GraphNode[types.AudioRef]):
    """

    Generates music from text descriptions using latent diffusion models.
    audio, music, generation, huggingface, text-to-audio, diffusion

    Use cases:
    - Create custom background music for videos and games
    - Generate music clips based on textual mood descriptions
    - Produce audio content for multimedia projects
    - Explore AI-generated music for creative inspiration
    """

    model: types.HFTextToAudio | OutputHandle[types.HFTextToAudio] = connect_field(
        default=types.HFTextToAudio(
            type="hf.text_to_audio",
            repo_id="",
            path=None,
            variant=None,
            allow_patterns=None,
            ignore_patterns=None,
        ),
        description="The MusicLDM model to use for audio generation.",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="Describe the music you want (e.g., 'electronic dance music with heavy bass').",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=10,
        description="Number of denoising steps. More steps = better quality but slower generation.",
    )
    audio_length_in_s: float | OutputHandle[float] = connect_field(
        default=5.0, description="Duration of the generated audio in seconds."
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.huggingface.text_to_audio.MusicLDM

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.huggingface.text_to_audio
from nodetool.workflows.base_node import BaseNode


class StableAudioNode(SingleOutputGraphNode[types.AudioRef], GraphNode[types.AudioRef]):
    """

    Generates high-quality, long-form audio from text prompts using Stability AI's Stable Audio.
    audio, generation, synthesis, text-to-audio, music, sound-effects

    Use cases:
    - Create professional-quality music and soundtracks
    - Generate ambient sounds and environmental audio
    - Produce sound effects for multimedia projects
    - Create experimental and artistic audio content
    - Generate up to 5 minutes of continuous audio
    """

    prompt: str | OutputHandle[str] = connect_field(
        default="A peaceful piano melody.",
        description="Text description of the audio you want to generate.",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="Low quality.",
        description="Describe what to avoid in the generated audio (e.g., 'noise, distortion').",
    )
    duration: float | OutputHandle[float] = connect_field(
        default=10.0,
        description="Duration of the generated audio in seconds. Stable Audio supports up to 300 seconds.",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=200,
        description="Denoising steps. 200 is recommended for quality; lower values for faster generation.",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=0,
        description="Random seed for reproducible generation. Use -1 for random.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.huggingface.text_to_audio.StableAudioNode

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()
