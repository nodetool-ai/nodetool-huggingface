# This file is auto-generated by nodetool.dsl.codegen.
# Please do not edit this file manually.

# Instead, edit the node class in the source module and run the following commands to regenerate the DSL:
# nodetool package scan
# nodetool codegen

from pydantic import BaseModel, Field
import typing
from typing import Any
import nodetool.metadata.types
import nodetool.metadata.types as types
from nodetool.dsl.graph import GraphNode, SingleOutputGraphNode

import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.huggingface.automatic_speech_recognition
from nodetool.workflows.base_node import BaseNode


class ChunksToSRT(SingleOutputGraphNode[str], GraphNode[str]):
    """

    Converts Whisper audio chunks to SubRip Subtitle (SRT) format for video captioning.
    subtitle, srt, whisper, transcription, captions

    Use cases:
    - Generate .srt subtitle files for video players
    - Create closed captions for accessibility compliance
    - Convert Whisper transcription output to industry-standard format
    - Build automated subtitle generation pipelines
    """

    chunks: list[types.AudioChunk] | OutputHandle[list[types.AudioChunk]] = (
        connect_field(
            default=[],
            description="List of timestamped audio chunks from Whisper transcription output.",
        )
    )
    time_offset: float | OutputHandle[float] = connect_field(
        default=0.0,
        description="Offset in seconds to add to all timestamps (useful when audio is from a clip within a longer video).",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.huggingface.automatic_speech_recognition.ChunksToSRT

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.huggingface.automatic_speech_recognition
from nodetool.workflows.base_node import BaseNode


class Whisper(
    GraphNode[
        nodetool.nodes.huggingface.automatic_speech_recognition.Whisper.OutputType
    ]
):
    """

    Converts speech to text using OpenAI's Whisper models with multilingual support.
    asr, automatic-speech-recognition, speech-to-text, translate, transcribe, audio, huggingface

    Use cases:
    - Transcribe audio files into text for documentation or analysis
    - Enable voice input for chatbots and virtual assistants
    - Create subtitles and closed captions for videos
    - Translate speech from one language to English
    - Build voice-controlled applications

    **Note:** Language selection follows Whisper's FLEURS benchmark word error rate ranking.
    Multiple model variants are available, optimized for different speed/accuracy trade-offs.

    **Links:**
    - https://github.com/openai/whisper
    - https://platform.openai.com/docs/guides/speech-to-text/supported-languages
    """

    Task: typing.ClassVar[type] = (
        nodetool.nodes.huggingface.automatic_speech_recognition.Task
    )
    WhisperLanguage: typing.ClassVar[type] = (
        nodetool.nodes.huggingface.automatic_speech_recognition.WhisperLanguage
    )
    Timestamps: typing.ClassVar[type] = (
        nodetool.nodes.huggingface.automatic_speech_recognition.Timestamps
    )

    model: (
        types.HFAutomaticSpeechRecognition
        | OutputHandle[types.HFAutomaticSpeechRecognition]
    ) = connect_field(
        default=types.HFAutomaticSpeechRecognition(
            type="hf.automatic_speech_recognition",
            repo_id="",
            path=None,
            variant=None,
            allow_patterns=None,
            ignore_patterns=None,
        ),
        description="The Whisper model variant to use. Larger models (large-v3) offer better accuracy; smaller models (small, tiny) are faster. Turbo variants balance speed and quality.",
    )
    audio: types.AudioRef | OutputHandle[types.AudioRef] = connect_field(
        default=types.AudioRef(
            type="audio", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The audio file to transcribe. Supports WAV, MP3, FLAC and other common formats.",
    )
    task: nodetool.nodes.huggingface.automatic_speech_recognition.Task = Field(
        default=nodetool.nodes.huggingface.automatic_speech_recognition.Task.TRANSCRIBE,
        description="Choose 'transcribe' for speech-to-text in the original language, or 'translate' to convert any language to English.",
    )
    language: (
        nodetool.nodes.huggingface.automatic_speech_recognition.WhisperLanguage
    ) = Field(
        default=nodetool.nodes.huggingface.automatic_speech_recognition.WhisperLanguage.NONE,
        description="Specify the audio's language for better accuracy, or use 'auto_detect' to let the model identify it automatically.",
    )
    timestamps: nodetool.nodes.huggingface.automatic_speech_recognition.Timestamps = (
        Field(
            default=nodetool.nodes.huggingface.automatic_speech_recognition.Timestamps.NONE,
            description="Choose 'word' for word-level timing, 'sentence' for phrase-level timing, or 'none' to disable timestamps.",
        )
    )

    @property
    def out(self) -> "WhisperOutputs":
        return WhisperOutputs(self)

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.huggingface.automatic_speech_recognition.Whisper

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


class WhisperOutputs(OutputsProxy):
    @property
    def text(self) -> OutputHandle[typing.Any]:
        return typing.cast(OutputHandle[typing.Any], self["text"])

    @property
    def chunks(self) -> OutputHandle[typing.Any]:
        return typing.cast(OutputHandle[typing.Any], self["chunks"])
