# This file is auto-generated by nodetool.dsl.codegen.
# Please do not edit this file manually.

# Instead, edit the node class in the source module and run the following commands to regenerate the DSL:
# nodetool package scan
# nodetool codegen

from pydantic import BaseModel, Field
import typing
from typing import Any
import nodetool.metadata.types
import nodetool.metadata.types as types
from nodetool.dsl.graph import GraphNode, SingleOutputGraphNode

import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.huggingface.automatic_speech_recognition
from nodetool.workflows.base_node import BaseNode


class ChunksToSRT(SingleOutputGraphNode[str], GraphNode[str]):
    """

    Convert audio chunks to SRT (SubRip Subtitle) format
    subtitle, srt, whisper, transcription

    **Use Cases:**
    - Generate subtitles for videos
    - Create closed captions from audio transcriptions
    - Convert speech-to-text output to a standardized subtitle format

    **Features:**
    - Converts Whisper audio chunks to SRT format
    - Supports customizable time offset
    - Generates properly formatted SRT file content
    """

    chunks: list[types.AudioChunk] | OutputHandle[list[types.AudioChunk]] = (
        connect_field(
            default=[], description="List of audio chunks from Whisper transcription"
        )
    )
    time_offset: float | OutputHandle[float] = connect_field(
        default=0.0, description="Time offset in seconds to apply to all timestamps"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.huggingface.automatic_speech_recognition.ChunksToSRT

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.huggingface.automatic_speech_recognition
from nodetool.workflows.base_node import BaseNode


class Whisper(
    GraphNode[
        nodetool.nodes.huggingface.automatic_speech_recognition.Whisper.OutputType
    ]
):
    """

    Convert speech to text
    asr, automatic-speech-recognition, speech-to-text, translate, transcribe, audio, huggingface

    **Use Cases:**
    - Voice input for a chatbot
    - Transcribe or translate audio files
    - Create subtitles for videos

    **Features:**
    - Multilingual speech recognition
    - Speech translation
    - Language identification

    **Note**
    - Language selection is sorted by word error rate in the FLEURS benchmark
    - There are many variants of Whisper that are optimized for different use cases.

    **Links:**
    - https://github.com/openai/whisper
    - https://platform.openai.com/docs/guides/speech-to-text/supported-languages
    """

    Task: typing.ClassVar[type] = (
        nodetool.nodes.huggingface.automatic_speech_recognition.Task
    )
    WhisperLanguage: typing.ClassVar[type] = (
        nodetool.nodes.huggingface.automatic_speech_recognition.WhisperLanguage
    )
    Timestamps: typing.ClassVar[type] = (
        nodetool.nodes.huggingface.automatic_speech_recognition.Timestamps
    )

    model: (
        types.HFAutomaticSpeechRecognition
        | OutputHandle[types.HFAutomaticSpeechRecognition]
    ) = connect_field(
        default=types.HFAutomaticSpeechRecognition(
            type="hf.automatic_speech_recognition",
            repo_id="",
            path=None,
            variant=None,
            allow_patterns=None,
            ignore_patterns=None,
        ),
        description="The model ID to use for the speech recognition.",
    )
    audio: types.AudioRef | OutputHandle[types.AudioRef] = connect_field(
        default=types.AudioRef(type="audio", uri="", asset_id=None, data=None),
        description="The input audio to transcribe.",
    )
    task: nodetool.nodes.huggingface.automatic_speech_recognition.Task = Field(
        default=nodetool.nodes.huggingface.automatic_speech_recognition.Task.TRANSCRIBE,
        description="The task to perform: 'transcribe' for speech-to-text or 'translate' for speech translation.",
    )
    language: (
        nodetool.nodes.huggingface.automatic_speech_recognition.WhisperLanguage
    ) = Field(
        default=nodetool.nodes.huggingface.automatic_speech_recognition.WhisperLanguage.NONE,
        description="The language of the input audio. If not specified, the model will attempt to detect it automatically.",
    )
    timestamps: nodetool.nodes.huggingface.automatic_speech_recognition.Timestamps = (
        Field(
            default=nodetool.nodes.huggingface.automatic_speech_recognition.Timestamps.NONE,
            description="The type of timestamps to return for the generated text.",
        )
    )

    @property
    def out(self) -> "WhisperOutputs":
        return WhisperOutputs(self)

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.huggingface.automatic_speech_recognition.Whisper

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


class WhisperOutputs(OutputsProxy):
    @property
    def text(self) -> OutputHandle[str]:
        return typing.cast(OutputHandle[str], self["text"])

    @property
    def chunks(self) -> OutputHandle[list[types.AudioChunk]]:
        return typing.cast(OutputHandle[list[types.AudioChunk]], self["chunks"])
