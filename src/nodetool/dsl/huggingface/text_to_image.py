# This file is auto-generated by nodetool.dsl.codegen.
# Please do not edit this file manually.

# Instead, edit the node class in the source module and run the following commands to regenerate the DSL:
# nodetool package scan
# nodetool codegen

from pydantic import BaseModel, Field
import typing
from typing import Any
import nodetool.metadata.types
import nodetool.metadata.types as types
from nodetool.dsl.graph import GraphNode, SingleOutputGraphNode

import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.huggingface.text_to_image
from nodetool.workflows.base_node import BaseNode


class Chroma(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """

    Generates high-quality images from text prompts using Chroma, a Flux-based architecture with enhanced color control.
    image, generation, AI, text-to-image, flux, chroma, transformer, artistic

    Use cases:
    - Generate professional-quality images with precise color control
    - Create artistic images with advanced attention mechanisms
    - Produce images with optimized memory usage via CPU offload
    - Build creative applications requiring high-fidelity output
    """

    prompt: str | OutputHandle[str] = connect_field(
        default="A high-fashion close-up portrait of a blonde woman in clear sunglasses. The image uses a bold teal and red color split for dramatic lighting. The background is a simple teal-green. The photo is sharp and well-composed, and is designed for viewing with anaglyph 3D glasses for optimal effect. It looks professionally done.",
        description="Detailed text description of the image to generate.",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="low quality, ugly, unfinished, out of focus, deformed, disfigure, blurry, smudged, restricted palette, flat colors",
        description="Describe what to avoid (e.g., 'blurry, low quality, distorted').",
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=3.0, description="Prompt adherence strength. 2-5 is typical for Chroma."
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=40,
        description="Denoising steps. 30-50 is typical; more = better quality but slower.",
    )
    height: int | OutputHandle[int] = connect_field(
        default=1024, description="Output image height in pixels."
    )
    width: int | OutputHandle[int] = connect_field(
        default=1024, description="Output image width in pixels."
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="Random seed for reproducible generation. Use -1 for random.",
    )
    max_sequence_length: int | OutputHandle[int] = connect_field(
        default=512, description="Maximum prompt length in tokens."
    )
    enable_cpu_offload: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Offload model components to CPU to reduce VRAM usage.",
    )
    enable_attention_slicing: bool | OutputHandle[bool] = connect_field(
        default=True, description="Process attention in slices to reduce memory usage."
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.huggingface.text_to_image.Chroma

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.huggingface.text_to_image
from nodetool.workflows.base_node import BaseNode


class Flux(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """

    Generates high-quality images using Black Forest Labs' FLUX diffusion models with Nunchaku quantization.
    image, generation, AI, text-to-image, flux, quantization, high-quality

    Use cases:
    - Generate high-fidelity images with excellent text rendering
    - Create images with memory-efficient INT4/FP4 quantization
    - Fast generation with FLUX.1-schnell (4 steps)
    - High-quality generation with FLUX.1-dev
    - Build production image generation systems
    """

    FluxVariant: typing.ClassVar[type] = (
        nodetool.nodes.huggingface.text_to_image.FluxVariant
    )
    FluxQuantization: typing.ClassVar[type] = (
        nodetool.nodes.huggingface.text_to_image.FluxQuantization
    )

    variant: nodetool.nodes.huggingface.text_to_image.FluxVariant = Field(
        default=nodetool.nodes.huggingface.text_to_image.FluxVariant.DEV,
        description="FLUX variant: 'schnell' for fast 4-step generation, 'dev' for higher quality with more steps.",
    )
    quantization: nodetool.nodes.huggingface.text_to_image.FluxQuantization = Field(
        default=nodetool.nodes.huggingface.text_to_image.FluxQuantization.INT4,
        description="Quantization level: INT4/FP4 for lower VRAM, FP16 for full precision.",
    )
    enable_cpu_offload: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Offload model components to CPU to reduce VRAM usage.",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="A cat holding a sign that says hello world",
        description="Text description of the image to generate. FLUX excels at text rendering.",
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=3.5,
        description="Prompt adherence strength. Use 0.0 for schnell, 3-4 for dev.",
    )
    width: int | OutputHandle[int] = connect_field(
        default=1024, description="Output image width in pixels. 1024 is recommended."
    )
    height: int | OutputHandle[int] = connect_field(
        default=1024, description="Output image height in pixels. 1024 is recommended."
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=20, description="Denoising steps. Schnell uses 4 steps; dev uses 20-50."
    )
    max_sequence_length: int | OutputHandle[int] = connect_field(
        default=512,
        description="Maximum prompt length. Use 256 for schnell, 512 for dev.",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="Random seed for reproducible generation. Use -1 for random.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.huggingface.text_to_image.Flux

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.huggingface.text_to_image
from nodetool.workflows.base_node import BaseNode


class FluxControl(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """

    Generates images using FLUX Control models with depth or other control guidance.
    image, generation, AI, text-to-image, flux, control, depth, guidance

    Use cases:
    - Generate images with depth-based control guidance
    - Create images following structural guidance from control images
    - High-quality controlled generation with FLUX models
    - Depth-aware image generation
    """

    FluxControlQuantization: typing.ClassVar[type] = (
        nodetool.nodes.huggingface.text_to_image.FluxControlQuantization
    )

    model: types.HFControlNetFlux | OutputHandle[types.HFControlNetFlux] = (
        connect_field(
            default=types.HFControlNetFlux(
                type="hf.controlnet_flux",
                repo_id="black-forest-labs/FLUX.1-Depth-dev",
                path=None,
                variant=None,
                allow_patterns=None,
                ignore_patterns=None,
            ),
            description="The FLUX Control model to use for controlled image generation.",
        )
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="A robot made of exotic candies and chocolates of different kinds. The background is filled with confetti and celebratory gifts.",
        description="A text prompt describing the desired image.",
    )
    control_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The control image to guide the generation process.",
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=10.0, description="The scale for classifier-free guidance."
    )
    width: int | OutputHandle[int] = connect_field(
        default=1024, description="The width of the generated image."
    )
    height: int | OutputHandle[int] = connect_field(
        default=1024, description="The height of the generated image."
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=30, description="The number of denoising steps."
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="Seed for the random number generator. Use -1 for a random seed.",
    )
    enable_cpu_offload: bool | OutputHandle[bool] = connect_field(
        default=True, description="Enable CPU offload to reduce VRAM usage."
    )
    quantization: nodetool.nodes.huggingface.text_to_image.FluxControlQuantization = (
        Field(
            default=nodetool.nodes.huggingface.text_to_image.FluxControlQuantization.INT4,
            description="Quantization level for the FLUX Control transformer.",
        )
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.huggingface.text_to_image.FluxControl

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.huggingface.text_to_image
from nodetool.workflows.base_node import BaseNode


class LoadTextToImageModel(
    SingleOutputGraphNode[types.HFTextToImage], GraphNode[types.HFTextToImage]
):
    """

    Loads and validates a Hugging Face text-to-image model for use in downstream nodes.
    model-loader, text-to-image, pipeline

    Use cases:
    - Pre-load text-to-image models before running pipelines
    - Validate model availability and compatibility
    - Configure model settings for Text2Image processing
    """

    repo_id: str | OutputHandle[str] = connect_field(
        default="",
        description="The Hugging Face repository ID for the text-to-image model.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.huggingface.text_to_image.LoadTextToImageModel

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.huggingface.text_to_image
from nodetool.workflows.base_node import BaseNode


class QwenImage(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """

    Generates images from text prompts using Alibaba's Qwen-Image model with Nunchaku quantization support.
    image, generation, AI, text-to-image, qwen, quantization, multilingual

    Use cases:
    - Generate high-quality images with strong multilingual prompt support
    - Memory-efficient generation using INT4/FP4 quantization
    - Create images with precise semantic understanding
    - Build production image generation systems
    """

    QwenQuantization: typing.ClassVar[type] = (
        nodetool.nodes.huggingface.text_to_image.QwenQuantization
    )

    quantization: nodetool.nodes.huggingface.text_to_image.QwenQuantization = Field(
        default=nodetool.nodes.huggingface.text_to_image.QwenQuantization.INT4,
        description="Quantization level: INT4/FP4 for lower VRAM, FP16 for full precision.",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="A cat holding a sign that says hello world",
        description="Text description of the image to generate.",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="Describe what to avoid in the image (e.g., 'blurry, low quality').",
    )
    true_cfg_scale: float | OutputHandle[float] = connect_field(
        default=1.0, description="True CFG scale for enhanced prompt following."
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=50, description="Denoising steps. 30-50 is typical."
    )
    height: int | OutputHandle[int] = connect_field(
        default=1024, description="Output image height in pixels."
    )
    width: int | OutputHandle[int] = connect_field(
        default=1024, description="Output image width in pixels."
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="Random seed for reproducible generation. Use -1 for random.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.huggingface.text_to_image.QwenImage

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.huggingface.text_to_image
from nodetool.workflows.base_node import BaseNode
import nodetool.nodes.huggingface.stable_diffusion_base


class StableDiffusion(
    GraphNode[nodetool.nodes.huggingface.text_to_image.StableDiffusion.OutputType]
):
    """

    Generates images from text prompts using Stable Diffusion 1.x/2.x models.
    image, generation, AI, text-to-image, SD, creative

    Use cases:
    - Create custom illustrations and artwork from text descriptions
    - Generate concept art for games, films, and creative projects
    - Produce unique visual content for marketing and media
    - Explore AI-generated art with extensive community models
    - Build image generation applications with well-understood architecture
    """

    StableDiffusionScheduler: typing.ClassVar[type] = (
        nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionBaseNode.StableDiffusionScheduler
    )
    StableDiffusionOutputType: typing.ClassVar[type] = (
        nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionBaseNode.StableDiffusionOutputType
    )

    model: types.HFStableDiffusion | OutputHandle[types.HFStableDiffusion] = (
        connect_field(
            default=types.HFStableDiffusion(
                type="hf.stable_diffusion",
                repo_id="",
                path=None,
                variant=None,
                allow_patterns=None,
                ignore_patterns=None,
            ),
            description="The model to use for image generation.",
        )
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt for image generation."
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="The negative prompt to guide what should not appear in the generated image.",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="Seed for the random number generator. Use -1 for a random seed.",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=25, description="Number of denoising steps."
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=7.5, description="Guidance scale for generation."
    )
    scheduler: (
        nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionBaseNode.StableDiffusionScheduler
    ) = Field(
        default=nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionBaseNode.StableDiffusionScheduler.EulerDiscreteScheduler,
        description="The scheduler to use for the diffusion process.",
    )
    loras: list[types.HFLoraSDConfig] | OutputHandle[list[types.HFLoraSDConfig]] = (
        connect_field(
            default=[], description="The LoRA models to use for image processing"
        )
    )
    ip_adapter_model: types.HFIPAdapter | OutputHandle[types.HFIPAdapter] = (
        connect_field(
            default=types.HFIPAdapter(
                type="hf.ip_adapter",
                repo_id="",
                path=None,
                variant=None,
                allow_patterns=None,
                ignore_patterns=None,
            ),
            description="The IP adapter model to use for image processing",
        )
    )
    ip_adapter_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="When provided the image will be fed into the IP adapter",
    )
    ip_adapter_scale: float | OutputHandle[float] = connect_field(
        default=0.5, description="The strength of the IP adapter"
    )
    latents: types.TorchTensor | OutputHandle[types.TorchTensor] = connect_field(
        default=types.TorchTensor(
            type="torch_tensor", value=None, dtype="<i8", shape=(1,)
        ),
        description="Optional initial latents to start generation from.",
    )
    enable_attention_slicing: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Enable attention slicing for the pipeline. This can reduce VRAM usage.",
    )
    enable_tiling: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Legacy VAE tiling flag (disabled in favor of PyTorch 2 attention optimizations).",
    )
    enable_cpu_offload: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Enable CPU offload for the pipeline. This can reduce VRAM usage.",
    )
    output_type: (
        nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionBaseNode.StableDiffusionOutputType
    ) = Field(
        default=nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionBaseNode.StableDiffusionOutputType.IMAGE,
        description="The type of output to generate.",
    )
    width: int | OutputHandle[int] = connect_field(
        default=512,
        description="Output image width in pixels. 512 is standard for SD 1.x.",
    )
    height: int | OutputHandle[int] = connect_field(
        default=512,
        description="Output image height in pixels. 512 is standard for SD 1.x.",
    )

    @property
    def out(self) -> "StableDiffusionOutputs":
        return StableDiffusionOutputs(self)

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.huggingface.text_to_image.StableDiffusion

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


class StableDiffusionOutputs(OutputsProxy):
    @property
    def image(self) -> OutputHandle[typing.Any]:
        return typing.cast(OutputHandle[typing.Any], self["image"])

    @property
    def latent(self) -> OutputHandle[typing.Any]:
        return typing.cast(OutputHandle[typing.Any], self["latent"])


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.huggingface.text_to_image
from nodetool.workflows.base_node import BaseNode
import nodetool.nodes.huggingface.stable_diffusion_base


class StableDiffusionXL(
    GraphNode[nodetool.nodes.huggingface.text_to_image.StableDiffusionXL.OutputType]
):
    """

    Generates high-resolution images from text prompts using Stable Diffusion XL.
    image, generation, AI, text-to-image, SDXL, high-resolution

    Use cases:
    - Create detailed, high-resolution images (1024x1024) from text
    - Generate marketing visuals and product imagery
    - Produce concept art and illustrations with enhanced detail
    - Create stock imagery and visual content for publications
    - Build professional image generation applications
    """

    StableDiffusionXLQuantization: typing.ClassVar[type] = (
        nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionXLQuantization
    )
    StableDiffusionScheduler: typing.ClassVar[type] = (
        nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionXLBase.StableDiffusionScheduler
    )
    StableDiffusionOutputType: typing.ClassVar[type] = (
        nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionXLBase.StableDiffusionOutputType
    )

    model: types.HFStableDiffusionXL | OutputHandle[types.HFStableDiffusionXL] = (
        connect_field(
            default=types.HFStableDiffusionXL(
                type="hf.stable_diffusion_xl",
                repo_id="",
                path=None,
                variant=None,
                allow_patterns=None,
                ignore_patterns=None,
            ),
            description="The Stable Diffusion XL model to use for generation.",
        )
    )
    quantization: (
        nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionXLQuantization
    ) = Field(
        default=nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionXLQuantization.FP16,
        description="Quantization level for Stable Diffusion XL (enable INT4/FP4 to use a Nunchaku UNet).",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt for image generation."
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="The negative prompt to guide what should not appear in the generated image.",
    )
    width: int | OutputHandle[int] = connect_field(
        default=1024, description="Width of the generated image."
    )
    height: int | OutputHandle[int] = connect_field(
        default=1024, description="Height of the generated image"
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="Seed for the random number generator."
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=25, description="Number of inference steps."
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=7.0, description="Guidance scale for generation."
    )
    scheduler: (
        nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionXLBase.StableDiffusionScheduler
    ) = Field(
        default=nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionXLBase.StableDiffusionScheduler.EulerDiscreteScheduler,
        description="The scheduler to use for the diffusion process.",
    )
    loras: list[types.HFLoraSDXLConfig] | OutputHandle[list[types.HFLoraSDXLConfig]] = (
        connect_field(
            default=[], description="The LoRA models to use for image processing"
        )
    )
    lora_scale: float | OutputHandle[float] = connect_field(
        default=0.5, description="Strength of the LoRAs"
    )
    ip_adapter_model: types.HFIPAdapter | OutputHandle[types.HFIPAdapter] = (
        connect_field(
            default=types.HFIPAdapter(
                type="hf.ip_adapter",
                repo_id="",
                path=None,
                variant=None,
                allow_patterns=None,
                ignore_patterns=None,
            ),
            description="The IP adapter model to use for image processing",
        )
    )
    ip_adapter_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="When provided the image will be fed into the IP adapter",
    )
    ip_adapter_scale: float | OutputHandle[float] = connect_field(
        default=0.5, description="Strength of the IP adapter image"
    )
    enable_attention_slicing: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Enable attention slicing for the pipeline. This can reduce VRAM usage.",
    )
    enable_tiling: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Legacy VAE tiling flag (disabled in favor of PyTorch 2 attention optimizations).",
    )
    enable_cpu_offload: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Enable CPU offload for the pipeline. This can reduce VRAM usage.",
    )
    output_type: (
        nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionXLBase.StableDiffusionOutputType
    ) = Field(
        default=nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionXLBase.StableDiffusionOutputType.IMAGE,
        description="The type of output to generate.",
    )

    @property
    def out(self) -> "StableDiffusionXLOutputs":
        return StableDiffusionXLOutputs(self)

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.huggingface.text_to_image.StableDiffusionXL

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


class StableDiffusionXLOutputs(OutputsProxy):
    @property
    def image(self) -> OutputHandle[typing.Any]:
        return typing.cast(OutputHandle[typing.Any], self["image"])

    @property
    def latent(self) -> OutputHandle[typing.Any]:
        return typing.cast(OutputHandle[typing.Any], self["latent"])


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.huggingface.text_to_image
from nodetool.workflows.base_node import BaseNode


class Text2Image(
    GraphNode[nodetool.nodes.huggingface.text_to_image.Text2Image.OutputType]
):
    """

    Generates images from text prompts using AutoPipeline for automatic model detection.
    image, generation, AI, text-to-image, auto, flexible

    Use cases:
    - Generate images with automatic pipeline selection for any supported model
    - Quickly prototype with various text-to-image architectures
    - Build flexible workflows that adapt to different model types
    - Create images without needing pipeline-specific configuration
    """

    model: types.HFTextToImage | OutputHandle[types.HFTextToImage] = connect_field(
        default=types.HFTextToImage(
            type="hf.text_to_image",
            repo_id="",
            path=None,
            variant=None,
            allow_patterns=None,
            ignore_patterns=None,
        ),
        description="The text-to-image model. AutoPipeline automatically selects the correct pipeline type.",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="A cat holding a sign that says hello world",
        description="Text description of the image to generate. Be specific for better results.",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="Describe what to avoid in the image (e.g., 'blurry, low quality').",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=50,
        description="Denoising steps. 20-50 is typical; more steps = better quality but slower.",
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=7.5,
        description="How strongly to follow the prompt. 7-9 is typical for SD models.",
    )
    width: int | OutputHandle[int] = connect_field(
        default=512, description="Output image width in pixels."
    )
    height: int | OutputHandle[int] = connect_field(
        default=512, description="Output image height in pixels."
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="Random seed for reproducible generation. Use -1 for random.",
    )

    @property
    def out(self) -> "Text2ImageOutputs":
        return Text2ImageOutputs(self)

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.huggingface.text_to_image.Text2Image

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


class Text2ImageOutputs(OutputsProxy):
    @property
    def image(self) -> OutputHandle[typing.Any]:
        return typing.cast(OutputHandle[typing.Any], self["image"])

    @property
    def latent(self) -> OutputHandle[typing.Any]:
        return typing.cast(OutputHandle[typing.Any], self["latent"])
