# This file is auto-generated by nodetool.dsl.codegen.
# Please do not edit this file manually.

# Instead, edit the node class in the source module and run the following commands to regenerate the DSL:
# nodetool package scan
# nodetool codegen

from pydantic import BaseModel, Field
import typing
from typing import Any
import nodetool.metadata.types
import nodetool.metadata.types as types
from nodetool.dsl.graph import GraphNode, SingleOutputGraphNode

import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.huggingface.text_to_image
from nodetool.workflows.base_node import BaseNode


class Chroma(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """

    Generates images from text prompts using Chroma, a text-to-image model based on Flux.
    image, generation, AI, text-to-image, flux, chroma, transformer

    Use cases:
    - Generate high-quality images with Flux-based architecture
    - Create images with advanced attention masking for enhanced fidelity
    - Generate images with optimized memory usage
    - Create professional-quality images with precise color control
    """

    prompt: str | OutputHandle[str] = connect_field(
        default="A high-fashion close-up portrait of a blonde woman in clear sunglasses. The image uses a bold teal and red color split for dramatic lighting. The background is a simple teal-green. The photo is sharp and well-composed, and is designed for viewing with anaglyph 3D glasses for optimal effect. It looks professionally done.",
        description="A text prompt describing the desired image.",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="low quality, ugly, unfinished, out of focus, deformed, disfigure, blurry, smudged, restricted palette, flat colors",
        description="A text prompt describing what to avoid in the image.",
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=3.0, description="The scale for classifier-free guidance."
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=40, description="The number of denoising steps."
    )
    height: int | OutputHandle[int] = connect_field(
        default=1024, description="The height of the generated image."
    )
    width: int | OutputHandle[int] = connect_field(
        default=1024, description="The width of the generated image."
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="Seed for the random number generator. Use -1 for a random seed.",
    )
    max_sequence_length: int | OutputHandle[int] = connect_field(
        default=512, description="Maximum sequence length to use with the prompt."
    )
    enable_cpu_offload: bool | OutputHandle[bool] = connect_field(
        default=True, description="Enable CPU offload to reduce VRAM usage."
    )
    enable_attention_slicing: bool | OutputHandle[bool] = connect_field(
        default=True, description="Enable attention slicing to reduce memory usage."
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.huggingface.text_to_image.Chroma

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.huggingface.text_to_image
from nodetool.workflows.base_node import BaseNode


class Flux(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """

    Generates images using FLUX models with support for Nunchaku quantization.
    image, generation, AI, text-to-image, flux, quantization

    Use cases:
    - High-quality image generation with FLUX models
    - Memory-efficient generation using Nunchaku quantization
    - Fast generation with FLUX.1-schnell
    - High-fidelity generation with FLUX.1-dev
    - Controlled generation with Fill, Canny, or Depth variants
    """

    FluxVariant: typing.ClassVar[type] = (
        nodetool.nodes.huggingface.text_to_image.FluxVariant
    )
    FluxQuantization: typing.ClassVar[type] = (
        nodetool.nodes.huggingface.text_to_image.FluxQuantization
    )

    variant: nodetool.nodes.huggingface.text_to_image.FluxVariant = Field(
        default=nodetool.nodes.huggingface.text_to_image.FluxVariant.DEV,
        description="The FLUX variant to use.",
    )
    quantization: nodetool.nodes.huggingface.text_to_image.FluxQuantization = Field(
        default=nodetool.nodes.huggingface.text_to_image.FluxQuantization.INT4,
        description="The quantization level to use.",
    )
    enable_cpu_offload: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Enable CPU offload for the pipeline. This can reduce VRAM usage.",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="A cat holding a sign that says hello world",
        description="A text prompt describing the desired image.",
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=3.5,
        description="The scale for classifier-free guidance. Use 0.0 for schnell, 3.5 for dev.",
    )
    width: int | OutputHandle[int] = connect_field(
        default=1024, description="The width of the generated image."
    )
    height: int | OutputHandle[int] = connect_field(
        default=1024, description="The height of the generated image."
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=20,
        description="The number of denoising steps. 4 steps is forced for schnell models.",
    )
    max_sequence_length: int | OutputHandle[int] = connect_field(
        default=512,
        description="Maximum sequence length for the prompt. Use 256 for schnell, 512 for dev.",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="Seed for the random number generator. Use -1 for a random seed.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.huggingface.text_to_image.Flux

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.huggingface.text_to_image
from nodetool.workflows.base_node import BaseNode


class FluxControl(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """

    Generates images using FLUX Control models with depth or other control guidance.
    image, generation, AI, text-to-image, flux, control, depth, guidance

    Use cases:
    - Generate images with depth-based control guidance
    - Create images following structural guidance from control images
    - High-quality controlled generation with FLUX models
    - Depth-aware image generation
    """

    FluxControlQuantization: typing.ClassVar[type] = (
        nodetool.nodes.huggingface.text_to_image.FluxControlQuantization
    )

    model: types.HFControlNetFlux | OutputHandle[types.HFControlNetFlux] = (
        connect_field(
            default=types.HFControlNetFlux(
                type="hf.controlnet_flux",
                repo_id="black-forest-labs/FLUX.1-Depth-dev",
                path=None,
                variant=None,
                allow_patterns=None,
                ignore_patterns=None,
            ),
            description="The FLUX Control model to use for controlled image generation.",
        )
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="A robot made of exotic candies and chocolates of different kinds. The background is filled with confetti and celebratory gifts.",
        description="A text prompt describing the desired image.",
    )
    control_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(type="image", uri="", asset_id=None, data=None),
        description="The control image to guide the generation process.",
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=10.0, description="The scale for classifier-free guidance."
    )
    width: int | OutputHandle[int] = connect_field(
        default=1024, description="The width of the generated image."
    )
    height: int | OutputHandle[int] = connect_field(
        default=1024, description="The height of the generated image."
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=30, description="The number of denoising steps."
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="Seed for the random number generator. Use -1 for a random seed.",
    )
    enable_cpu_offload: bool | OutputHandle[bool] = connect_field(
        default=True, description="Enable CPU offload to reduce VRAM usage."
    )
    quantization: nodetool.nodes.huggingface.text_to_image.FluxControlQuantization = (
        Field(
            default=nodetool.nodes.huggingface.text_to_image.FluxControlQuantization.INT4,
            description="Quantization level for the FLUX Control transformer.",
        )
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.huggingface.text_to_image.FluxControl

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.huggingface.text_to_image
from nodetool.workflows.base_node import BaseNode


class LoadTextToImageModel(
    SingleOutputGraphNode[types.HFTextToImage], GraphNode[types.HFTextToImage]
):
    """

    Load HuggingFace model for image-to-image generation from a repo_id.

    Use cases:
    - Loads a pipeline directly from a repo_id
    - Used for AutoPipelineForImage2Image
    """

    repo_id: str | OutputHandle[str] = connect_field(
        default="",
        description="The repository ID of the model to use for image-to-image generation.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.huggingface.text_to_image.LoadTextToImageModel

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.huggingface.text_to_image
from nodetool.workflows.base_node import BaseNode


class QwenImage(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """

    Generates images from text prompts using Qwen-Image with support for Nunchaku quantization.
    image, generation, AI, text-to-image, qwen, quantization

    Use cases:
    - High-quality, general-purpose text-to-image generation
    - Memory-efficient generation using Nunchaku quantization
    - Quick prototyping leveraging AutoPipeline
    - Works out-of-the-box with the official Qwen model
    """

    QwenQuantization: typing.ClassVar[type] = (
        nodetool.nodes.huggingface.text_to_image.QwenQuantization
    )

    quantization: nodetool.nodes.huggingface.text_to_image.QwenQuantization = Field(
        default=nodetool.nodes.huggingface.text_to_image.QwenQuantization.INT4,
        description="The quantization level to use.",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="A cat holding a sign that says hello world",
        description="A text prompt describing the desired image.",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="", description="A text prompt describing what to avoid in the image."
    )
    true_cfg_scale: float | OutputHandle[float] = connect_field(
        default=1.0, description="True CFG scale for Qwen-Image models."
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=50, description="The number of denoising steps."
    )
    height: int | OutputHandle[int] = connect_field(
        default=1024, description="The height of the generated image."
    )
    width: int | OutputHandle[int] = connect_field(
        default=1024, description="The width of the generated image."
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="Seed for the random number generator. Use -1 for a random seed.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.huggingface.text_to_image.QwenImage

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.huggingface.text_to_image
from nodetool.workflows.base_node import BaseNode
import nodetool.nodes.huggingface.stable_diffusion_base


class StableDiffusion(
    GraphNode[nodetool.nodes.huggingface.text_to_image.StableDiffusion.OutputType]
):
    """

    Generates images from text prompts using Stable Diffusion.
    image, generation, AI, text-to-image, SD

    Use cases:
    - Creating custom illustrations for various projects
    - Generating concept art for creative endeavors
    - Producing unique visual content for marketing materials
    - Exploring AI-generated art for personal or professional use
    """

    StableDiffusionScheduler: typing.ClassVar[type] = (
        nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionBaseNode.StableDiffusionScheduler
    )
    StableDiffusionOutputType: typing.ClassVar[type] = (
        nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionBaseNode.StableDiffusionOutputType
    )

    model: types.HFStableDiffusion | OutputHandle[types.HFStableDiffusion] = (
        connect_field(
            default=types.HFStableDiffusion(
                type="hf.stable_diffusion",
                repo_id="",
                path=None,
                variant=None,
                allow_patterns=None,
                ignore_patterns=None,
            ),
            description="The model to use for image generation.",
        )
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt for image generation."
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="The negative prompt to guide what should not appear in the generated image.",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="Seed for the random number generator. Use -1 for a random seed.",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=25, description="Number of denoising steps."
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=7.5, description="Guidance scale for generation."
    )
    scheduler: (
        nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionBaseNode.StableDiffusionScheduler
    ) = Field(
        default=nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionBaseNode.StableDiffusionScheduler.EulerDiscreteScheduler,
        description="The scheduler to use for the diffusion process.",
    )
    loras: list[types.HFLoraSDConfig] | OutputHandle[list[types.HFLoraSDConfig]] = (
        connect_field(
            default=[], description="The LoRA models to use for image processing"
        )
    )
    ip_adapter_model: types.HFIPAdapter | OutputHandle[types.HFIPAdapter] = (
        connect_field(
            default=types.HFIPAdapter(
                type="hf.ip_adapter",
                repo_id="",
                path=None,
                variant=None,
                allow_patterns=None,
                ignore_patterns=None,
            ),
            description="The IP adapter model to use for image processing",
        )
    )
    ip_adapter_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(type="image", uri="", asset_id=None, data=None),
        description="When provided the image will be fed into the IP adapter",
    )
    ip_adapter_scale: float | OutputHandle[float] = connect_field(
        default=0.5, description="The strength of the IP adapter"
    )
    latents: types.TorchTensor | OutputHandle[types.TorchTensor] = connect_field(
        default=types.TorchTensor(
            type="torch_tensor", value=None, dtype="<i8", shape=(1,)
        ),
        description="Optional initial latents to start generation from.",
    )
    enable_attention_slicing: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Enable attention slicing for the pipeline. This can reduce VRAM usage.",
    )
    enable_tiling: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Legacy VAE tiling flag (disabled in favor of PyTorch 2 attention optimizations).",
    )
    enable_cpu_offload: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Enable CPU offload for the pipeline. This can reduce VRAM usage.",
    )
    output_type: (
        nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionBaseNode.StableDiffusionOutputType
    ) = Field(
        default=nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionBaseNode.StableDiffusionOutputType.IMAGE,
        description="The type of output to generate.",
    )
    width: int | OutputHandle[int] = connect_field(
        default=512, description="Width of the generated image."
    )
    height: int | OutputHandle[int] = connect_field(
        default=512, description="Height of the generated image"
    )

    @property
    def out(self) -> "StableDiffusionOutputs":
        return StableDiffusionOutputs(self)

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.huggingface.text_to_image.StableDiffusion

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


class StableDiffusionOutputs(OutputsProxy):
    @property
    def image(self) -> OutputHandle[nodetool.metadata.types.ImageRef]:
        return typing.cast(
            OutputHandle[nodetool.metadata.types.ImageRef], self["image"]
        )

    @property
    def latent(self) -> OutputHandle[nodetool.metadata.types.TorchTensor]:
        return typing.cast(
            OutputHandle[nodetool.metadata.types.TorchTensor], self["latent"]
        )


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.huggingface.text_to_image
from nodetool.workflows.base_node import BaseNode
import nodetool.nodes.huggingface.stable_diffusion_base


class StableDiffusionXL(
    GraphNode[nodetool.nodes.huggingface.text_to_image.StableDiffusionXL.OutputType]
):
    """

    Generates images from text prompts using Stable Diffusion XL.
    image, generation, AI, text-to-image, SDXL

    Use cases:
    - Creating custom illustrations for marketing materials
    - Generating concept art for game and film development
    - Producing unique stock imagery for websites and publications
    - Visualizing interior design concepts for clients
    """

    StableDiffusionXLQuantization: typing.ClassVar[type] = (
        nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionXLQuantization
    )
    StableDiffusionScheduler: typing.ClassVar[type] = (
        nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionXLBase.StableDiffusionScheduler
    )
    StableDiffusionOutputType: typing.ClassVar[type] = (
        nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionXLBase.StableDiffusionOutputType
    )

    model: types.HFStableDiffusionXL | OutputHandle[types.HFStableDiffusionXL] = (
        connect_field(
            default=types.HFStableDiffusionXL(
                type="hf.stable_diffusion_xl",
                repo_id="",
                path=None,
                variant=None,
                allow_patterns=None,
                ignore_patterns=None,
            ),
            description="The Stable Diffusion XL model to use for generation.",
        )
    )
    quantization: (
        nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionXLQuantization
    ) = Field(
        default=nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionXLQuantization.FP16,
        description="Quantization level for Stable Diffusion XL (enable INT4/FP4 to use a Nunchaku UNet).",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt for image generation."
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="The negative prompt to guide what should not appear in the generated image.",
    )
    width: int | OutputHandle[int] = connect_field(
        default=1024, description="Width of the generated image."
    )
    height: int | OutputHandle[int] = connect_field(
        default=1024, description="Height of the generated image"
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="Seed for the random number generator."
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=25, description="Number of inference steps."
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=7.0, description="Guidance scale for generation."
    )
    scheduler: (
        nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionXLBase.StableDiffusionScheduler
    ) = Field(
        default=nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionXLBase.StableDiffusionScheduler.EulerDiscreteScheduler,
        description="The scheduler to use for the diffusion process.",
    )
    loras: list[types.HFLoraSDXLConfig] | OutputHandle[list[types.HFLoraSDXLConfig]] = (
        connect_field(
            default=[], description="The LoRA models to use for image processing"
        )
    )
    lora_scale: float | OutputHandle[float] = connect_field(
        default=0.5, description="Strength of the LoRAs"
    )
    ip_adapter_model: types.HFIPAdapter | OutputHandle[types.HFIPAdapter] = (
        connect_field(
            default=types.HFIPAdapter(
                type="hf.ip_adapter",
                repo_id="",
                path=None,
                variant=None,
                allow_patterns=None,
                ignore_patterns=None,
            ),
            description="The IP adapter model to use for image processing",
        )
    )
    ip_adapter_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(type="image", uri="", asset_id=None, data=None),
        description="When provided the image will be fed into the IP adapter",
    )
    ip_adapter_scale: float | OutputHandle[float] = connect_field(
        default=0.5, description="Strength of the IP adapter image"
    )
    enable_attention_slicing: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Enable attention slicing for the pipeline. This can reduce VRAM usage.",
    )
    enable_tiling: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Legacy VAE tiling flag (disabled in favor of PyTorch 2 attention optimizations).",
    )
    enable_cpu_offload: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Enable CPU offload for the pipeline. This can reduce VRAM usage.",
    )
    output_type: (
        nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionXLBase.StableDiffusionOutputType
    ) = Field(
        default=nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionXLBase.StableDiffusionOutputType.IMAGE,
        description="The type of output to generate.",
    )

    @property
    def out(self) -> "StableDiffusionXLOutputs":
        return StableDiffusionXLOutputs(self)

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.huggingface.text_to_image.StableDiffusionXL

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


class StableDiffusionXLOutputs(OutputsProxy):
    @property
    def image(self) -> OutputHandle[nodetool.metadata.types.ImageRef]:
        return typing.cast(
            OutputHandle[nodetool.metadata.types.ImageRef], self["image"]
        )

    @property
    def latent(self) -> OutputHandle[nodetool.metadata.types.TorchTensor]:
        return typing.cast(
            OutputHandle[nodetool.metadata.types.TorchTensor], self["latent"]
        )


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.huggingface.text_to_image
from nodetool.workflows.base_node import BaseNode


class Text2Image(
    GraphNode[nodetool.nodes.huggingface.text_to_image.Text2Image.OutputType]
):
    """

    Generates images from text prompts using AutoPipeline for automatic pipeline selection.
    image, generation, AI, text-to-image, auto

    Use cases:
    - Automatic selection of the best pipeline for a given model
    - Flexible image generation without pipeline-specific knowledge
    - Quick prototyping with various text-to-image models
    - Streamlined workflow for different model architectures
    """

    model: types.HFTextToImage | OutputHandle[types.HFTextToImage] = connect_field(
        default=types.HFTextToImage(
            type="hf.text_to_image",
            repo_id="",
            path=None,
            variant=None,
            allow_patterns=None,
            ignore_patterns=None,
        ),
        description="The model to use for text-to-image generation.",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="A cat holding a sign that says hello world",
        description="A text prompt describing the desired image.",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="", description="A text prompt describing what to avoid in the image."
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=50, description="The number of denoising steps."
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=7.5, description="The scale for classifier-free guidance."
    )
    width: int | OutputHandle[int] = connect_field(
        default=512, description="The width of the generated image."
    )
    height: int | OutputHandle[int] = connect_field(
        default=512, description="The height of the generated image."
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="Seed for the random number generator. Use -1 for a random seed.",
    )

    @property
    def out(self) -> "Text2ImageOutputs":
        return Text2ImageOutputs(self)

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.huggingface.text_to_image.Text2Image

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


class Text2ImageOutputs(OutputsProxy):
    @property
    def image(self) -> OutputHandle[nodetool.metadata.types.ImageRef]:
        return typing.cast(
            OutputHandle[nodetool.metadata.types.ImageRef], self["image"]
        )

    @property
    def latent(self) -> OutputHandle[nodetool.metadata.types.TorchTensor]:
        return typing.cast(
            OutputHandle[nodetool.metadata.types.TorchTensor], self["latent"]
        )
