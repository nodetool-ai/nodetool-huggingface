# This file is auto-generated by nodetool.dsl.codegen.
# Please do not edit this file manually.

# Instead, edit the node class in the source module and run the following commands to regenerate the DSL:
# nodetool package scan
# nodetool codegen

from pydantic import BaseModel, Field
import typing
from typing import Any
import nodetool.metadata.types
import nodetool.metadata.types as types
from nodetool.dsl.graph import GraphNode, SingleOutputGraphNode

import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.huggingface.audio_classification
from nodetool.workflows.base_node import BaseNode


class AudioClassifier(
    SingleOutputGraphNode[dict[str, float]], GraphNode[dict[str, float]]
):
    """

    Classifies audio into predefined categories using pretrained transformer models.
    audio, classification, labeling, categorization, sound-recognition

    Use cases:
    - Classify music by genre or mood
    - Detect speech vs. non-speech audio segments
    - Identify environmental sounds (e.g., car horn, dog bark)
    - Recognize emotions in speech recordings
    - Content moderation for audio platforms
    """

    model: types.HFAudioClassification | OutputHandle[types.HFAudioClassification] = (
        connect_field(
            default=types.HFAudioClassification(
                type="hf.audio_classification",
                repo_id="",
                path=None,
                variant=None,
                allow_patterns=None,
                ignore_patterns=None,
            ),
            description="The Hugging Face model for audio classification. Recommended: MIT/ast-finetuned-audioset for general sounds, wav2vec2-lg-xlsr-en-speech-emotion-recognition for speech emotions.",
        )
    )
    audio: types.AudioRef | OutputHandle[types.AudioRef] = connect_field(
        default=types.AudioRef(
            type="audio", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The audio file to classify. Supports common formats like WAV, MP3, FLAC.",
    )
    top_k: int | OutputHandle[int] = connect_field(
        default=10,
        description="Number of top classification results to return, ranked by confidence score.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.huggingface.audio_classification.AudioClassifier

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.huggingface.audio_classification
from nodetool.workflows.base_node import BaseNode


class ZeroShotAudioClassifier(
    SingleOutputGraphNode[dict[str, float]], GraphNode[dict[str, float]]
):
    """

    Classifies audio into custom categories without requiring task-specific training data.
    audio, classification, labeling, categorization, zero-shot, flexible

    Use cases:
    - Categorize audio with custom, user-defined labels on the fly
    - Identify sounds or music genres without predefined model training
    - Quickly prototype audio classification systems
    - Automate tagging for large audio datasets with dynamic categories
    """

    model: (
        types.HFZeroShotAudioClassification
        | OutputHandle[types.HFZeroShotAudioClassification]
    ) = connect_field(
        default=types.HFZeroShotAudioClassification(
            type="hf.zero_shot_audio_classification",
            repo_id="",
            path=None,
            variant=None,
            allow_patterns=None,
            ignore_patterns=None,
        ),
        description="The Hugging Face model for zero-shot audio classification. Uses CLIP-based models for flexible label matching.",
    )
    audio: types.AudioRef | OutputHandle[types.AudioRef] = connect_field(
        default=types.AudioRef(
            type="audio", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The audio file to classify. Supports common formats like WAV, MP3, FLAC.",
    )
    candidate_labels: str | OutputHandle[str] = connect_field(
        default="",
        description="Comma-separated list of labels to classify against (e.g., 'music,speech,noise,silence').",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.huggingface.audio_classification.ZeroShotAudioClassifier

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()
