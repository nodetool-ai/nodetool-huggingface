# This file is auto-generated by nodetool.dsl.codegen.
# Please do not edit this file manually.

# Instead, edit the node class in the source module and run the following commands to regenerate the DSL:
# nodetool package scan
# nodetool codegen

from pydantic import BaseModel, Field
import typing
from typing import Any
import nodetool.metadata.types
import nodetool.metadata.types as types
from nodetool.dsl.graph import GraphNode, SingleOutputGraphNode

import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.huggingface.image_to_video
from nodetool.workflows.base_node import BaseNode


class Wan_I2V(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

    Transforms a static image into a dynamic video clip using Wan image-to-video diffusion models.
    video, generation, AI, image-to-video, diffusion, Wan, animation

    Use cases:
    - Animate photographs and artwork into short video clips
    - Create motion from still images with text-guided direction
    - Generate video content for social media from static images
    - Bring product images to life with realistic movement
    - Create dynamic visual effects from single frames

    **Note:** Model variants offer different quality/speed tradeoffs. A14B is balanced; 720P provides higher resolution.
    """

    WanI2VModel: typing.ClassVar[type] = (
        nodetool.nodes.huggingface.image_to_video.Wan_I2V.WanI2VModel
    )

    input_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The source image to animate. Image content guides the video's appearance.",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="An astronaut walking on the moon, cinematic lighting, high detail",
        description="Text description guiding how the image should animate and move.",
    )
    model_variant: nodetool.nodes.huggingface.image_to_video.Wan_I2V.WanI2VModel = (
        Field(
            default=nodetool.nodes.huggingface.image_to_video.Wan_I2V.WanI2VModel.WAN_2_2_I2V_A14B,
            description="The Wan I2V model variant. A14B is balanced; 720P offers higher resolution output.",
        )
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="Describe what to avoid in the generated video (e.g., 'blurry, distorted').",
    )
    num_frames: int | OutputHandle[int] = connect_field(
        default=81,
        description="Total frames in the output video. More frames = longer duration.",
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=5.0,
        description="How strongly to follow the prompt. Higher values = more prompt adherence.",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=50,
        description="Denoising steps. More steps = better quality but slower generation.",
    )
    height: int | OutputHandle[int] = connect_field(
        default=480, description="Output video height in pixels."
    )
    width: int | OutputHandle[int] = connect_field(
        default=832, description="Output video width in pixels."
    )
    fps: int | OutputHandle[int] = connect_field(
        default=16, description="Frames per second for the output video file."
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="Random seed for reproducibility. Use -1 for random generation.",
    )
    max_sequence_length: int | OutputHandle[int] = connect_field(
        default=512,
        description="Maximum prompt encoding length. Higher allows longer prompts.",
    )
    enable_cpu_offload: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Offload model components to CPU to reduce VRAM usage.",
    )
    enable_vae_slicing: bool | OutputHandle[bool] = connect_field(
        default=True, description="Process VAE in slices to reduce peak memory usage."
    )
    enable_vae_tiling: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Process VAE in tiles for very large videos. May affect quality.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.huggingface.image_to_video.Wan_I2V

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()
