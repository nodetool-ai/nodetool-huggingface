# This file is auto-generated by nodetool.dsl.codegen.
# Please do not edit this file manually.

# Instead, edit the node class in the source module and run the following commands to regenerate the DSL:
# nodetool package scan
# nodetool codegen

from pydantic import BaseModel, Field
import typing
from typing import Any
import nodetool.metadata.types
import nodetool.metadata.types as types
from nodetool.dsl.graph import GraphNode, SingleOutputGraphNode

import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.huggingface.text_generation
from nodetool.workflows.base_node import BaseNode


class TextGeneration(
    GraphNode[nodetool.nodes.huggingface.text_generation.TextGeneration.OutputType]
):
    """

    Generates text continuations and responses from prompts using large language models.
    text, generation, NLP, LLM, chatbot, creative-writing

    Use cases:
    - Generate creative writing, stories, and content
    - Build chatbots and conversational AI assistants
    - Create code completions and programming assistance
    - Produce automated content for various applications
    - Answer questions and provide explanations
    """

    model: types.HFTextGeneration | OutputHandle[types.HFTextGeneration] = (
        connect_field(
            default=types.HFTextGeneration(
                type="hf.text_generation",
                repo_id="",
                path=None,
                variant=None,
                allow_patterns=None,
                ignore_patterns=None,
            ),
            description="The language model to use. Instruction-tuned models (Instruct) follow prompts better; Base models are for completion tasks. BNB-4bit variants reduce memory.",
        )
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="The input text to generate from. For instruct models, phrase as a request; for base models, provide text to continue.",
    )
    max_new_tokens: int | OutputHandle[int] = connect_field(
        default=512, description="Maximum number of tokens to generate in the response."
    )
    temperature: float | OutputHandle[float] = connect_field(
        default=1.0,
        description="Controls randomness: lower values (0.1-0.5) for focused responses, higher (0.7-1.5) for creative output.",
    )
    top_p: float | OutputHandle[float] = connect_field(
        default=1.0,
        description="Nucleus sampling: limits token selection to top probability mass. Lower values (0.1-0.5) increase focus.",
    )
    do_sample: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Enable sampling for varied outputs. Disable for deterministic, greedy decoding.",
    )

    @property
    def out(self) -> "TextGenerationOutputs":
        return TextGenerationOutputs(self)

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.huggingface.text_generation.TextGeneration

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


class TextGenerationOutputs(OutputsProxy):
    @property
    def text(self) -> OutputHandle[str]:
        return typing.cast(OutputHandle[str], self["text"])

    @property
    def chunk(self) -> OutputHandle[types.Chunk]:
        return typing.cast(OutputHandle[types.Chunk], self["chunk"])
