# This file is auto-generated by nodetool.dsl.codegen.
# Please do not edit this file manually.

# Instead, edit the node class in the source module and run the following commands to regenerate the DSL:
# nodetool package scan
# nodetool codegen

from pydantic import BaseModel, Field
import typing
from typing import Any
import nodetool.metadata.types
import nodetool.metadata.types as types
from nodetool.dsl.graph import GraphNode, SingleOutputGraphNode

import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.huggingface.image_to_image
from nodetool.workflows.base_node import BaseNode


class ImageToImage(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """

    Transforms existing images based on text prompts using AutoPipeline for Image-to-Image.
    This node automatically detects the appropriate pipeline class based on the model used.
    image, generation, image-to-image, autopipeline

    Use cases:
    - Transform existing images with any compatible model (Stable Diffusion, SDXL, Kandinsky, etc.)
    - Apply specific styles or concepts to photographs or artwork
    - Modify existing images based on text descriptions
    - Create variations of existing visual content with automatic pipeline selection
    """

    model: types.HFImageToImage | OutputHandle[types.HFImageToImage] = connect_field(
        default=types.HFImageToImage(
            type="hf.image_to_image",
            repo_id="",
            path=None,
            variant=None,
            allow_patterns=None,
            ignore_patterns=None,
        ),
        description="The HuggingFace model to use for image-to-image generation.",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="A beautiful landscape with mountains and a lake at sunset",
        description="Text prompt describing the desired image transformation.",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="Text prompt describing what should not appear in the generated image.",
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(type="image", uri="", asset_id=None, data=None),
        description="The input image to transform",
    )
    strength: float | OutputHandle[float] = connect_field(
        default=0.8,
        description="Strength of the transformation. Higher values allow for more deviation from the original image.",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=25, description="Number of denoising steps."
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=7.5,
        description="Guidance scale for generation. Higher values follow the prompt more closely.",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="Seed for the random number generator. Use -1 for a random seed.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.huggingface.image_to_image.ImageToImage

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.huggingface.image_to_image
from nodetool.workflows.base_node import BaseNode


class Inpaint(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """

    Performs inpainting on images using AutoPipeline for Inpainting.
    This node automatically detects the appropriate pipeline class based on the model used.
    image, inpainting, autopipeline, stable-diffusion, SDXL, kandinsky

    Use cases:
    - Remove unwanted objects from images with any compatible model
    - Fill in missing parts of images using various diffusion models
    - Modify specific areas of images while preserving the rest
    - Automatic pipeline selection for different model architectures
    """

    model: types.HFImageToImage | OutputHandle[types.HFImageToImage] = connect_field(
        default=types.HFImageToImage(
            type="hf.image_to_image",
            repo_id="",
            path=None,
            variant=None,
            allow_patterns=None,
            ignore_patterns=None,
        ),
        description="The HuggingFace model to use for inpainting.",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="Text prompt describing what should be generated in the masked area.",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="Text prompt describing what should not appear in the generated content.",
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(type="image", uri="", asset_id=None, data=None),
        description="The input image to inpaint",
    )
    mask_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(type="image", uri="", asset_id=None, data=None),
        description="The mask image indicating areas to be inpainted (white areas will be inpainted)",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=25, description="Number of denoising steps."
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=7.5,
        description="Guidance scale for generation. Higher values follow the prompt more closely.",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="Seed for the random number generator. Use -1 for a random seed.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.huggingface.image_to_image.Inpaint

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.huggingface.image_to_image
from nodetool.workflows.base_node import BaseNode
import nodetool.nodes.huggingface.stable_diffusion_base


class LoadImageToImageModel(
    SingleOutputGraphNode[types.HFImageToImage], GraphNode[types.HFImageToImage]
):
    """

    Load HuggingFace model for image-to-image generation from a repo_id.

    Use cases:
    - Loads a pipeline directly from a repo_id
    - Used for ImageToImage node
    """

    ModelVariant: typing.ClassVar[type] = (
        nodetool.nodes.huggingface.stable_diffusion_base.ModelVariant
    )

    repo_id: str | OutputHandle[str] = connect_field(
        default="",
        description="The repository ID of the model to use for image-to-image generation.",
    )
    variant: nodetool.nodes.huggingface.stable_diffusion_base.ModelVariant = Field(
        default=nodetool.nodes.huggingface.stable_diffusion_base.ModelVariant.FP16,
        description="The variant of the model to use for image-to-image generation.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.huggingface.image_to_image.LoadImageToImageModel

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.huggingface.image_to_image
from nodetool.workflows.base_node import BaseNode


class OmniGenNode(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """

    Generates and edits images using the OmniGen model, supporting multimodal inputs.
    image, generation, text-to-image, image-editing, multimodal, omnigen

    Use cases:
    - Generate images from text prompts
    - Edit existing images with text instructions
    - Controllable image generation with reference images
    - Visual reasoning and image manipulation
    - ID and object preserving generation
    """

    prompt: str | OutputHandle[str] = connect_field(
        default="A realistic photo of a young woman sitting on a sofa, holding a book and facing the camera.",
        description="The text prompt for image generation. Use <img><|image_1|></img> placeholders to reference input images.",
    )
    input_images: list[types.ImageRef] | OutputHandle[list[types.ImageRef]] = (
        connect_field(
            default=[],
            description="List of input images to use for editing or as reference. Referenced in prompt using <img><|image_1|></img>, <img><|image_2|></img>, etc.",
        )
    )
    height: int | OutputHandle[int] = connect_field(
        default=1024, description="Height of the generated image."
    )
    width: int | OutputHandle[int] = connect_field(
        default=1024, description="Width of the generated image."
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=2.5,
        description="Guidance scale for generation. Higher values follow the prompt more closely.",
    )
    img_guidance_scale: float | OutputHandle[float] = connect_field(
        default=1.6, description="Image guidance scale when using input images."
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=25, description="Number of denoising steps."
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="Seed for the random number generator. Use -1 for a random seed.",
    )
    use_input_image_size_as_output: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If True, use the input image size as output size. Recommended for image editing.",
    )
    max_input_image_size: int | OutputHandle[int] = connect_field(
        default=1024,
        description="Maximum input image size. Smaller values reduce memory usage but may affect quality.",
    )
    enable_model_cpu_offload: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Enable CPU offload to reduce memory usage when using multiple images.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.huggingface.image_to_image.OmniGenNode

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.huggingface.image_to_image
from nodetool.workflows.base_node import BaseNode


class QwenImageEdit(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """

    Performs image editing using the Qwen Image Edit model.
    image, editing, semantic, appearance, qwen, multimodal

    Use cases:
    - Semantic editing (object rotation, style transfer)
    - Appearance editing (adding/removing elements)
    - Precise text modifications in images
    - Background and clothing changes
    - Complex image transformations guided by text
    """

    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(type="image", uri="", asset_id=None, data=None),
        description="The input image to edit",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="Change the object's color to blue",
        description="Text description of the desired edit to apply to the image",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="Text describing what should not appear in the edited image",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=50, description="Number of denoising steps for the editing process"
    )
    true_cfg_scale: float | OutputHandle[float] = connect_field(
        default=4.0,
        description="Guidance scale for editing. Higher values follow the prompt more closely",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="Seed for the random number generator. Use -1 for a random seed",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.huggingface.image_to_image.QwenImageEdit

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.huggingface.image_to_image
from nodetool.workflows.base_node import BaseNode


class RealESRGANNode(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """

    Performs image super-resolution using the RealESRGAN model.
    image, super-resolution, enhancement, huggingface

    Use cases:
    - Enhance low-resolution images
    - Improve image quality for printing or display
    - Upscale images for better detail
    """

    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(type="image", uri="", asset_id=None, data=None),
        description="The input image to transform",
    )
    model: types.HFRealESRGAN | OutputHandle[types.HFRealESRGAN] = connect_field(
        default=types.HFRealESRGAN(
            type="hf.real_esrgan",
            repo_id="",
            path=None,
            variant=None,
            allow_patterns=None,
            ignore_patterns=None,
        ),
        description="The RealESRGAN model to use for image super-resolution",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.huggingface.image_to_image.RealESRGANNode

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.huggingface.image_to_image
from nodetool.workflows.base_node import BaseNode
import nodetool.nodes.huggingface.stable_diffusion_base


class StableDiffusionControlNet(
    GraphNode[
        nodetool.nodes.huggingface.image_to_image.StableDiffusionControlNet.OutputType
    ]
):
    """

    Generates images using Stable Diffusion with ControlNet guidance.
    image, generation, text-to-image, controlnet, SD

    Use cases:
    - Generate images with precise control over composition and structure
    - Create variations of existing images while maintaining specific features
    - Artistic image generation with guided outputs
    """

    ModelVariant: typing.ClassVar[type] = (
        nodetool.nodes.huggingface.stable_diffusion_base.ModelVariant
    )
    StableDiffusionScheduler: typing.ClassVar[type] = (
        nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionBaseNode.StableDiffusionScheduler
    )
    StableDiffusionOutputType: typing.ClassVar[type] = (
        nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionBaseNode.StableDiffusionOutputType
    )

    model: types.HFStableDiffusion | OutputHandle[types.HFStableDiffusion] = (
        connect_field(
            default=types.HFStableDiffusion(
                type="hf.stable_diffusion",
                repo_id="",
                path=None,
                variant=None,
                allow_patterns=None,
                ignore_patterns=None,
            ),
            description="The model to use for image generation.",
        )
    )
    variant: nodetool.nodes.huggingface.stable_diffusion_base.ModelVariant = Field(
        default=nodetool.nodes.huggingface.stable_diffusion_base.ModelVariant.FP16,
        description="The variant of the model to use for generation.",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt for image generation."
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="The negative prompt to guide what should not appear in the generated image.",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="Seed for the random number generator. Use -1 for a random seed.",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=25, description="Number of denoising steps."
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=7.5, description="Guidance scale for generation."
    )
    scheduler: (
        nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionBaseNode.StableDiffusionScheduler
    ) = Field(
        default=nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionBaseNode.StableDiffusionScheduler.EulerDiscreteScheduler,
        description="The scheduler to use for the diffusion process.",
    )
    loras: list[types.HFLoraSDConfig] | OutputHandle[list[types.HFLoraSDConfig]] = (
        connect_field(
            default=[], description="The LoRA models to use for image processing"
        )
    )
    ip_adapter_model: types.HFIPAdapter | OutputHandle[types.HFIPAdapter] = (
        connect_field(
            default=types.HFIPAdapter(
                type="hf.ip_adapter",
                repo_id="",
                path=None,
                variant=None,
                allow_patterns=None,
                ignore_patterns=None,
            ),
            description="The IP adapter model to use for image processing",
        )
    )
    ip_adapter_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(type="image", uri="", asset_id=None, data=None),
        description="When provided the image will be fed into the IP adapter",
    )
    ip_adapter_scale: float | OutputHandle[float] = connect_field(
        default=0.5, description="The strength of the IP adapter"
    )
    pag_scale: float | OutputHandle[float] = connect_field(
        default=3.0,
        description="Scale of the Perturbed-Attention Guidance applied to the image.",
    )
    latents: types.TorchTensor | OutputHandle[types.TorchTensor] = connect_field(
        default=types.TorchTensor(
            type="torch_tensor", value=None, dtype="<i8", shape=(1,)
        ),
        description="Optional initial latents to start generation from.",
    )
    enable_attention_slicing: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Enable attention slicing for the pipeline. This can reduce VRAM usage.",
    )
    enable_tiling: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Enable tiling for the VAE. This can reduce VRAM usage.",
    )
    enable_cpu_offload: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Enable CPU offload for the pipeline. This can reduce VRAM usage.",
    )
    output_type: (
        nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionBaseNode.StableDiffusionOutputType
    ) = Field(
        default=nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionBaseNode.StableDiffusionOutputType.IMAGE,
        description="The type of output to generate.",
    )
    controlnet: types.HFControlNet | OutputHandle[types.HFControlNet] = connect_field(
        default=types.HFControlNet(
            type="hf.controlnet",
            repo_id="",
            path=None,
            variant=None,
            allow_patterns=None,
            ignore_patterns=None,
        ),
        description="The ControlNet model to use for guidance.",
    )
    control_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(type="image", uri="", asset_id=None, data=None),
        description="The control image to guide the generation process.",
    )
    controlnet_conditioning_scale: float | OutputHandle[float] = connect_field(
        default=1.0, description="The scale for ControlNet conditioning."
    )

    @property
    def out(self) -> "StableDiffusionControlNetOutputs":
        return StableDiffusionControlNetOutputs(self)

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.huggingface.image_to_image.StableDiffusionControlNet

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


class StableDiffusionControlNetOutputs(OutputsProxy):
    @property
    def image(self) -> OutputHandle[nodetool.metadata.types.ImageRef]:
        return typing.cast(
            OutputHandle[nodetool.metadata.types.ImageRef], self["image"]
        )

    @property
    def latent(self) -> OutputHandle[nodetool.metadata.types.TorchTensor]:
        return typing.cast(
            OutputHandle[nodetool.metadata.types.TorchTensor], self["latent"]
        )


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.huggingface.image_to_image
from nodetool.workflows.base_node import BaseNode
import nodetool.nodes.huggingface.stable_diffusion_base


class StableDiffusionControlNetImg2ImgNode(
    GraphNode[
        nodetool.nodes.huggingface.image_to_image.StableDiffusionControlNetImg2ImgNode.OutputType
    ]
):
    """

    Transforms existing images using Stable Diffusion with ControlNet guidance.
    image, generation, image-to-image, controlnet, SD, style-transfer, ipadapter

    Use cases:
    - Modify existing images with precise control over composition and structure
    - Apply specific styles or concepts to photographs or artwork with guided transformations
    - Create variations of existing visual content while maintaining certain features
    - Enhance image editing capabilities with AI-guided transformations
    """

    ModelVariant: typing.ClassVar[type] = (
        nodetool.nodes.huggingface.stable_diffusion_base.ModelVariant
    )
    StableDiffusionScheduler: typing.ClassVar[type] = (
        nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionBaseNode.StableDiffusionScheduler
    )
    StableDiffusionOutputType: typing.ClassVar[type] = (
        nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionBaseNode.StableDiffusionOutputType
    )

    model: types.HFStableDiffusion | OutputHandle[types.HFStableDiffusion] = (
        connect_field(
            default=types.HFStableDiffusion(
                type="hf.stable_diffusion",
                repo_id="",
                path=None,
                variant=None,
                allow_patterns=None,
                ignore_patterns=None,
            ),
            description="The model to use for image generation.",
        )
    )
    variant: nodetool.nodes.huggingface.stable_diffusion_base.ModelVariant = Field(
        default=nodetool.nodes.huggingface.stable_diffusion_base.ModelVariant.FP16,
        description="The variant of the model to use for generation.",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt for image generation."
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="The negative prompt to guide what should not appear in the generated image.",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="Seed for the random number generator. Use -1 for a random seed.",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=25, description="Number of denoising steps."
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=7.5, description="Guidance scale for generation."
    )
    scheduler: (
        nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionBaseNode.StableDiffusionScheduler
    ) = Field(
        default=nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionBaseNode.StableDiffusionScheduler.EulerDiscreteScheduler,
        description="The scheduler to use for the diffusion process.",
    )
    loras: list[types.HFLoraSDConfig] | OutputHandle[list[types.HFLoraSDConfig]] = (
        connect_field(
            default=[], description="The LoRA models to use for image processing"
        )
    )
    ip_adapter_model: types.HFIPAdapter | OutputHandle[types.HFIPAdapter] = (
        connect_field(
            default=types.HFIPAdapter(
                type="hf.ip_adapter",
                repo_id="",
                path=None,
                variant=None,
                allow_patterns=None,
                ignore_patterns=None,
            ),
            description="The IP adapter model to use for image processing",
        )
    )
    ip_adapter_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(type="image", uri="", asset_id=None, data=None),
        description="When provided the image will be fed into the IP adapter",
    )
    ip_adapter_scale: float | OutputHandle[float] = connect_field(
        default=0.5, description="The strength of the IP adapter"
    )
    pag_scale: float | OutputHandle[float] = connect_field(
        default=3.0,
        description="Scale of the Perturbed-Attention Guidance applied to the image.",
    )
    latents: types.TorchTensor | OutputHandle[types.TorchTensor] = connect_field(
        default=types.TorchTensor(
            type="torch_tensor", value=None, dtype="<i8", shape=(1,)
        ),
        description="Optional initial latents to start generation from.",
    )
    enable_attention_slicing: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Enable attention slicing for the pipeline. This can reduce VRAM usage.",
    )
    enable_tiling: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Enable tiling for the VAE. This can reduce VRAM usage.",
    )
    enable_cpu_offload: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Enable CPU offload for the pipeline. This can reduce VRAM usage.",
    )
    output_type: (
        nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionBaseNode.StableDiffusionOutputType
    ) = Field(
        default=nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionBaseNode.StableDiffusionOutputType.IMAGE,
        description="The type of output to generate.",
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(type="image", uri="", asset_id=None, data=None),
        description="The input image to be transformed.",
    )
    strength: float | OutputHandle[float] = connect_field(
        default=0.5, description="Similarity to the input image"
    )
    controlnet: types.HFControlNet | OutputHandle[types.HFControlNet] = connect_field(
        default=types.HFControlNet(
            type="hf.controlnet",
            repo_id="",
            path=None,
            variant=None,
            allow_patterns=None,
            ignore_patterns=None,
        ),
        description="The ControlNet model to use for guidance.",
    )
    control_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(type="image", uri="", asset_id=None, data=None),
        description="The control image to guide the transformation.",
    )

    @property
    def out(self) -> "StableDiffusionControlNetImg2ImgNodeOutputs":
        return StableDiffusionControlNetImg2ImgNodeOutputs(self)

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return (
            nodetool.nodes.huggingface.image_to_image.StableDiffusionControlNetImg2ImgNode
        )

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


class StableDiffusionControlNetImg2ImgNodeOutputs(OutputsProxy):
    @property
    def image(self) -> OutputHandle[nodetool.metadata.types.ImageRef]:
        return typing.cast(
            OutputHandle[nodetool.metadata.types.ImageRef], self["image"]
        )

    @property
    def latent(self) -> OutputHandle[nodetool.metadata.types.TorchTensor]:
        return typing.cast(
            OutputHandle[nodetool.metadata.types.TorchTensor], self["latent"]
        )


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.huggingface.image_to_image
from nodetool.workflows.base_node import BaseNode
import nodetool.nodes.huggingface.stable_diffusion_base


class StableDiffusionControlNetInpaintNode(
    GraphNode[
        nodetool.nodes.huggingface.image_to_image.StableDiffusionControlNetInpaintNode.OutputType
    ]
):
    """

    Performs inpainting on images using Stable Diffusion with ControlNet guidance.
    image, inpainting, controlnet, SD, style-transfer, ipadapter

    Use cases:
    - Remove unwanted objects from images with precise control
    - Fill in missing parts of images guided by control images
    - Modify specific areas of images while preserving the rest and maintaining structure
    """

    ModelVariant: typing.ClassVar[type] = (
        nodetool.nodes.huggingface.stable_diffusion_base.ModelVariant
    )
    StableDiffusionScheduler: typing.ClassVar[type] = (
        nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionBaseNode.StableDiffusionScheduler
    )
    StableDiffusionOutputType: typing.ClassVar[type] = (
        nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionBaseNode.StableDiffusionOutputType
    )
    StableDiffusionControlNetModel: typing.ClassVar[type] = (
        nodetool.nodes.huggingface.image_to_image.StableDiffusionControlNetModel
    )

    model: types.HFStableDiffusion | OutputHandle[types.HFStableDiffusion] = (
        connect_field(
            default=types.HFStableDiffusion(
                type="hf.stable_diffusion",
                repo_id="",
                path=None,
                variant=None,
                allow_patterns=None,
                ignore_patterns=None,
            ),
            description="The model to use for image generation.",
        )
    )
    variant: nodetool.nodes.huggingface.stable_diffusion_base.ModelVariant = Field(
        default=nodetool.nodes.huggingface.stable_diffusion_base.ModelVariant.FP16,
        description="The variant of the model to use for generation.",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt for image generation."
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="The negative prompt to guide what should not appear in the generated image.",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="Seed for the random number generator. Use -1 for a random seed.",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=25, description="Number of denoising steps."
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=7.5, description="Guidance scale for generation."
    )
    scheduler: (
        nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionBaseNode.StableDiffusionScheduler
    ) = Field(
        default=nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionBaseNode.StableDiffusionScheduler.EulerDiscreteScheduler,
        description="The scheduler to use for the diffusion process.",
    )
    loras: list[types.HFLoraSDConfig] | OutputHandle[list[types.HFLoraSDConfig]] = (
        connect_field(
            default=[], description="The LoRA models to use for image processing"
        )
    )
    ip_adapter_model: types.HFIPAdapter | OutputHandle[types.HFIPAdapter] = (
        connect_field(
            default=types.HFIPAdapter(
                type="hf.ip_adapter",
                repo_id="",
                path=None,
                variant=None,
                allow_patterns=None,
                ignore_patterns=None,
            ),
            description="The IP adapter model to use for image processing",
        )
    )
    ip_adapter_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(type="image", uri="", asset_id=None, data=None),
        description="When provided the image will be fed into the IP adapter",
    )
    ip_adapter_scale: float | OutputHandle[float] = connect_field(
        default=0.5, description="The strength of the IP adapter"
    )
    pag_scale: float | OutputHandle[float] = connect_field(
        default=3.0,
        description="Scale of the Perturbed-Attention Guidance applied to the image.",
    )
    latents: types.TorchTensor | OutputHandle[types.TorchTensor] = connect_field(
        default=types.TorchTensor(
            type="torch_tensor", value=None, dtype="<i8", shape=(1,)
        ),
        description="Optional initial latents to start generation from.",
    )
    enable_attention_slicing: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Enable attention slicing for the pipeline. This can reduce VRAM usage.",
    )
    enable_tiling: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Enable tiling for the VAE. This can reduce VRAM usage.",
    )
    enable_cpu_offload: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Enable CPU offload for the pipeline. This can reduce VRAM usage.",
    )
    output_type: (
        nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionBaseNode.StableDiffusionOutputType
    ) = Field(
        default=nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionBaseNode.StableDiffusionOutputType.IMAGE,
        description="The type of output to generate.",
    )
    controlnet: (
        nodetool.nodes.huggingface.image_to_image.StableDiffusionControlNetModel
    ) = Field(
        default=nodetool.nodes.huggingface.image_to_image.StableDiffusionControlNetModel.INPAINT,
        description="The ControlNet model to use for guidance.",
    )
    init_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(type="image", uri="", asset_id=None, data=None),
        description="The initial image to be inpainted.",
    )
    mask_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(type="image", uri="", asset_id=None, data=None),
        description="The mask image indicating areas to be inpainted.",
    )
    control_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(type="image", uri="", asset_id=None, data=None),
        description="The control image to guide the inpainting process.",
    )
    controlnet_conditioning_scale: float | OutputHandle[float] = connect_field(
        default=0.5, description="The scale for ControlNet conditioning."
    )

    @property
    def out(self) -> "StableDiffusionControlNetInpaintNodeOutputs":
        return StableDiffusionControlNetInpaintNodeOutputs(self)

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return (
            nodetool.nodes.huggingface.image_to_image.StableDiffusionControlNetInpaintNode
        )

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


class StableDiffusionControlNetInpaintNodeOutputs(OutputsProxy):
    @property
    def image(self) -> OutputHandle[nodetool.metadata.types.ImageRef]:
        return typing.cast(
            OutputHandle[nodetool.metadata.types.ImageRef], self["image"]
        )

    @property
    def latent(self) -> OutputHandle[nodetool.metadata.types.TorchTensor]:
        return typing.cast(
            OutputHandle[nodetool.metadata.types.TorchTensor], self["latent"]
        )


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.huggingface.image_to_image
from nodetool.workflows.base_node import BaseNode
import nodetool.nodes.huggingface.stable_diffusion_base


class StableDiffusionImg2ImgNode(
    GraphNode[
        nodetool.nodes.huggingface.image_to_image.StableDiffusionImg2ImgNode.OutputType
    ]
):
    """

    Transforms existing images based on text prompts using Stable Diffusion.
    image, generation, image-to-image, SD, img2img, style-transfer, ipadapter

    Use cases:
    - Modifying existing images to fit a specific style or theme
    - Enhancing or altering photographs
    - Creating variations of existing artwork
    - Applying text-guided edits to images
    """

    ModelVariant: typing.ClassVar[type] = (
        nodetool.nodes.huggingface.stable_diffusion_base.ModelVariant
    )
    StableDiffusionScheduler: typing.ClassVar[type] = (
        nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionBaseNode.StableDiffusionScheduler
    )
    StableDiffusionOutputType: typing.ClassVar[type] = (
        nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionBaseNode.StableDiffusionOutputType
    )

    model: types.HFStableDiffusion | OutputHandle[types.HFStableDiffusion] = (
        connect_field(
            default=types.HFStableDiffusion(
                type="hf.stable_diffusion",
                repo_id="",
                path=None,
                variant=None,
                allow_patterns=None,
                ignore_patterns=None,
            ),
            description="The model to use for image generation.",
        )
    )
    variant: nodetool.nodes.huggingface.stable_diffusion_base.ModelVariant = Field(
        default=nodetool.nodes.huggingface.stable_diffusion_base.ModelVariant.FP16,
        description="The variant of the model to use for generation.",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt for image generation."
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="The negative prompt to guide what should not appear in the generated image.",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="Seed for the random number generator. Use -1 for a random seed.",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=25, description="Number of denoising steps."
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=7.5, description="Guidance scale for generation."
    )
    scheduler: (
        nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionBaseNode.StableDiffusionScheduler
    ) = Field(
        default=nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionBaseNode.StableDiffusionScheduler.EulerDiscreteScheduler,
        description="The scheduler to use for the diffusion process.",
    )
    loras: list[types.HFLoraSDConfig] | OutputHandle[list[types.HFLoraSDConfig]] = (
        connect_field(
            default=[], description="The LoRA models to use for image processing"
        )
    )
    ip_adapter_model: types.HFIPAdapter | OutputHandle[types.HFIPAdapter] = (
        connect_field(
            default=types.HFIPAdapter(
                type="hf.ip_adapter",
                repo_id="",
                path=None,
                variant=None,
                allow_patterns=None,
                ignore_patterns=None,
            ),
            description="The IP adapter model to use for image processing",
        )
    )
    ip_adapter_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(type="image", uri="", asset_id=None, data=None),
        description="When provided the image will be fed into the IP adapter",
    )
    ip_adapter_scale: float | OutputHandle[float] = connect_field(
        default=0.5, description="The strength of the IP adapter"
    )
    pag_scale: float | OutputHandle[float] = connect_field(
        default=3.0,
        description="Scale of the Perturbed-Attention Guidance applied to the image.",
    )
    latents: types.TorchTensor | OutputHandle[types.TorchTensor] = connect_field(
        default=types.TorchTensor(
            type="torch_tensor", value=None, dtype="<i8", shape=(1,)
        ),
        description="Optional initial latents to start generation from.",
    )
    enable_attention_slicing: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Enable attention slicing for the pipeline. This can reduce VRAM usage.",
    )
    enable_tiling: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Enable tiling for the VAE. This can reduce VRAM usage.",
    )
    enable_cpu_offload: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Enable CPU offload for the pipeline. This can reduce VRAM usage.",
    )
    output_type: (
        nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionBaseNode.StableDiffusionOutputType
    ) = Field(
        default=nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionBaseNode.StableDiffusionOutputType.IMAGE,
        description="The type of output to generate.",
    )
    init_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(type="image", uri="", asset_id=None, data=None),
        description="The initial image for Image-to-Image generation.",
    )
    strength: float | OutputHandle[float] = connect_field(
        default=0.8,
        description="Strength for Image-to-Image generation. Higher values allow for more deviation from the original image.",
    )

    @property
    def out(self) -> "StableDiffusionImg2ImgNodeOutputs":
        return StableDiffusionImg2ImgNodeOutputs(self)

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.huggingface.image_to_image.StableDiffusionImg2ImgNode

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


class StableDiffusionImg2ImgNodeOutputs(OutputsProxy):
    @property
    def image(self) -> OutputHandle[nodetool.metadata.types.ImageRef]:
        return typing.cast(
            OutputHandle[nodetool.metadata.types.ImageRef], self["image"]
        )

    @property
    def latent(self) -> OutputHandle[nodetool.metadata.types.TorchTensor]:
        return typing.cast(
            OutputHandle[nodetool.metadata.types.TorchTensor], self["latent"]
        )


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.huggingface.image_to_image
from nodetool.workflows.base_node import BaseNode
import nodetool.nodes.huggingface.stable_diffusion_base


class StableDiffusionInpaintNode(
    GraphNode[
        nodetool.nodes.huggingface.image_to_image.StableDiffusionInpaintNode.OutputType
    ]
):
    """

    Performs inpainting on images using Stable Diffusion.
    image, inpainting, SD

    Use cases:
    - Remove unwanted objects from images
    - Fill in missing parts of images
    - Modify specific areas of images while preserving the rest
    """

    ModelVariant: typing.ClassVar[type] = (
        nodetool.nodes.huggingface.stable_diffusion_base.ModelVariant
    )
    StableDiffusionScheduler: typing.ClassVar[type] = (
        nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionBaseNode.StableDiffusionScheduler
    )
    StableDiffusionOutputType: typing.ClassVar[type] = (
        nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionBaseNode.StableDiffusionOutputType
    )

    model: types.HFStableDiffusion | OutputHandle[types.HFStableDiffusion] = (
        connect_field(
            default=types.HFStableDiffusion(
                type="hf.stable_diffusion",
                repo_id="",
                path=None,
                variant=None,
                allow_patterns=None,
                ignore_patterns=None,
            ),
            description="The model to use for image generation.",
        )
    )
    variant: nodetool.nodes.huggingface.stable_diffusion_base.ModelVariant = Field(
        default=nodetool.nodes.huggingface.stable_diffusion_base.ModelVariant.FP16,
        description="The variant of the model to use for Image-to-Image generation.",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt for image generation."
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="The negative prompt to guide what should not appear in the generated image.",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="Seed for the random number generator. Use -1 for a random seed.",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=25, description="Number of denoising steps."
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=7.5, description="Guidance scale for generation."
    )
    scheduler: (
        nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionBaseNode.StableDiffusionScheduler
    ) = Field(
        default=nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionBaseNode.StableDiffusionScheduler.EulerDiscreteScheduler,
        description="The scheduler to use for the diffusion process.",
    )
    loras: list[types.HFLoraSDConfig] | OutputHandle[list[types.HFLoraSDConfig]] = (
        connect_field(
            default=[], description="The LoRA models to use for image processing"
        )
    )
    ip_adapter_model: types.HFIPAdapter | OutputHandle[types.HFIPAdapter] = (
        connect_field(
            default=types.HFIPAdapter(
                type="hf.ip_adapter",
                repo_id="",
                path=None,
                variant=None,
                allow_patterns=None,
                ignore_patterns=None,
            ),
            description="The IP adapter model to use for image processing",
        )
    )
    ip_adapter_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(type="image", uri="", asset_id=None, data=None),
        description="When provided the image will be fed into the IP adapter",
    )
    ip_adapter_scale: float | OutputHandle[float] = connect_field(
        default=0.5, description="The strength of the IP adapter"
    )
    pag_scale: float | OutputHandle[float] = connect_field(
        default=3.0,
        description="Scale of the Perturbed-Attention Guidance applied to the image.",
    )
    latents: types.TorchTensor | OutputHandle[types.TorchTensor] = connect_field(
        default=types.TorchTensor(
            type="torch_tensor", value=None, dtype="<i8", shape=(1,)
        ),
        description="Optional initial latents to start generation from.",
    )
    enable_attention_slicing: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Enable attention slicing for the pipeline. This can reduce VRAM usage.",
    )
    enable_tiling: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Enable tiling for the VAE. This can reduce VRAM usage.",
    )
    enable_cpu_offload: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Enable CPU offload for the pipeline. This can reduce VRAM usage.",
    )
    output_type: (
        nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionBaseNode.StableDiffusionOutputType
    ) = Field(
        default=nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionBaseNode.StableDiffusionOutputType.IMAGE,
        description="The type of output to generate.",
    )
    init_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(type="image", uri="", asset_id=None, data=None),
        description="The initial image to be inpainted.",
    )
    mask_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(type="image", uri="", asset_id=None, data=None),
        description="The mask image indicating areas to be inpainted.",
    )
    strength: float | OutputHandle[float] = connect_field(
        default=0.8,
        description="Strength for inpainting. Higher values allow for more deviation from the original image.",
    )

    @property
    def out(self) -> "StableDiffusionInpaintNodeOutputs":
        return StableDiffusionInpaintNodeOutputs(self)

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.huggingface.image_to_image.StableDiffusionInpaintNode

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


class StableDiffusionInpaintNodeOutputs(OutputsProxy):
    @property
    def image(self) -> OutputHandle[nodetool.metadata.types.ImageRef]:
        return typing.cast(
            OutputHandle[nodetool.metadata.types.ImageRef], self["image"]
        )

    @property
    def latent(self) -> OutputHandle[nodetool.metadata.types.TorchTensor]:
        return typing.cast(
            OutputHandle[nodetool.metadata.types.TorchTensor], self["latent"]
        )


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.huggingface.image_to_image
from nodetool.workflows.base_node import BaseNode


class StableDiffusionLatentUpscaler(
    SingleOutputGraphNode[types.TorchTensor], GraphNode[types.TorchTensor]
):
    """

    Upscales Stable Diffusion latents (x2) using the SD Latent Upscaler pipeline.
    tensor, upscaling, stable-diffusion, latent, SD

    Input and output are tensors for chaining with latent-based workflows.
    """

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt for upscaling guidance."
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="The negative prompt to guide what should not appear in the result.",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=10, description="Number of upscaling denoising steps."
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=0.0,
        description="Guidance scale for upscaling. 0 preserves content strongly.",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="Seed for the random number generator. Use -1 for a random seed.",
    )
    latents: types.TorchTensor | OutputHandle[types.TorchTensor] = connect_field(
        default=types.TorchTensor(
            type="torch_tensor", value=None, dtype="<i8", shape=(1,)
        ),
        description="Low-resolution latents tensor to upscale.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.huggingface.image_to_image.StableDiffusionLatentUpscaler

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.huggingface.image_to_image
from nodetool.workflows.base_node import BaseNode
import nodetool.nodes.huggingface.stable_diffusion_base


class StableDiffusionUpscale(
    SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]
):
    """

    Upscales an image using Stable Diffusion 4x upscaler.
    image, upscaling, stable-diffusion, SD

    Use cases:
    - Enhance low-resolution images
    - Improve image quality for printing or display
    - Create high-resolution versions of small images
    """

    StableDiffusionScheduler: typing.ClassVar[type] = (
        nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionBaseNode.StableDiffusionScheduler
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt for image generation."
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="The negative prompt to guide what should not appear in the generated image.",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=25, description="Number of upscaling steps."
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=7.5, description="Guidance scale for generation."
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(type="image", uri="", asset_id=None, data=None),
        description="The initial image for Image-to-Image generation.",
    )
    scheduler: (
        nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionBaseNode.StableDiffusionScheduler
    ) = Field(
        default=nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionBaseNode.StableDiffusionScheduler.HeunDiscreteScheduler,
        description="The scheduler to use for the diffusion process.",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="Seed for the random number generator. Use -1 for a random seed.",
    )
    enable_tiling: bool | OutputHandle[bool] = connect_field(
        default=False, description="Enable tiling to save VRAM"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.huggingface.image_to_image.StableDiffusionUpscale

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.huggingface.image_to_image
from nodetool.workflows.base_node import BaseNode
import nodetool.nodes.huggingface.stable_diffusion_base


class StableDiffusionXLControlNetNode(
    GraphNode[
        nodetool.nodes.huggingface.image_to_image.StableDiffusionXLControlNetNode.OutputType
    ]
):
    """

    Transforms existing images using Stable Diffusion XL with ControlNet guidance.
    image, generation, image-to-image, controlnet, SDXL

    Use cases:
    - Modify existing images with precise control over composition and structure
    - Apply specific styles or concepts to photographs or artwork with guided transformations
    - Create variations of existing visual content while maintaining certain features
    - Enhance image editing capabilities with AI-guided transformations
    """

    ModelVariant: typing.ClassVar[type] = (
        nodetool.nodes.huggingface.stable_diffusion_base.ModelVariant
    )
    StableDiffusionScheduler: typing.ClassVar[type] = (
        nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionXLBase.StableDiffusionScheduler
    )
    StableDiffusionOutputType: typing.ClassVar[type] = (
        nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionXLBase.StableDiffusionOutputType
    )

    model: types.HFStableDiffusionXL | OutputHandle[types.HFStableDiffusionXL] = (
        connect_field(
            default=types.HFStableDiffusionXL(
                type="hf.stable_diffusion_xl",
                repo_id="",
                path=None,
                variant=None,
                allow_patterns=None,
                ignore_patterns=None,
            ),
            description="The Stable Diffusion XL model to use for generation.",
        )
    )
    variant: nodetool.nodes.huggingface.stable_diffusion_base.ModelVariant = Field(
        default=nodetool.nodes.huggingface.stable_diffusion_base.ModelVariant.FP16,
        description="The variant of the model to use for generation.",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt for image generation."
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="The negative prompt to guide what should not appear in the generated image.",
    )
    width: int | OutputHandle[int] = connect_field(
        default=1024, description="Width of the generated image."
    )
    height: int | OutputHandle[int] = connect_field(
        default=1024, description="Height of the generated image"
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="Seed for the random number generator."
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=25, description="Number of inference steps."
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=7.0, description="Guidance scale for generation."
    )
    scheduler: (
        nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionXLBase.StableDiffusionScheduler
    ) = Field(
        default=nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionXLBase.StableDiffusionScheduler.EulerDiscreteScheduler,
        description="The scheduler to use for the diffusion process.",
    )
    pag_scale: float | OutputHandle[float] = connect_field(
        default=3.0,
        description="Scale of the Perturbed-Attention Guidance applied to the image.",
    )
    loras: list[types.HFLoraSDXLConfig] | OutputHandle[list[types.HFLoraSDXLConfig]] = (
        connect_field(
            default=[], description="The LoRA models to use for image processing"
        )
    )
    lora_scale: float | OutputHandle[float] = connect_field(
        default=0.5, description="Strength of the LoRAs"
    )
    ip_adapter_model: types.HFIPAdapter | OutputHandle[types.HFIPAdapter] = (
        connect_field(
            default=types.HFIPAdapter(
                type="hf.ip_adapter",
                repo_id="",
                path=None,
                variant=None,
                allow_patterns=None,
                ignore_patterns=None,
            ),
            description="The IP adapter model to use for image processing",
        )
    )
    ip_adapter_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(type="image", uri="", asset_id=None, data=None),
        description="When provided the image will be fed into the IP adapter",
    )
    ip_adapter_scale: float | OutputHandle[float] = connect_field(
        default=0.5, description="Strength of the IP adapter image"
    )
    enable_attention_slicing: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Enable attention slicing for the pipeline. This can reduce VRAM usage.",
    )
    enable_tiling: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Enable tiling for the VAE. This can reduce VRAM usage.",
    )
    enable_cpu_offload: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Enable CPU offload for the pipeline. This can reduce VRAM usage.",
    )
    output_type: (
        nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionXLBase.StableDiffusionOutputType
    ) = Field(
        default=nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionXLBase.StableDiffusionOutputType.IMAGE,
        description="The type of output to generate.",
    )
    init_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(type="image", uri="", asset_id=None, data=None),
        description="The initial image for Image-to-Image generation.",
    )
    strength: float | OutputHandle[float] = connect_field(
        default=0.8,
        description="Strength for Image-to-Image generation. Higher values allow for more deviation from the original image.",
    )
    controlnet: types.HFControlNet | OutputHandle[types.HFControlNet] = connect_field(
        default=types.HFControlNet(
            type="hf.controlnet",
            repo_id="",
            path=None,
            variant=None,
            allow_patterns=None,
            ignore_patterns=None,
        ),
        description="The ControlNet model to use for guidance.",
    )
    control_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(type="image", uri="", asset_id=None, data=None),
        description="The control image to guide the transformation.",
    )
    controlnet_conditioning_scale: float | OutputHandle[float] = connect_field(
        default=1.0, description="The scale for ControlNet conditioning."
    )

    @property
    def out(self) -> "StableDiffusionXLControlNetNodeOutputs":
        return StableDiffusionXLControlNetNodeOutputs(self)

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.huggingface.image_to_image.StableDiffusionXLControlNetNode

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


class StableDiffusionXLControlNetNodeOutputs(OutputsProxy):
    @property
    def image(self) -> OutputHandle[nodetool.metadata.types.ImageRef]:
        return typing.cast(
            OutputHandle[nodetool.metadata.types.ImageRef], self["image"]
        )

    @property
    def latent(self) -> OutputHandle[nodetool.metadata.types.TorchTensor]:
        return typing.cast(
            OutputHandle[nodetool.metadata.types.TorchTensor], self["latent"]
        )


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.huggingface.image_to_image
from nodetool.workflows.base_node import BaseNode
import nodetool.nodes.huggingface.stable_diffusion_base


class StableDiffusionXLImg2Img(
    GraphNode[
        nodetool.nodes.huggingface.image_to_image.StableDiffusionXLImg2Img.OutputType
    ]
):
    """

    Transforms existing images based on text prompts using Stable Diffusion XL.
    image, generation, image-to-image, SDXL, style-transfer, ipadapter

    Use cases:
    - Modifying existing images to fit a specific style or theme
    - Enhancing or altering photographs
    - Creating variations of existing artwork
    - Applying text-guided edits to images
    """

    ModelVariant: typing.ClassVar[type] = (
        nodetool.nodes.huggingface.stable_diffusion_base.ModelVariant
    )
    StableDiffusionScheduler: typing.ClassVar[type] = (
        nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionXLBase.StableDiffusionScheduler
    )
    StableDiffusionOutputType: typing.ClassVar[type] = (
        nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionXLBase.StableDiffusionOutputType
    )

    model: types.HFStableDiffusionXL | OutputHandle[types.HFStableDiffusionXL] = (
        connect_field(
            default=types.HFStableDiffusionXL(
                type="hf.stable_diffusion_xl",
                repo_id="",
                path=None,
                variant=None,
                allow_patterns=None,
                ignore_patterns=None,
            ),
            description="The Stable Diffusion XL model to use for generation.",
        )
    )
    variant: nodetool.nodes.huggingface.stable_diffusion_base.ModelVariant = Field(
        default=nodetool.nodes.huggingface.stable_diffusion_base.ModelVariant.FP16,
        description="The variant of the model to use for generation.",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt for image generation."
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="The negative prompt to guide what should not appear in the generated image.",
    )
    width: int | OutputHandle[int] = connect_field(
        default=1024, description="Width of the generated image."
    )
    height: int | OutputHandle[int] = connect_field(
        default=1024, description="Height of the generated image"
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="Seed for the random number generator."
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=25, description="Number of inference steps."
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=7.0, description="Guidance scale for generation."
    )
    scheduler: (
        nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionXLBase.StableDiffusionScheduler
    ) = Field(
        default=nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionXLBase.StableDiffusionScheduler.EulerDiscreteScheduler,
        description="The scheduler to use for the diffusion process.",
    )
    pag_scale: float | OutputHandle[float] = connect_field(
        default=3.0,
        description="Scale of the Perturbed-Attention Guidance applied to the image.",
    )
    loras: list[types.HFLoraSDXLConfig] | OutputHandle[list[types.HFLoraSDXLConfig]] = (
        connect_field(
            default=[], description="The LoRA models to use for image processing"
        )
    )
    lora_scale: float | OutputHandle[float] = connect_field(
        default=0.5, description="Strength of the LoRAs"
    )
    ip_adapter_model: types.HFIPAdapter | OutputHandle[types.HFIPAdapter] = (
        connect_field(
            default=types.HFIPAdapter(
                type="hf.ip_adapter",
                repo_id="",
                path=None,
                variant=None,
                allow_patterns=None,
                ignore_patterns=None,
            ),
            description="The IP adapter model to use for image processing",
        )
    )
    ip_adapter_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(type="image", uri="", asset_id=None, data=None),
        description="When provided the image will be fed into the IP adapter",
    )
    ip_adapter_scale: float | OutputHandle[float] = connect_field(
        default=0.5, description="Strength of the IP adapter image"
    )
    enable_attention_slicing: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Enable attention slicing for the pipeline. This can reduce VRAM usage.",
    )
    enable_tiling: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Enable tiling for the VAE. This can reduce VRAM usage.",
    )
    enable_cpu_offload: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Enable CPU offload for the pipeline. This can reduce VRAM usage.",
    )
    output_type: (
        nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionXLBase.StableDiffusionOutputType
    ) = Field(
        default=nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionXLBase.StableDiffusionOutputType.IMAGE,
        description="The type of output to generate.",
    )
    init_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(type="image", uri="", asset_id=None, data=None),
        description="The initial image for Image-to-Image generation.",
    )
    strength: float | OutputHandle[float] = connect_field(
        default=0.8,
        description="Strength for Image-to-Image generation. Higher values allow for more deviation from the original image.",
    )

    @property
    def out(self) -> "StableDiffusionXLImg2ImgOutputs":
        return StableDiffusionXLImg2ImgOutputs(self)

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.huggingface.image_to_image.StableDiffusionXLImg2Img

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


class StableDiffusionXLImg2ImgOutputs(OutputsProxy):
    @property
    def image(self) -> OutputHandle[nodetool.metadata.types.ImageRef]:
        return typing.cast(
            OutputHandle[nodetool.metadata.types.ImageRef], self["image"]
        )

    @property
    def latent(self) -> OutputHandle[nodetool.metadata.types.TorchTensor]:
        return typing.cast(
            OutputHandle[nodetool.metadata.types.TorchTensor], self["latent"]
        )


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.huggingface.image_to_image
from nodetool.workflows.base_node import BaseNode
import nodetool.nodes.huggingface.stable_diffusion_base


class StableDiffusionXLInpainting(
    GraphNode[
        nodetool.nodes.huggingface.image_to_image.StableDiffusionXLInpainting.OutputType
    ]
):
    """

    Performs inpainting on images using Stable Diffusion XL.
    image, inpainting, SDXL

    Use cases:
    - Remove unwanted objects from images
    - Fill in missing parts of images
    - Modify specific areas of images while preserving the rest
    """

    ModelVariant: typing.ClassVar[type] = (
        nodetool.nodes.huggingface.stable_diffusion_base.ModelVariant
    )
    StableDiffusionScheduler: typing.ClassVar[type] = (
        nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionXLBase.StableDiffusionScheduler
    )
    StableDiffusionOutputType: typing.ClassVar[type] = (
        nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionXLBase.StableDiffusionOutputType
    )

    model: types.HFStableDiffusionXL | OutputHandle[types.HFStableDiffusionXL] = (
        connect_field(
            default=types.HFStableDiffusionXL(
                type="hf.stable_diffusion_xl",
                repo_id="",
                path=None,
                variant=None,
                allow_patterns=None,
                ignore_patterns=None,
            ),
            description="The Stable Diffusion XL model to use for generation.",
        )
    )
    variant: nodetool.nodes.huggingface.stable_diffusion_base.ModelVariant = Field(
        default=nodetool.nodes.huggingface.stable_diffusion_base.ModelVariant.FP16,
        description="The variant of the model to use for generation.",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt for image generation."
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="The negative prompt to guide what should not appear in the generated image.",
    )
    width: int | OutputHandle[int] = connect_field(
        default=1024, description="Width of the generated image."
    )
    height: int | OutputHandle[int] = connect_field(
        default=1024, description="Height of the generated image"
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="Seed for the random number generator."
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=25, description="Number of inference steps."
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=7.0, description="Guidance scale for generation."
    )
    scheduler: (
        nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionXLBase.StableDiffusionScheduler
    ) = Field(
        default=nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionXLBase.StableDiffusionScheduler.EulerDiscreteScheduler,
        description="The scheduler to use for the diffusion process.",
    )
    pag_scale: float | OutputHandle[float] = connect_field(
        default=3.0,
        description="Scale of the Perturbed-Attention Guidance applied to the image.",
    )
    loras: list[types.HFLoraSDXLConfig] | OutputHandle[list[types.HFLoraSDXLConfig]] = (
        connect_field(
            default=[], description="The LoRA models to use for image processing"
        )
    )
    lora_scale: float | OutputHandle[float] = connect_field(
        default=0.5, description="Strength of the LoRAs"
    )
    ip_adapter_model: types.HFIPAdapter | OutputHandle[types.HFIPAdapter] = (
        connect_field(
            default=types.HFIPAdapter(
                type="hf.ip_adapter",
                repo_id="",
                path=None,
                variant=None,
                allow_patterns=None,
                ignore_patterns=None,
            ),
            description="The IP adapter model to use for image processing",
        )
    )
    ip_adapter_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(type="image", uri="", asset_id=None, data=None),
        description="When provided the image will be fed into the IP adapter",
    )
    ip_adapter_scale: float | OutputHandle[float] = connect_field(
        default=0.5, description="Strength of the IP adapter image"
    )
    enable_attention_slicing: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Enable attention slicing for the pipeline. This can reduce VRAM usage.",
    )
    enable_tiling: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Enable tiling for the VAE. This can reduce VRAM usage.",
    )
    enable_cpu_offload: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Enable CPU offload for the pipeline. This can reduce VRAM usage.",
    )
    output_type: (
        nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionXLBase.StableDiffusionOutputType
    ) = Field(
        default=nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionXLBase.StableDiffusionOutputType.IMAGE,
        description="The type of output to generate.",
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(type="image", uri="", asset_id=None, data=None),
        description="The initial image to be inpainted.",
    )
    mask_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(type="image", uri="", asset_id=None, data=None),
        description="The mask image indicating areas to be inpainted.",
    )
    strength: float | OutputHandle[float] = connect_field(
        default=0.8,
        description="Strength for inpainting. Higher values allow for more deviation from the original image.",
    )

    @property
    def out(self) -> "StableDiffusionXLInpaintingOutputs":
        return StableDiffusionXLInpaintingOutputs(self)

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.huggingface.image_to_image.StableDiffusionXLInpainting

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


class StableDiffusionXLInpaintingOutputs(OutputsProxy):
    @property
    def image(self) -> OutputHandle[nodetool.metadata.types.ImageRef]:
        return typing.cast(
            OutputHandle[nodetool.metadata.types.ImageRef], self["image"]
        )

    @property
    def latent(self) -> OutputHandle[nodetool.metadata.types.TorchTensor]:
        return typing.cast(
            OutputHandle[nodetool.metadata.types.TorchTensor], self["latent"]
        )


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.huggingface.image_to_image
from nodetool.workflows.base_node import BaseNode


class Swin2SR(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """

    Performs image super-resolution using the Swin2SR model.
    image, super-resolution, enhancement, huggingface

    Use cases:
    - Enhance low-resolution images
    - Improve image quality for printing or display
    - Upscale images for better detail
    """

    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(type="image", uri="", asset_id=None, data=None),
        description="The input image to transform",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="The text prompt to guide the image transformation (if applicable)",
    )
    model: types.HFImageToImage | OutputHandle[types.HFImageToImage] = connect_field(
        default=types.HFImageToImage(
            type="hf.image_to_image",
            repo_id="",
            path=None,
            variant=None,
            allow_patterns=None,
            ignore_patterns=None,
        ),
        description="The model ID to use for image super-resolution",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.huggingface.image_to_image.Swin2SR

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.huggingface.image_to_image
from nodetool.workflows.base_node import BaseNode


class VAEDecode(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """

    Decodes latents into an image using a VAE.
    tensor (TorchTensor) -> image
    """

    model: types.HFVAE | OutputHandle[types.HFVAE] = connect_field(
        default=types.HFVAE(
            type="hf.vae",
            repo_id="",
            path=None,
            variant=None,
            allow_patterns=None,
            ignore_patterns=None,
        ),
        description="The VAE model to use.",
    )
    latents: types.TorchTensor | OutputHandle[types.TorchTensor] = connect_field(
        default=types.TorchTensor(
            type="torch_tensor", value=None, dtype="<i8", shape=(1,)
        ),
        description="Latent tensor to decode.",
    )
    scale_factor: float | OutputHandle[float] = connect_field(
        default=0.18215,
        description="Scaling factor used for encoding (inverse is applied before decode)",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.huggingface.image_to_image.VAEDecode

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.huggingface.image_to_image
from nodetool.workflows.base_node import BaseNode


class VAEEncode(SingleOutputGraphNode[types.TorchTensor], GraphNode[types.TorchTensor]):
    """

    Encodes an image into latents using a VAE.
    image -> tensor (TorchTensor)
    """

    model: types.HFVAE | OutputHandle[types.HFVAE] = connect_field(
        default=types.HFVAE(
            type="hf.vae",
            repo_id="",
            path=None,
            variant=None,
            allow_patterns=None,
            ignore_patterns=None,
        ),
        description="The VAE model to use.",
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(type="image", uri="", asset_id=None, data=None),
        description="Input image to encode.",
    )
    scale_factor: float | OutputHandle[float] = connect_field(
        default=0.18215,
        description="Scaling factor applied to latents (e.g., 0.18215 for SD15)",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.huggingface.image_to_image.VAEEncode

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()
