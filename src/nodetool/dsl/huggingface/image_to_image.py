# This file is auto-generated by nodetool.dsl.codegen.
# Please do not edit this file manually.

# Instead, edit the node class in the source module and run the following commands to regenerate the DSL:
# nodetool package scan
# nodetool codegen

from pydantic import BaseModel, Field
import typing
from typing import Any
import nodetool.metadata.types
import nodetool.metadata.types as types
from nodetool.dsl.graph import GraphNode, SingleOutputGraphNode

import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.huggingface.image_to_image
from nodetool.workflows.base_node import BaseNode


class FluxFill(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """

    Performs image inpainting/filling using FLUX Fill models with support for GGUF quantization.
    image, inpainting, fill, flux, quantization, mask

    Use cases:
    - Fill masked regions in images with high-quality content
    - Remove unwanted objects from images
    - Complete missing parts of images
    - Memory-efficient inpainting using GGUF quantization
    - High-quality image editing with FLUX models
    """

    FluxFillQuantization: typing.ClassVar[type] = (
        nodetool.nodes.huggingface.image_to_image.FluxFillQuantization
    )

    model: types.HFFluxFill | OutputHandle[types.HFFluxFill] = connect_field(
        default=types.HFFluxFill(
            type="hf.inpainting",
            repo_id="black-forest-labs/FLUX.1-Fill-dev",
            path=None,
            variant=None,
            allow_patterns=None,
            ignore_patterns=None,
        ),
        description="The FLUX Fill model to use for image inpainting.",
    )
    quantization: nodetool.nodes.huggingface.image_to_image.FluxFillQuantization = (
        Field(
            default=nodetool.nodes.huggingface.image_to_image.FluxFillQuantization.FP16,
            description="Quantization level for the FLUX Fill transformer.",
        )
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="a white paper cup",
        description="A text prompt describing what should fill the masked area.",
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The input image to fill/inpaint",
    )
    mask_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The mask image indicating areas to be filled (white areas will be filled)",
    )
    height: int | OutputHandle[int] = connect_field(
        default=1024, description="The height of the generated image."
    )
    width: int | OutputHandle[int] = connect_field(
        default=1024, description="The width of the generated image."
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=30.0,
        description="Guidance scale for generation. Higher values follow the prompt more closely",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=50, description="Number of denoising steps"
    )
    max_sequence_length: int | OutputHandle[int] = connect_field(
        default=512, description="Maximum sequence length for the prompt."
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="Seed for the random number generator. Use -1 for a random seed.",
    )
    enable_cpu_offload: bool | OutputHandle[bool] = connect_field(
        default=True, description="Enable CPU offload to reduce VRAM usage."
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.huggingface.image_to_image.FluxFill

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.huggingface.image_to_image
from nodetool.workflows.base_node import BaseNode


class FluxKontext(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """

    Performs image editing using FLUX Kontext models for context-aware image generation.
    image, editing, flux, kontext, context-aware, generation

    Use cases:
    - Edit images based on reference context
    - Add elements to images guided by prompts
    - Context-aware image modifications
    - High-quality image editing with FLUX models
    """

    FluxKontextQuantization: typing.ClassVar[type] = (
        nodetool.nodes.huggingface.image_to_image.FluxKontextQuantization
    )

    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The input image to edit",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="Add a hat to the cat",
        description="Text description of the desired edit to apply to the image",
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=2.5,
        description="Guidance scale for editing. Higher values follow the prompt more closely",
    )
    quantization: nodetool.nodes.huggingface.image_to_image.FluxKontextQuantization = (
        Field(
            default=nodetool.nodes.huggingface.image_to_image.FluxKontextQuantization.INT4,
            description="Quantization level for the FLUX Kontext transformer.",
        )
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="Seed for the random number generator. Use -1 for a random seed",
    )
    enable_cpu_offload: bool | OutputHandle[bool] = connect_field(
        default=True, description="Enable CPU offload to reduce VRAM usage."
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.huggingface.image_to_image.FluxKontext

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.huggingface.image_to_image
from nodetool.workflows.base_node import BaseNode


class ImageToImage(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """

    Transforms existing images based on text prompts using AutoPipeline for Image-to-Image.
    This node automatically detects the appropriate pipeline class based on the model used.
    image, generation, image-to-image, autopipeline

    Use cases:
    - Transform existing images with any compatible model (Stable Diffusion, SDXL, Kandinsky, etc.)
    - Apply specific styles or concepts to photographs or artwork
    - Modify existing images based on text descriptions
    - Create variations of existing visual content with automatic pipeline selection
    """

    model: types.HFImageToImage | OutputHandle[types.HFImageToImage] = connect_field(
        default=types.HFImageToImage(
            type="hf.image_to_image",
            repo_id="runwayml/stable-diffusion-v1-5",
            path=None,
            variant=None,
            allow_patterns=["*.safetensors", "*.bin", "*.json", "**/*.json"],
            ignore_patterns=None,
        ),
        description="The HuggingFace model to use for image-to-image generation.",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="A beautiful landscape with mountains and a lake at sunset",
        description="Text prompt describing the desired image transformation.",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="Text prompt describing what should not appear in the generated image.",
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The input image to transform",
    )
    strength: float | OutputHandle[float] = connect_field(
        default=0.8,
        description="Strength of the transformation. Higher values allow for more deviation from the original image.",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=25, description="Number of denoising steps."
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=7.5,
        description="Guidance scale for generation. Higher values follow the prompt more closely.",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="Seed for the random number generator. Use -1 for a random seed.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.huggingface.image_to_image.ImageToImage

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.huggingface.image_to_image
from nodetool.workflows.base_node import BaseNode


class LoadImageToImageModel(
    SingleOutputGraphNode[types.HFImageToImage], GraphNode[types.HFImageToImage]
):
    """

    Load HuggingFace model for image-to-image generation from a repo_id.

    Use cases:
    - Loads a pipeline directly from a repo_id
    - Used for ImageToImage node
    """

    repo_id: str | OutputHandle[str] = connect_field(
        default="runwayml/stable-diffusion-v1-5",
        description="The repository ID of the model to use for image-to-image generation.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.huggingface.image_to_image.LoadImageToImageModel

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.huggingface.image_to_image
from nodetool.workflows.base_node import BaseNode


class OmniGenNode(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """

    Generates and edits images using the OmniGen model, supporting multimodal inputs.
    image, generation, text-to-image, image-editing, multimodal, omnigen

    Use cases:
    - Generate images from text prompts
    - Edit existing images with text instructions
    - Controllable image generation with reference images
    - Visual reasoning and image manipulation
    - ID and object preserving generation
    """

    prompt: str | OutputHandle[str] = connect_field(
        default="A realistic photo of a young woman sitting on a sofa, holding a book and facing the camera.",
        description="The text prompt for image generation. Use <img><|image_1|></img> placeholders to reference input images.",
    )
    input_images: list[types.ImageRef] | OutputHandle[list[types.ImageRef]] = (
        connect_field(
            default=[],
            description="List of input images to use for editing or as reference. Referenced in prompt using <img><|image_1|></img>, <img><|image_2|></img>, etc.",
        )
    )
    height: int | OutputHandle[int] = connect_field(
        default=1024, description="Height of the generated image."
    )
    width: int | OutputHandle[int] = connect_field(
        default=1024, description="Width of the generated image."
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=2.5,
        description="Guidance scale for generation. Higher values follow the prompt more closely.",
    )
    img_guidance_scale: float | OutputHandle[float] = connect_field(
        default=1.6, description="Image guidance scale when using input images."
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=25, description="Number of denoising steps."
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="Seed for the random number generator. Use -1 for a random seed.",
    )
    use_input_image_size_as_output: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If True, use the input image size as output size. Recommended for image editing.",
    )
    max_input_image_size: int | OutputHandle[int] = connect_field(
        default=1024,
        description="Maximum input image size. Smaller values reduce memory usage but may affect quality.",
    )
    enable_model_cpu_offload: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Enable CPU offload to reduce memory usage when using multiple images.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.huggingface.image_to_image.OmniGenNode

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.huggingface.image_to_image
from nodetool.workflows.base_node import BaseNode


class QwenImageEdit(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """

    Performs image editing using the Qwen Image Edit model with support for Nunchaku quantization.
    image, editing, semantic, appearance, qwen, multimodal, quantization

    Use cases:
    - Semantic editing (object rotation, style transfer)
    - Appearance editing (adding/removing elements)
    - Precise text modifications in images
    - Background and clothing changes
    - Complex image transformations guided by text
    - Memory-efficient editing using Nunchaku quantization
    """

    QwenImageEditQuantization: typing.ClassVar[type] = (
        nodetool.nodes.huggingface.image_to_image.QwenImageEditQuantization
    )

    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The input image to edit",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="Change the object's color to blue",
        description="Text description of the desired edit to apply to the image",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="Text describing what should not appear in the edited image",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=50, description="Number of denoising steps for the editing process"
    )
    true_cfg_scale: float | OutputHandle[float] = connect_field(
        default=4.0,
        description="Guidance scale for editing. Higher values follow the prompt more closely",
    )
    quantization: (
        nodetool.nodes.huggingface.image_to_image.QwenImageEditQuantization
    ) = Field(
        default=nodetool.nodes.huggingface.image_to_image.QwenImageEditQuantization.INT4,
        description="Quantization level for the Qwen Image Edit transformer.",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="Seed for the random number generator. Use -1 for a random seed",
    )
    enable_memory_efficient_attention: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Enable memory efficient attention to reduce VRAM usage.",
    )
    enable_cpu_offload: bool | OutputHandle[bool] = connect_field(
        default=True, description="Enable CPU offload to reduce VRAM usage."
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.huggingface.image_to_image.QwenImageEdit

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.huggingface.image_to_image
from nodetool.workflows.base_node import BaseNode


class RealESRGANNode(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """

    Performs image super-resolution using the RealESRGAN model.
    image, super-resolution, enhancement, huggingface

    Use cases:
    - Enhance low-resolution images
    - Improve image quality for printing or display
    - Upscale images for better detail
    """

    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The input image to transform",
    )
    model: types.HFRealESRGAN | OutputHandle[types.HFRealESRGAN] = connect_field(
        default=types.HFRealESRGAN(
            type="hf.real_esrgan",
            repo_id="",
            path=None,
            variant=None,
            allow_patterns=None,
            ignore_patterns=None,
        ),
        description="The RealESRGAN model to use for image super-resolution",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.huggingface.image_to_image.RealESRGANNode

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.huggingface.image_to_image
from nodetool.workflows.base_node import BaseNode
import nodetool.nodes.huggingface.stable_diffusion_base


class StableDiffusionControlNet(
    GraphNode[
        nodetool.nodes.huggingface.image_to_image.StableDiffusionControlNet.OutputType
    ]
):
    """

    Generates images using Stable Diffusion with ControlNet guidance.
    image, generation, text-to-image, controlnet, SD

    Use cases:
    - Generate images with precise control over composition and structure
    - Create variations of existing images while maintaining specific features
    - Artistic image generation with guided outputs
    """

    StableDiffusionScheduler: typing.ClassVar[type] = (
        nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionBaseNode.StableDiffusionScheduler
    )
    StableDiffusionOutputType: typing.ClassVar[type] = (
        nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionBaseNode.StableDiffusionOutputType
    )

    model: types.HFStableDiffusion | OutputHandle[types.HFStableDiffusion] = (
        connect_field(
            default=types.HFStableDiffusion(
                type="hf.stable_diffusion",
                repo_id="",
                path=None,
                variant=None,
                allow_patterns=None,
                ignore_patterns=None,
            ),
            description="The model to use for image generation.",
        )
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt for image generation."
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="The negative prompt to guide what should not appear in the generated image.",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="Seed for the random number generator. Use -1 for a random seed.",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=25, description="Number of denoising steps."
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=7.5, description="Guidance scale for generation."
    )
    scheduler: (
        nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionBaseNode.StableDiffusionScheduler
    ) = Field(
        default=nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionBaseNode.StableDiffusionScheduler.EulerDiscreteScheduler,
        description="The scheduler to use for the diffusion process.",
    )
    loras: list[types.HFLoraSDConfig] | OutputHandle[list[types.HFLoraSDConfig]] = (
        connect_field(
            default=[], description="The LoRA models to use for image processing"
        )
    )
    ip_adapter_model: types.HFIPAdapter | OutputHandle[types.HFIPAdapter] = (
        connect_field(
            default=types.HFIPAdapter(
                type="hf.ip_adapter",
                repo_id="",
                path=None,
                variant=None,
                allow_patterns=None,
                ignore_patterns=None,
            ),
            description="The IP adapter model to use for image processing",
        )
    )
    ip_adapter_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="When provided the image will be fed into the IP adapter",
    )
    ip_adapter_scale: float | OutputHandle[float] = connect_field(
        default=0.5, description="The strength of the IP adapter"
    )
    latents: types.TorchTensor | OutputHandle[types.TorchTensor] = connect_field(
        default=types.TorchTensor(
            type="torch_tensor", value=None, dtype="<i8", shape=(1,)
        ),
        description="Optional initial latents to start generation from.",
    )
    enable_attention_slicing: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Enable attention slicing for the pipeline. This can reduce VRAM usage.",
    )
    enable_tiling: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Legacy VAE tiling flag (disabled in favor of PyTorch 2 attention optimizations).",
    )
    enable_cpu_offload: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Enable CPU offload for the pipeline. This can reduce VRAM usage.",
    )
    output_type: (
        nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionBaseNode.StableDiffusionOutputType
    ) = Field(
        default=nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionBaseNode.StableDiffusionOutputType.IMAGE,
        description="The type of output to generate.",
    )
    controlnet: types.HFControlNet | OutputHandle[types.HFControlNet] = connect_field(
        default=types.HFControlNet(
            type="hf.controlnet",
            repo_id="",
            path=None,
            variant=None,
            allow_patterns=None,
            ignore_patterns=None,
        ),
        description="The ControlNet model to use for guidance.",
    )
    control_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The control image to guide the generation process.",
    )
    controlnet_conditioning_scale: float | OutputHandle[float] = connect_field(
        default=1.0, description="The scale for ControlNet conditioning."
    )

    @property
    def out(self) -> "StableDiffusionControlNetOutputs":
        return StableDiffusionControlNetOutputs(self)

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.huggingface.image_to_image.StableDiffusionControlNet

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


class StableDiffusionControlNetOutputs(OutputsProxy):
    @property
    def image(self) -> OutputHandle[typing.Any]:
        return typing.cast(OutputHandle[typing.Any], self["image"])

    @property
    def latent(self) -> OutputHandle[typing.Any]:
        return typing.cast(OutputHandle[typing.Any], self["latent"])


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.huggingface.image_to_image
from nodetool.workflows.base_node import BaseNode
import nodetool.nodes.huggingface.stable_diffusion_base


class StableDiffusionControlNetImg2ImgNode(
    GraphNode[
        nodetool.nodes.huggingface.image_to_image.StableDiffusionControlNetImg2ImgNode.OutputType
    ]
):
    """

    Transforms existing images using Stable Diffusion with ControlNet guidance.
    image, generation, image-to-image, controlnet, SD, style-transfer

    Use cases:
    - Modify existing images with precise control over composition and structure
    - Apply specific styles or concepts to photographs or artwork with guided transformations
    - Create variations of existing visual content while maintaining certain features
    - Enhance image editing capabilities with AI-guided transformations
    """

    StableDiffusionScheduler: typing.ClassVar[type] = (
        nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionBaseNode.StableDiffusionScheduler
    )
    StableDiffusionOutputType: typing.ClassVar[type] = (
        nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionBaseNode.StableDiffusionOutputType
    )

    model: types.HFStableDiffusion | OutputHandle[types.HFStableDiffusion] = (
        connect_field(
            default=types.HFStableDiffusion(
                type="hf.stable_diffusion",
                repo_id="",
                path=None,
                variant=None,
                allow_patterns=None,
                ignore_patterns=None,
            ),
            description="The model to use for image generation.",
        )
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt for image generation."
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="The negative prompt to guide what should not appear in the generated image.",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="Seed for the random number generator. Use -1 for a random seed.",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=25, description="Number of denoising steps."
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=7.5, description="Guidance scale for generation."
    )
    scheduler: (
        nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionBaseNode.StableDiffusionScheduler
    ) = Field(
        default=nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionBaseNode.StableDiffusionScheduler.EulerDiscreteScheduler,
        description="The scheduler to use for the diffusion process.",
    )
    loras: list[types.HFLoraSDConfig] | OutputHandle[list[types.HFLoraSDConfig]] = (
        connect_field(
            default=[], description="The LoRA models to use for image processing"
        )
    )
    ip_adapter_model: types.HFIPAdapter | OutputHandle[types.HFIPAdapter] = (
        connect_field(
            default=types.HFIPAdapter(
                type="hf.ip_adapter",
                repo_id="",
                path=None,
                variant=None,
                allow_patterns=None,
                ignore_patterns=None,
            ),
            description="The IP adapter model to use for image processing",
        )
    )
    ip_adapter_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="When provided the image will be fed into the IP adapter",
    )
    ip_adapter_scale: float | OutputHandle[float] = connect_field(
        default=0.5, description="The strength of the IP adapter"
    )
    latents: types.TorchTensor | OutputHandle[types.TorchTensor] = connect_field(
        default=types.TorchTensor(
            type="torch_tensor", value=None, dtype="<i8", shape=(1,)
        ),
        description="Optional initial latents to start generation from.",
    )
    enable_attention_slicing: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Enable attention slicing for the pipeline. This can reduce VRAM usage.",
    )
    enable_tiling: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Legacy VAE tiling flag (disabled in favor of PyTorch 2 attention optimizations).",
    )
    enable_cpu_offload: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Enable CPU offload for the pipeline. This can reduce VRAM usage.",
    )
    output_type: (
        nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionBaseNode.StableDiffusionOutputType
    ) = Field(
        default=nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionBaseNode.StableDiffusionOutputType.IMAGE,
        description="The type of output to generate.",
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The input image to be transformed.",
    )
    strength: float | OutputHandle[float] = connect_field(
        default=0.5, description="Similarity to the input image"
    )
    controlnet: types.HFControlNet | OutputHandle[types.HFControlNet] = connect_field(
        default=types.HFControlNet(
            type="hf.controlnet",
            repo_id="",
            path=None,
            variant=None,
            allow_patterns=None,
            ignore_patterns=None,
        ),
        description="The ControlNet model to use for guidance.",
    )
    control_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The control image to guide the transformation.",
    )

    @property
    def out(self) -> "StableDiffusionControlNetImg2ImgNodeOutputs":
        return StableDiffusionControlNetImg2ImgNodeOutputs(self)

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return (
            nodetool.nodes.huggingface.image_to_image.StableDiffusionControlNetImg2ImgNode
        )

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


class StableDiffusionControlNetImg2ImgNodeOutputs(OutputsProxy):
    @property
    def image(self) -> OutputHandle[typing.Any]:
        return typing.cast(OutputHandle[typing.Any], self["image"])

    @property
    def latent(self) -> OutputHandle[typing.Any]:
        return typing.cast(OutputHandle[typing.Any], self["latent"])


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.huggingface.image_to_image
from nodetool.workflows.base_node import BaseNode
import nodetool.nodes.huggingface.stable_diffusion_base


class StableDiffusionControlNetInpaintNode(
    GraphNode[
        nodetool.nodes.huggingface.image_to_image.StableDiffusionControlNetInpaintNode.OutputType
    ]
):
    """

    Performs inpainting on images using Stable Diffusion with ControlNet guidance.
    image, inpainting, controlnet, SD, style-transfer

    Use cases:
    - Remove unwanted objects from images with precise control
    - Fill in missing parts of images guided by control images
    - Modify specific areas of images while preserving the rest and maintaining structure
    """

    StableDiffusionScheduler: typing.ClassVar[type] = (
        nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionBaseNode.StableDiffusionScheduler
    )
    StableDiffusionOutputType: typing.ClassVar[type] = (
        nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionBaseNode.StableDiffusionOutputType
    )
    StableDiffusionControlNetModel: typing.ClassVar[type] = (
        nodetool.nodes.huggingface.image_to_image.StableDiffusionControlNetModel
    )

    model: types.HFStableDiffusion | OutputHandle[types.HFStableDiffusion] = (
        connect_field(
            default=types.HFStableDiffusion(
                type="hf.stable_diffusion",
                repo_id="",
                path=None,
                variant=None,
                allow_patterns=None,
                ignore_patterns=None,
            ),
            description="The model to use for image generation.",
        )
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt for image generation."
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="The negative prompt to guide what should not appear in the generated image.",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="Seed for the random number generator. Use -1 for a random seed.",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=25, description="Number of denoising steps."
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=7.5, description="Guidance scale for generation."
    )
    scheduler: (
        nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionBaseNode.StableDiffusionScheduler
    ) = Field(
        default=nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionBaseNode.StableDiffusionScheduler.EulerDiscreteScheduler,
        description="The scheduler to use for the diffusion process.",
    )
    loras: list[types.HFLoraSDConfig] | OutputHandle[list[types.HFLoraSDConfig]] = (
        connect_field(
            default=[], description="The LoRA models to use for image processing"
        )
    )
    ip_adapter_model: types.HFIPAdapter | OutputHandle[types.HFIPAdapter] = (
        connect_field(
            default=types.HFIPAdapter(
                type="hf.ip_adapter",
                repo_id="",
                path=None,
                variant=None,
                allow_patterns=None,
                ignore_patterns=None,
            ),
            description="The IP adapter model to use for image processing",
        )
    )
    ip_adapter_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="When provided the image will be fed into the IP adapter",
    )
    ip_adapter_scale: float | OutputHandle[float] = connect_field(
        default=0.5, description="The strength of the IP adapter"
    )
    latents: types.TorchTensor | OutputHandle[types.TorchTensor] = connect_field(
        default=types.TorchTensor(
            type="torch_tensor", value=None, dtype="<i8", shape=(1,)
        ),
        description="Optional initial latents to start generation from.",
    )
    enable_attention_slicing: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Enable attention slicing for the pipeline. This can reduce VRAM usage.",
    )
    enable_tiling: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Legacy VAE tiling flag (disabled in favor of PyTorch 2 attention optimizations).",
    )
    enable_cpu_offload: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Enable CPU offload for the pipeline. This can reduce VRAM usage.",
    )
    output_type: (
        nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionBaseNode.StableDiffusionOutputType
    ) = Field(
        default=nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionBaseNode.StableDiffusionOutputType.IMAGE,
        description="The type of output to generate.",
    )
    controlnet: (
        nodetool.nodes.huggingface.image_to_image.StableDiffusionControlNetModel
    ) = Field(
        default=nodetool.nodes.huggingface.image_to_image.StableDiffusionControlNetModel.INPAINT,
        description="The ControlNet model to use for guidance.",
    )
    init_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The initial image to be inpainted.",
    )
    mask_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The mask image indicating areas to be inpainted.",
    )
    control_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The control image to guide the inpainting process.",
    )
    controlnet_conditioning_scale: float | OutputHandle[float] = connect_field(
        default=0.5, description="The scale for ControlNet conditioning."
    )

    @property
    def out(self) -> "StableDiffusionControlNetInpaintNodeOutputs":
        return StableDiffusionControlNetInpaintNodeOutputs(self)

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return (
            nodetool.nodes.huggingface.image_to_image.StableDiffusionControlNetInpaintNode
        )

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


class StableDiffusionControlNetInpaintNodeOutputs(OutputsProxy):
    @property
    def image(self) -> OutputHandle[typing.Any]:
        return typing.cast(OutputHandle[typing.Any], self["image"])

    @property
    def latent(self) -> OutputHandle[typing.Any]:
        return typing.cast(OutputHandle[typing.Any], self["latent"])


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.huggingface.image_to_image
from nodetool.workflows.base_node import BaseNode
import nodetool.nodes.huggingface.stable_diffusion_base


class StableDiffusionImg2ImgNode(
    GraphNode[
        nodetool.nodes.huggingface.image_to_image.StableDiffusionImg2ImgNode.OutputType
    ]
):
    """

    Transforms existing images based on text prompts using Stable Diffusion.
    image, generation, image-to-image, SD, img2img, style-transfer

    Use cases:
    - Modifying existing images to fit a specific style or theme
    - Enhancing or altering photographs
    - Creating variations of existing artwork
    - Applying text-guided edits to images
    """

    StableDiffusionScheduler: typing.ClassVar[type] = (
        nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionBaseNode.StableDiffusionScheduler
    )
    StableDiffusionOutputType: typing.ClassVar[type] = (
        nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionBaseNode.StableDiffusionOutputType
    )

    model: types.HFStableDiffusion | OutputHandle[types.HFStableDiffusion] = (
        connect_field(
            default=types.HFStableDiffusion(
                type="hf.stable_diffusion",
                repo_id="",
                path=None,
                variant=None,
                allow_patterns=None,
                ignore_patterns=None,
            ),
            description="The model to use for image generation.",
        )
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt for image generation."
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="The negative prompt to guide what should not appear in the generated image.",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="Seed for the random number generator. Use -1 for a random seed.",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=25, description="Number of denoising steps."
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=7.5, description="Guidance scale for generation."
    )
    scheduler: (
        nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionBaseNode.StableDiffusionScheduler
    ) = Field(
        default=nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionBaseNode.StableDiffusionScheduler.EulerDiscreteScheduler,
        description="The scheduler to use for the diffusion process.",
    )
    loras: list[types.HFLoraSDConfig] | OutputHandle[list[types.HFLoraSDConfig]] = (
        connect_field(
            default=[], description="The LoRA models to use for image processing"
        )
    )
    ip_adapter_model: types.HFIPAdapter | OutputHandle[types.HFIPAdapter] = (
        connect_field(
            default=types.HFIPAdapter(
                type="hf.ip_adapter",
                repo_id="",
                path=None,
                variant=None,
                allow_patterns=None,
                ignore_patterns=None,
            ),
            description="The IP adapter model to use for image processing",
        )
    )
    ip_adapter_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="When provided the image will be fed into the IP adapter",
    )
    ip_adapter_scale: float | OutputHandle[float] = connect_field(
        default=0.5, description="The strength of the IP adapter"
    )
    latents: types.TorchTensor | OutputHandle[types.TorchTensor] = connect_field(
        default=types.TorchTensor(
            type="torch_tensor", value=None, dtype="<i8", shape=(1,)
        ),
        description="Optional initial latents to start generation from.",
    )
    enable_attention_slicing: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Enable attention slicing for the pipeline. This can reduce VRAM usage.",
    )
    enable_tiling: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Legacy VAE tiling flag (disabled in favor of PyTorch 2 attention optimizations).",
    )
    enable_cpu_offload: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Enable CPU offload for the pipeline. This can reduce VRAM usage.",
    )
    output_type: (
        nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionBaseNode.StableDiffusionOutputType
    ) = Field(
        default=nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionBaseNode.StableDiffusionOutputType.IMAGE,
        description="The type of output to generate.",
    )
    init_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The initial image for Image-to-Image generation.",
    )
    strength: float | OutputHandle[float] = connect_field(
        default=0.8,
        description="Strength for Image-to-Image generation. Higher values allow for more deviation from the original image.",
    )

    @property
    def out(self) -> "StableDiffusionImg2ImgNodeOutputs":
        return StableDiffusionImg2ImgNodeOutputs(self)

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.huggingface.image_to_image.StableDiffusionImg2ImgNode

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


class StableDiffusionImg2ImgNodeOutputs(OutputsProxy):
    @property
    def image(self) -> OutputHandle[typing.Any]:
        return typing.cast(OutputHandle[typing.Any], self["image"])

    @property
    def latent(self) -> OutputHandle[typing.Any]:
        return typing.cast(OutputHandle[typing.Any], self["latent"])


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.huggingface.image_to_image
from nodetool.workflows.base_node import BaseNode
import nodetool.nodes.huggingface.stable_diffusion_base


class StableDiffusionInpaintNode(
    GraphNode[
        nodetool.nodes.huggingface.image_to_image.StableDiffusionInpaintNode.OutputType
    ]
):
    """

    Performs inpainting on images using Stable Diffusion.
    image, inpainting, SD

    Use cases:
    - Remove unwanted objects from images
    - Fill in missing parts of images
    - Modify specific areas of images while preserving the rest
    """

    StableDiffusionScheduler: typing.ClassVar[type] = (
        nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionBaseNode.StableDiffusionScheduler
    )
    StableDiffusionOutputType: typing.ClassVar[type] = (
        nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionBaseNode.StableDiffusionOutputType
    )

    model: types.HFStableDiffusion | OutputHandle[types.HFStableDiffusion] = (
        connect_field(
            default=types.HFStableDiffusion(
                type="hf.stable_diffusion",
                repo_id="",
                path=None,
                variant=None,
                allow_patterns=None,
                ignore_patterns=None,
            ),
            description="The model to use for image generation.",
        )
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt for image generation."
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="The negative prompt to guide what should not appear in the generated image.",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="Seed for the random number generator. Use -1 for a random seed.",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=25, description="Number of denoising steps."
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=7.5, description="Guidance scale for generation."
    )
    scheduler: (
        nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionBaseNode.StableDiffusionScheduler
    ) = Field(
        default=nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionBaseNode.StableDiffusionScheduler.EulerDiscreteScheduler,
        description="The scheduler to use for the diffusion process.",
    )
    loras: list[types.HFLoraSDConfig] | OutputHandle[list[types.HFLoraSDConfig]] = (
        connect_field(
            default=[], description="The LoRA models to use for image processing"
        )
    )
    ip_adapter_model: types.HFIPAdapter | OutputHandle[types.HFIPAdapter] = (
        connect_field(
            default=types.HFIPAdapter(
                type="hf.ip_adapter",
                repo_id="",
                path=None,
                variant=None,
                allow_patterns=None,
                ignore_patterns=None,
            ),
            description="The IP adapter model to use for image processing",
        )
    )
    ip_adapter_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="When provided the image will be fed into the IP adapter",
    )
    ip_adapter_scale: float | OutputHandle[float] = connect_field(
        default=0.5, description="The strength of the IP adapter"
    )
    latents: types.TorchTensor | OutputHandle[types.TorchTensor] = connect_field(
        default=types.TorchTensor(
            type="torch_tensor", value=None, dtype="<i8", shape=(1,)
        ),
        description="Optional initial latents to start generation from.",
    )
    enable_attention_slicing: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Enable attention slicing for the pipeline. This can reduce VRAM usage.",
    )
    enable_tiling: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Legacy VAE tiling flag (disabled in favor of PyTorch 2 attention optimizations).",
    )
    enable_cpu_offload: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Enable CPU offload for the pipeline. This can reduce VRAM usage.",
    )
    output_type: (
        nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionBaseNode.StableDiffusionOutputType
    ) = Field(
        default=nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionBaseNode.StableDiffusionOutputType.IMAGE,
        description="The type of output to generate.",
    )
    init_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The initial image to be inpainted.",
    )
    mask_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The mask image indicating areas to be inpainted.",
    )
    strength: float | OutputHandle[float] = connect_field(
        default=0.8,
        description="Strength for inpainting. Higher values allow for more deviation from the original image.",
    )

    @property
    def out(self) -> "StableDiffusionInpaintNodeOutputs":
        return StableDiffusionInpaintNodeOutputs(self)

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.huggingface.image_to_image.StableDiffusionInpaintNode

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


class StableDiffusionInpaintNodeOutputs(OutputsProxy):
    @property
    def image(self) -> OutputHandle[typing.Any]:
        return typing.cast(OutputHandle[typing.Any], self["image"])

    @property
    def latent(self) -> OutputHandle[typing.Any]:
        return typing.cast(OutputHandle[typing.Any], self["latent"])


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.huggingface.image_to_image
from nodetool.workflows.base_node import BaseNode


class StableDiffusionLatentUpscaler(
    SingleOutputGraphNode[types.TorchTensor], GraphNode[types.TorchTensor]
):
    """

    Upscales Stable Diffusion latents (x2) using the SD Latent Upscaler pipeline.
    tensor, upscaling, stable-diffusion, latent, SD

    Input and output are tensors for chaining with latent-based workflows.
    """

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt for upscaling guidance."
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="The negative prompt to guide what should not appear in the result.",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=10, description="Number of upscaling denoising steps."
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=0.0,
        description="Guidance scale for upscaling. 0 preserves content strongly.",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="Seed for the random number generator. Use -1 for a random seed.",
    )
    latents: types.TorchTensor | OutputHandle[types.TorchTensor] = connect_field(
        default=types.TorchTensor(
            type="torch_tensor", value=None, dtype="<i8", shape=(1,)
        ),
        description="Low-resolution latents tensor to upscale.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.huggingface.image_to_image.StableDiffusionLatentUpscaler

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.huggingface.image_to_image
from nodetool.workflows.base_node import BaseNode
import nodetool.nodes.huggingface.stable_diffusion_base


class StableDiffusionUpscale(
    SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]
):
    """

    Upscales an image using Stable Diffusion 4x upscaler.
    image, upscaling, stable-diffusion, SD

    Use cases:
    - Enhance low-resolution images
    - Improve image quality for printing or display
    - Create high-resolution versions of small images
    """

    StableDiffusionScheduler: typing.ClassVar[type] = (
        nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionBaseNode.StableDiffusionScheduler
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt for image generation."
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="The negative prompt to guide what should not appear in the generated image.",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=25, description="Number of upscaling steps."
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=7.5, description="Guidance scale for generation."
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The initial image for Image-to-Image generation.",
    )
    scheduler: (
        nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionBaseNode.StableDiffusionScheduler
    ) = Field(
        default=nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionBaseNode.StableDiffusionScheduler.HeunDiscreteScheduler,
        description="The scheduler to use for the diffusion process.",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="Seed for the random number generator. Use -1 for a random seed.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.huggingface.image_to_image.StableDiffusionUpscale

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.huggingface.image_to_image
from nodetool.workflows.base_node import BaseNode
import nodetool.nodes.huggingface.stable_diffusion_base


class StableDiffusionXLControlNet(
    GraphNode[
        nodetool.nodes.huggingface.image_to_image.StableDiffusionXLControlNet.OutputType
    ]
):
    """

    Generates images using Stable Diffusion XL with ControlNet guidance.
    image, generation, text-to-image, controlnet, SDXL

    Use cases:
    - Generate images with precise control over composition and structure
    - Create variations of existing images while maintaining specific features
    - Artistic image generation with guided outputs
    """

    StableDiffusionXLQuantization: typing.ClassVar[type] = (
        nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionXLQuantization
    )
    StableDiffusionScheduler: typing.ClassVar[type] = (
        nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionXLBase.StableDiffusionScheduler
    )
    StableDiffusionOutputType: typing.ClassVar[type] = (
        nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionXLBase.StableDiffusionOutputType
    )

    model: types.HFStableDiffusionXL | OutputHandle[types.HFStableDiffusionXL] = (
        connect_field(
            default=types.HFStableDiffusionXL(
                type="hf.stable_diffusion_xl",
                repo_id="",
                path=None,
                variant=None,
                allow_patterns=None,
                ignore_patterns=None,
            ),
            description="The Stable Diffusion XL model to use for generation.",
        )
    )
    quantization: (
        nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionXLQuantization
    ) = Field(
        default=nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionXLQuantization.FP16,
        description="Quantization level for Stable Diffusion XL (enable INT4/FP4 to use a Nunchaku UNet).",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt for image generation."
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="The negative prompt to guide what should not appear in the generated image.",
    )
    width: int | OutputHandle[int] = connect_field(
        default=1024, description="Width of the generated image."
    )
    height: int | OutputHandle[int] = connect_field(
        default=1024, description="Height of the generated image"
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="Seed for the random number generator."
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=25, description="Number of inference steps."
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=7.0, description="Guidance scale for generation."
    )
    scheduler: (
        nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionXLBase.StableDiffusionScheduler
    ) = Field(
        default=nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionXLBase.StableDiffusionScheduler.EulerDiscreteScheduler,
        description="The scheduler to use for the diffusion process.",
    )
    loras: list[types.HFLoraSDXLConfig] | OutputHandle[list[types.HFLoraSDXLConfig]] = (
        connect_field(
            default=[], description="The LoRA models to use for image processing"
        )
    )
    lora_scale: float | OutputHandle[float] = connect_field(
        default=0.5, description="Strength of the LoRAs"
    )
    ip_adapter_model: types.HFIPAdapter | OutputHandle[types.HFIPAdapter] = (
        connect_field(
            default=types.HFIPAdapter(
                type="hf.ip_adapter",
                repo_id="",
                path=None,
                variant=None,
                allow_patterns=None,
                ignore_patterns=None,
            ),
            description="The IP adapter model to use for image processing",
        )
    )
    ip_adapter_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="When provided the image will be fed into the IP adapter",
    )
    ip_adapter_scale: float | OutputHandle[float] = connect_field(
        default=0.5, description="Strength of the IP adapter image"
    )
    enable_attention_slicing: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Enable attention slicing for the pipeline. This can reduce VRAM usage but may slow down generation.",
    )
    enable_tiling: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Legacy VAE tiling flag (disabled in favor of PyTorch 2 attention optimizations).",
    )
    enable_cpu_offload: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Enable CPU offload for the pipeline. This can reduce VRAM usage.",
    )
    output_type: (
        nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionXLBase.StableDiffusionOutputType
    ) = Field(
        default=nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionXLBase.StableDiffusionOutputType.IMAGE,
        description="The type of output to generate.",
    )
    controlnet: types.HFControlNet | OutputHandle[types.HFControlNet] = connect_field(
        default=types.HFControlNet(
            type="hf.controlnet",
            repo_id="",
            path=None,
            variant=None,
            allow_patterns=None,
            ignore_patterns=None,
        ),
        description="The ControlNet model to use for guidance.",
    )
    control_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The control image to guide the generation process.",
    )
    controlnet_conditioning_scale: float | OutputHandle[float] = connect_field(
        default=1.0, description="The scale for ControlNet conditioning."
    )

    @property
    def out(self) -> "StableDiffusionXLControlNetOutputs":
        return StableDiffusionXLControlNetOutputs(self)

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.huggingface.image_to_image.StableDiffusionXLControlNet

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


class StableDiffusionXLControlNetOutputs(OutputsProxy):
    @property
    def image(self) -> OutputHandle[typing.Any]:
        return typing.cast(OutputHandle[typing.Any], self["image"])

    @property
    def latent(self) -> OutputHandle[typing.Any]:
        return typing.cast(OutputHandle[typing.Any], self["latent"])


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.huggingface.image_to_image
from nodetool.workflows.base_node import BaseNode
import nodetool.nodes.huggingface.stable_diffusion_base


class StableDiffusionXLControlNetImg2ImgNode(
    GraphNode[
        nodetool.nodes.huggingface.image_to_image.StableDiffusionXLControlNetImg2ImgNode.OutputType
    ]
):
    """

    Transforms existing images using Stable Diffusion XL with ControlNet guidance.
    image, generation, image-to-image, controlnet, SDXL

    Use cases:
    - Modify existing images with precise control over composition and structure
    - Apply specific styles or concepts to photographs or artwork with guided transformations
    - Create variations of existing visual content while maintaining certain features
    - Enhance image editing capabilities with AI-guided transformations
    """

    StableDiffusionXLQuantization: typing.ClassVar[type] = (
        nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionXLQuantization
    )
    StableDiffusionScheduler: typing.ClassVar[type] = (
        nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionXLBase.StableDiffusionScheduler
    )
    StableDiffusionOutputType: typing.ClassVar[type] = (
        nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionXLBase.StableDiffusionOutputType
    )

    model: types.HFStableDiffusionXL | OutputHandle[types.HFStableDiffusionXL] = (
        connect_field(
            default=types.HFStableDiffusionXL(
                type="hf.stable_diffusion_xl",
                repo_id="",
                path=None,
                variant=None,
                allow_patterns=None,
                ignore_patterns=None,
            ),
            description="The Stable Diffusion XL model to use for generation.",
        )
    )
    quantization: (
        nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionXLQuantization
    ) = Field(
        default=nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionXLQuantization.FP16,
        description="Quantization level for Stable Diffusion XL (enable INT4/FP4 to use a Nunchaku UNet).",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt for image generation."
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="The negative prompt to guide what should not appear in the generated image.",
    )
    width: int | OutputHandle[int] = connect_field(
        default=1024, description="Width of the generated image."
    )
    height: int | OutputHandle[int] = connect_field(
        default=1024, description="Height of the generated image"
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="Seed for the random number generator."
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=25, description="Number of inference steps."
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=7.0, description="Guidance scale for generation."
    )
    scheduler: (
        nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionXLBase.StableDiffusionScheduler
    ) = Field(
        default=nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionXLBase.StableDiffusionScheduler.EulerDiscreteScheduler,
        description="The scheduler to use for the diffusion process.",
    )
    loras: list[types.HFLoraSDXLConfig] | OutputHandle[list[types.HFLoraSDXLConfig]] = (
        connect_field(
            default=[], description="The LoRA models to use for image processing"
        )
    )
    lora_scale: float | OutputHandle[float] = connect_field(
        default=0.5, description="Strength of the LoRAs"
    )
    ip_adapter_model: types.HFIPAdapter | OutputHandle[types.HFIPAdapter] = (
        connect_field(
            default=types.HFIPAdapter(
                type="hf.ip_adapter",
                repo_id="",
                path=None,
                variant=None,
                allow_patterns=None,
                ignore_patterns=None,
            ),
            description="The IP adapter model to use for image processing",
        )
    )
    ip_adapter_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="When provided the image will be fed into the IP adapter",
    )
    ip_adapter_scale: float | OutputHandle[float] = connect_field(
        default=0.5, description="Strength of the IP adapter image"
    )
    enable_attention_slicing: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Enable attention slicing for the pipeline. This can reduce VRAM usage but may slow down generation.",
    )
    enable_tiling: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Legacy VAE tiling flag (disabled in favor of PyTorch 2 attention optimizations).",
    )
    enable_cpu_offload: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Enable CPU offload for the pipeline. This can reduce VRAM usage.",
    )
    output_type: (
        nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionXLBase.StableDiffusionOutputType
    ) = Field(
        default=nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionXLBase.StableDiffusionOutputType.IMAGE,
        description="The type of output to generate.",
    )
    init_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The initial image for Image-to-Image generation.",
    )
    strength: float | OutputHandle[float] = connect_field(
        default=0.8,
        description="Strength for Image-to-Image generation. Higher values allow for more deviation from the original image.",
    )
    controlnet: types.HFControlNet | OutputHandle[types.HFControlNet] = connect_field(
        default=types.HFControlNet(
            type="hf.controlnet",
            repo_id="",
            path=None,
            variant=None,
            allow_patterns=None,
            ignore_patterns=None,
        ),
        description="The ControlNet model to use for guidance.",
    )
    control_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The control image to guide the transformation.",
    )
    controlnet_conditioning_scale: float | OutputHandle[float] = connect_field(
        default=1.0, description="The scale for ControlNet conditioning."
    )

    @property
    def out(self) -> "StableDiffusionXLControlNetImg2ImgNodeOutputs":
        return StableDiffusionXLControlNetImg2ImgNodeOutputs(self)

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return (
            nodetool.nodes.huggingface.image_to_image.StableDiffusionXLControlNetImg2ImgNode
        )

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


class StableDiffusionXLControlNetImg2ImgNodeOutputs(OutputsProxy):
    @property
    def image(self) -> OutputHandle[typing.Any]:
        return typing.cast(OutputHandle[typing.Any], self["image"])

    @property
    def latent(self) -> OutputHandle[typing.Any]:
        return typing.cast(OutputHandle[typing.Any], self["latent"])


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.huggingface.image_to_image
from nodetool.workflows.base_node import BaseNode
import nodetool.nodes.huggingface.stable_diffusion_base


class StableDiffusionXLImg2Img(
    GraphNode[
        nodetool.nodes.huggingface.image_to_image.StableDiffusionXLImg2Img.OutputType
    ]
):
    """

    Transforms existing images based on text prompts using Stable Diffusion XL.
    image, generation, image-to-image, SDXL, style-transfer

    Use cases:
    - Modifying existing images to fit a specific style or theme
    - Enhancing or altering photographs
    - Creating variations of existing artwork
    - Applying text-guided edits to images
    """

    StableDiffusionXLQuantization: typing.ClassVar[type] = (
        nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionXLQuantization
    )
    StableDiffusionScheduler: typing.ClassVar[type] = (
        nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionXLBase.StableDiffusionScheduler
    )
    StableDiffusionOutputType: typing.ClassVar[type] = (
        nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionXLBase.StableDiffusionOutputType
    )

    model: types.HFStableDiffusionXL | OutputHandle[types.HFStableDiffusionXL] = (
        connect_field(
            default=types.HFStableDiffusionXL(
                type="hf.stable_diffusion_xl",
                repo_id="",
                path=None,
                variant=None,
                allow_patterns=None,
                ignore_patterns=None,
            ),
            description="The Stable Diffusion XL model to use for generation.",
        )
    )
    quantization: (
        nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionXLQuantization
    ) = Field(
        default=nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionXLQuantization.FP16,
        description="Quantization level for Stable Diffusion XL (enable INT4/FP4 to use a Nunchaku UNet).",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt for image generation."
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="The negative prompt to guide what should not appear in the generated image.",
    )
    width: int | OutputHandle[int] = connect_field(
        default=1024, description="Width of the generated image."
    )
    height: int | OutputHandle[int] = connect_field(
        default=1024, description="Height of the generated image"
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="Seed for the random number generator."
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=25, description="Number of inference steps."
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=7.0, description="Guidance scale for generation."
    )
    scheduler: (
        nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionXLBase.StableDiffusionScheduler
    ) = Field(
        default=nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionXLBase.StableDiffusionScheduler.EulerDiscreteScheduler,
        description="The scheduler to use for the diffusion process.",
    )
    loras: list[types.HFLoraSDXLConfig] | OutputHandle[list[types.HFLoraSDXLConfig]] = (
        connect_field(
            default=[], description="The LoRA models to use for image processing"
        )
    )
    lora_scale: float | OutputHandle[float] = connect_field(
        default=0.5, description="Strength of the LoRAs"
    )
    ip_adapter_model: types.HFIPAdapter | OutputHandle[types.HFIPAdapter] = (
        connect_field(
            default=types.HFIPAdapter(
                type="hf.ip_adapter",
                repo_id="",
                path=None,
                variant=None,
                allow_patterns=None,
                ignore_patterns=None,
            ),
            description="The IP adapter model to use for image processing",
        )
    )
    ip_adapter_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="When provided the image will be fed into the IP adapter",
    )
    ip_adapter_scale: float | OutputHandle[float] = connect_field(
        default=0.5, description="Strength of the IP adapter image"
    )
    enable_attention_slicing: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Enable attention slicing for the pipeline. This can reduce VRAM usage.",
    )
    enable_tiling: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Legacy VAE tiling flag (disabled in favor of PyTorch 2 attention optimizations).",
    )
    enable_cpu_offload: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Enable CPU offload for the pipeline. This can reduce VRAM usage.",
    )
    output_type: (
        nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionXLBase.StableDiffusionOutputType
    ) = Field(
        default=nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionXLBase.StableDiffusionOutputType.IMAGE,
        description="The type of output to generate.",
    )
    init_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The initial image for Image-to-Image generation.",
    )
    strength: float | OutputHandle[float] = connect_field(
        default=0.8,
        description="Strength for Image-to-Image generation. Higher values allow for more deviation from the original image.",
    )

    @property
    def out(self) -> "StableDiffusionXLImg2ImgOutputs":
        return StableDiffusionXLImg2ImgOutputs(self)

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.huggingface.image_to_image.StableDiffusionXLImg2Img

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


class StableDiffusionXLImg2ImgOutputs(OutputsProxy):
    @property
    def image(self) -> OutputHandle[typing.Any]:
        return typing.cast(OutputHandle[typing.Any], self["image"])

    @property
    def latent(self) -> OutputHandle[typing.Any]:
        return typing.cast(OutputHandle[typing.Any], self["latent"])


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.huggingface.image_to_image
from nodetool.workflows.base_node import BaseNode
import nodetool.nodes.huggingface.stable_diffusion_base


class StableDiffusionXLInpainting(
    GraphNode[
        nodetool.nodes.huggingface.image_to_image.StableDiffusionXLInpainting.OutputType
    ]
):
    """

    Performs inpainting on images using Stable Diffusion XL.
    image, inpainting, SDXL

    Use cases:
    - Remove unwanted objects from images
    - Fill in missing parts of images
    - Modify specific areas of images while preserving the rest
    """

    StableDiffusionXLQuantization: typing.ClassVar[type] = (
        nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionXLQuantization
    )
    StableDiffusionScheduler: typing.ClassVar[type] = (
        nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionXLBase.StableDiffusionScheduler
    )
    StableDiffusionOutputType: typing.ClassVar[type] = (
        nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionXLBase.StableDiffusionOutputType
    )

    model: types.HFStableDiffusionXL | OutputHandle[types.HFStableDiffusionXL] = (
        connect_field(
            default=types.HFStableDiffusionXL(
                type="hf.stable_diffusion_xl",
                repo_id="",
                path=None,
                variant=None,
                allow_patterns=None,
                ignore_patterns=None,
            ),
            description="The Stable Diffusion XL model to use for generation.",
        )
    )
    quantization: (
        nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionXLQuantization
    ) = Field(
        default=nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionXLQuantization.FP16,
        description="Quantization level for Stable Diffusion XL (enable INT4/FP4 to use a Nunchaku UNet).",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt for image generation."
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="The negative prompt to guide what should not appear in the generated image.",
    )
    width: int | OutputHandle[int] = connect_field(
        default=1024, description="Width of the generated image."
    )
    height: int | OutputHandle[int] = connect_field(
        default=1024, description="Height of the generated image"
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="Seed for the random number generator."
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=25, description="Number of inference steps."
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=7.0, description="Guidance scale for generation."
    )
    scheduler: (
        nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionXLBase.StableDiffusionScheduler
    ) = Field(
        default=nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionXLBase.StableDiffusionScheduler.EulerDiscreteScheduler,
        description="The scheduler to use for the diffusion process.",
    )
    loras: list[types.HFLoraSDXLConfig] | OutputHandle[list[types.HFLoraSDXLConfig]] = (
        connect_field(
            default=[], description="The LoRA models to use for image processing"
        )
    )
    lora_scale: float | OutputHandle[float] = connect_field(
        default=0.5, description="Strength of the LoRAs"
    )
    ip_adapter_model: types.HFIPAdapter | OutputHandle[types.HFIPAdapter] = (
        connect_field(
            default=types.HFIPAdapter(
                type="hf.ip_adapter",
                repo_id="",
                path=None,
                variant=None,
                allow_patterns=None,
                ignore_patterns=None,
            ),
            description="The IP adapter model to use for image processing",
        )
    )
    ip_adapter_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="When provided the image will be fed into the IP adapter",
    )
    ip_adapter_scale: float | OutputHandle[float] = connect_field(
        default=0.5, description="Strength of the IP adapter image"
    )
    enable_attention_slicing: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Enable attention slicing for the pipeline. This can reduce VRAM usage.",
    )
    enable_tiling: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Legacy VAE tiling flag (disabled in favor of PyTorch 2 attention optimizations).",
    )
    enable_cpu_offload: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Enable CPU offload for the pipeline. This can reduce VRAM usage.",
    )
    output_type: (
        nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionXLBase.StableDiffusionOutputType
    ) = Field(
        default=nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionXLBase.StableDiffusionOutputType.IMAGE,
        description="The type of output to generate.",
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The initial image to be inpainted.",
    )
    mask_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The mask image indicating areas to be inpainted.",
    )
    strength: float | OutputHandle[float] = connect_field(
        default=0.8,
        description="Strength for inpainting. Higher values allow for more deviation from the original image.",
    )

    @property
    def out(self) -> "StableDiffusionXLInpaintingOutputs":
        return StableDiffusionXLInpaintingOutputs(self)

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.huggingface.image_to_image.StableDiffusionXLInpainting

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


class StableDiffusionXLInpaintingOutputs(OutputsProxy):
    @property
    def image(self) -> OutputHandle[typing.Any]:
        return typing.cast(OutputHandle[typing.Any], self["image"])

    @property
    def latent(self) -> OutputHandle[typing.Any]:
        return typing.cast(OutputHandle[typing.Any], self["latent"])


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.huggingface.image_to_image
from nodetool.workflows.base_node import BaseNode


class Swin2SR(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """

    Performs image super-resolution using the Swin2SR model.
    image, super-resolution, enhancement, huggingface

    Use cases:
    - Enhance low-resolution images
    - Improve image quality for printing or display
    - Upscale images for better detail
    """

    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The input image to transform",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="The text prompt to guide the image transformation (if applicable)",
    )
    model: types.HFImageToImage | OutputHandle[types.HFImageToImage] = connect_field(
        default=types.HFImageToImage(
            type="hf.image_to_image",
            repo_id="",
            path=None,
            variant=None,
            allow_patterns=None,
            ignore_patterns=None,
        ),
        description="The model ID to use for image super-resolution",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.huggingface.image_to_image.Swin2SR

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.huggingface.image_to_image
from nodetool.workflows.base_node import BaseNode


class VAEDecode(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """

    Decodes latents into an image using a VAE.
    tensor (TorchTensor) -> image
    """

    model: types.HFVAE | OutputHandle[types.HFVAE] = connect_field(
        default=types.HFVAE(
            type="hf.vae",
            repo_id="",
            path=None,
            variant=None,
            allow_patterns=None,
            ignore_patterns=None,
        ),
        description="The VAE model to use.",
    )
    latents: types.TorchTensor | OutputHandle[types.TorchTensor] = connect_field(
        default=types.TorchTensor(
            type="torch_tensor", value=None, dtype="<i8", shape=(1,)
        ),
        description="Latent tensor to decode.",
    )
    scale_factor: float | OutputHandle[float] = connect_field(
        default=0.18215,
        description="Scaling factor used for encoding (inverse is applied before decode)",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.huggingface.image_to_image.VAEDecode

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.huggingface.image_to_image
from nodetool.workflows.base_node import BaseNode


class VAEEncode(SingleOutputGraphNode[types.TorchTensor], GraphNode[types.TorchTensor]):
    """

    Encodes an image into latents using a VAE.
    image -> tensor (TorchTensor)
    """

    model: types.HFVAE | OutputHandle[types.HFVAE] = connect_field(
        default=types.HFVAE(
            type="hf.vae",
            repo_id="",
            path=None,
            variant=None,
            allow_patterns=None,
            ignore_patterns=None,
        ),
        description="The VAE model to use.",
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Input image to encode.",
    )
    scale_factor: float | OutputHandle[float] = connect_field(
        default=0.18215,
        description="Scaling factor applied to latents (e.g., 0.18215 for SD15)",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.huggingface.image_to_image.VAEEncode

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()
