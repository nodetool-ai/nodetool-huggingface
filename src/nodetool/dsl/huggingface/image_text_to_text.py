# This file is auto-generated by nodetool.dsl.codegen.
# Please do not edit this file manually.

# Instead, edit the node class in the source module and run the following commands to regenerate the DSL:
# nodetool package scan
# nodetool codegen

from pydantic import BaseModel, Field
import typing
from typing import Any
import nodetool.metadata.types
import nodetool.metadata.types as types
from nodetool.dsl.graph import GraphNode, SingleOutputGraphNode

import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.huggingface.image_text_to_text
from nodetool.workflows.base_node import BaseNode


class BaseQwenVL(
    GraphNode[nodetool.nodes.huggingface.image_text_to_text.BaseQwenVL.OutputType]
):
    """

    Base class for Qwen vision-language models supporting image and video understanding.
    image, video, multimodal, VLM, Qwen
    """

    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="An image to analyze. Leave empty if providing video input.",
    )
    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="A video to analyze. Leave empty if providing image input.",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="Describe this image.",
        description="Your question or instruction about the visual content.",
    )
    min_pixels: int | OutputHandle[int] = connect_field(
        default=0,
        description="Minimum pixel count for image resizing. Use 0 for automatic sizing.",
    )
    max_pixels: int | OutputHandle[int] = connect_field(
        default=0,
        description="Maximum pixel count for image resizing. Use 0 for automatic sizing.",
    )
    max_new_tokens: int | OutputHandle[int] = connect_field(
        default=128, description="Maximum length of the generated response in tokens."
    )

    @property
    def out(self) -> "BaseQwenVLOutputs":
        return BaseQwenVLOutputs(self)

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.huggingface.image_text_to_text.BaseQwenVL

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


class BaseQwenVLOutputs(OutputsProxy):
    @property
    def text(self) -> OutputHandle[str]:
        return typing.cast(OutputHandle[str], self["text"])

    @property
    def chunk(self) -> OutputHandle[types.Chunk]:
        return typing.cast(OutputHandle[types.Chunk], self["chunk"])


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.huggingface.image_text_to_text
from nodetool.workflows.base_node import BaseNode


class ImageTextToText(
    GraphNode[nodetool.nodes.huggingface.image_text_to_text.ImageTextToText.OutputType]
):
    """

    Generates text responses based on an image and text prompt using vision-language models.
    image, text, visual-question-answering, multimodal, VLM, captioning

    Use cases:
    - Answer questions about image content with detailed explanations
    - Generate comprehensive image descriptions and captions
    - Extract structured information (objects, text, layout) from images
    - Perform OCR-free document understanding via natural language
    - Build multi-turn visual conversations and assistants
    """

    model: types.HFImageTextToText | OutputHandle[types.HFImageTextToText] = (
        connect_field(
            default=types.HFImageTextToText(
                type="hf.image_text_to_text",
                repo_id="HuggingFaceTB/SmolVLM-Instruct",
                path=None,
                variant=None,
                allow_patterns=None,
                ignore_patterns=None,
            ),
            description="The vision-language model to use. SmolVLM is lightweight; LLaVA variants offer different capability levels.",
        )
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The image to analyze and discuss.",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="Describe this image.",
        description="Your question or instruction about the image. Be specific for better results.",
    )
    max_new_tokens: int | OutputHandle[int] = connect_field(
        default=256, description="Maximum length of the generated response in tokens."
    )

    @property
    def out(self) -> "ImageTextToTextOutputs":
        return ImageTextToTextOutputs(self)

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.huggingface.image_text_to_text.ImageTextToText

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


class ImageTextToTextOutputs(OutputsProxy):
    @property
    def text(self) -> OutputHandle[str]:
        return typing.cast(OutputHandle[str], self["text"])

    @property
    def chunk(self) -> OutputHandle[types.Chunk]:
        return typing.cast(OutputHandle[types.Chunk], self["chunk"])


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.huggingface.image_text_to_text
from nodetool.workflows.base_node import BaseNode


class LoadImageTextToTextModel(
    SingleOutputGraphNode[types.HFImageTextToText], GraphNode[types.HFImageTextToText]
):
    """

    Loads and validates a Hugging Face image-text-to-text model for use in downstream nodes.
    model-loader, vision-language, multimodal, VLM

    Use cases:
    - Pre-load vision-language models for image understanding tasks
    - Validate model availability before running pipelines
    - Configure model settings for ImageTextToText processing
    """

    repo_id: str | OutputHandle[str] = connect_field(
        default="HuggingFaceTB/SmolVLM-Instruct",
        description="The Hugging Face repository ID for the vision-language model (e.g., SmolVLM, LLaVA variants).",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.huggingface.image_text_to_text.LoadImageTextToTextModel

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.huggingface.image_text_to_text
from nodetool.workflows.base_node import BaseNode


class Qwen2_5_VL(
    GraphNode[nodetool.nodes.huggingface.image_text_to_text.BaseQwenVL.OutputType]
):
    """

    Analyzes images and videos using Alibaba's Qwen2.5-VL vision-language model.
    image, video, multimodal, VLM, Qwen, visual-understanding

    Use cases:
    - Understand visual content including objects, text, charts, and layouts
    - Comprehend videos with temporal event tracking and scene changes
    - Localize objects with bounding boxes or point annotations
    - Generate structured output (JSON, tables) from visual data
    - Read and interpret documents, diagrams, and UI screenshots

    **Note:** BNB-4bit variants from Unsloth reduce memory usage significantly.
    """

    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="An image to analyze. Leave empty if providing video input.",
    )
    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="A video to analyze. Leave empty if providing image input.",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="Describe this image.",
        description="Your question or instruction about the visual content.",
    )
    min_pixels: int | OutputHandle[int] = connect_field(
        default=0,
        description="Minimum pixel count for image resizing. Use 0 for automatic sizing.",
    )
    max_pixels: int | OutputHandle[int] = connect_field(
        default=0,
        description="Maximum pixel count for image resizing. Use 0 for automatic sizing.",
    )
    max_new_tokens: int | OutputHandle[int] = connect_field(
        default=128, description="Maximum length of the generated response in tokens."
    )
    model: types.HFQwen2_5_VL | OutputHandle[types.HFQwen2_5_VL] = connect_field(
        default=types.HFQwen2_5_VL(
            type="hf.qwen2_5_vl",
            repo_id="Qwen/Qwen2.5-VL-7B-Instruct",
            path=None,
            variant=None,
            allow_patterns=None,
            ignore_patterns=None,
        ),
        description="The Qwen2.5-VL model variant. Larger models (32B, 72B) offer better accuracy; BNB-4bit variants reduce memory usage.",
    )

    @property
    def out(self) -> "Qwen2_5_VLOutputs":
        return Qwen2_5_VLOutputs(self)

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.huggingface.image_text_to_text.Qwen2_5_VL

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


class Qwen2_5_VLOutputs(OutputsProxy):
    @property
    def text(self) -> OutputHandle[str]:
        return typing.cast(OutputHandle[str], self["text"])

    @property
    def chunk(self) -> OutputHandle[types.Chunk]:
        return typing.cast(OutputHandle[types.Chunk], self["chunk"])


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.huggingface.image_text_to_text
from nodetool.workflows.base_node import BaseNode


class Qwen3_VL(
    GraphNode[nodetool.nodes.huggingface.image_text_to_text.BaseQwenVL.OutputType]
):
    """

    Analyzes images and videos using Alibaba's next-generation Qwen3-VL vision-language model.
    image, video, multimodal, VLM, Qwen, visual-reasoning, thinking

    Use cases:
    - Advanced visual reasoning across images and video content
    - Instruction-following with improved spatial-temporal grounding
    - Complex multi-step visual analysis with chain-of-thought reasoning
    - Document, chart, and diagram understanding with enhanced accuracy
    - Long-context visual conversations with memory

    **Note:** Thinking variants include extended reasoning capabilities. BNB-4bit variants reduce memory usage.
    """

    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="An image to analyze. Leave empty if providing video input.",
    )
    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="A video to analyze. Leave empty if providing image input.",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="Describe this image.",
        description="Your question or instruction about the visual content.",
    )
    min_pixels: int | OutputHandle[int] = connect_field(
        default=0,
        description="Minimum pixel count for image resizing. Use 0 for automatic sizing.",
    )
    max_pixels: int | OutputHandle[int] = connect_field(
        default=0,
        description="Maximum pixel count for image resizing. Use 0 for automatic sizing.",
    )
    max_new_tokens: int | OutputHandle[int] = connect_field(
        default=128, description="Maximum length of the generated response in tokens."
    )
    model: types.HFQwen3_VL | OutputHandle[types.HFQwen3_VL] = connect_field(
        default=types.HFQwen3_VL(
            type="hf.qwen3_vl",
            repo_id="Qwen/Qwen3-VL-4B-Instruct",
            path=None,
            variant=None,
            allow_patterns=None,
            ignore_patterns=None,
        ),
        description="The Qwen3-VL model variant. Thinking variants add chain-of-thought; BNB-4bit reduces memory; larger models improve accuracy.",
    )

    @property
    def out(self) -> "Qwen3_VLOutputs":
        return Qwen3_VLOutputs(self)

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.huggingface.image_text_to_text.Qwen3_VL

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


class Qwen3_VLOutputs(OutputsProxy):
    @property
    def text(self) -> OutputHandle[str]:
        return typing.cast(OutputHandle[str], self["text"])

    @property
    def chunk(self) -> OutputHandle[types.Chunk]:
        return typing.cast(OutputHandle[types.Chunk], self["chunk"])
