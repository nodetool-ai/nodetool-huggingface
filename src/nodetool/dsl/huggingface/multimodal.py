# This file is auto-generated by nodetool.dsl.codegen.
# Please do not edit this file manually.

# Instead, edit the node class in the source module and run the following commands to regenerate the DSL:
# nodetool package scan
# nodetool codegen

from pydantic import BaseModel, Field
import typing
from typing import Any
import nodetool.metadata.types
import nodetool.metadata.types as types
from nodetool.dsl.graph import GraphNode, SingleOutputGraphNode

import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.huggingface.multimodal
from nodetool.workflows.base_node import BaseNode


class ImageToText(SingleOutputGraphNode[str], GraphNode[str]):
    """

    Generates descriptive text captions from images using vision-language models.
    image, text, captioning, vision-language, accessibility

    Use cases:
    - Generate natural language descriptions of image content
    - Create alt-text for web accessibility compliance
    - Build automatic image cataloging and search systems
    - Enable content discovery through text-based image queries
    """

    model: types.HFImageToText | OutputHandle[types.HFImageToText] = connect_field(
        default=types.HFImageToText(
            type="hf.image_to_text",
            repo_id="",
            path=None,
            variant=None,
            allow_patterns=None,
            ignore_patterns=None,
        ),
        description="The image captioning model. BLIP variants offer good quality/speed balance.",
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The image to generate a caption for.",
    )
    max_new_tokens: int | OutputHandle[int] = connect_field(
        default=1024, description="Maximum length of the generated caption in tokens."
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.huggingface.multimodal.ImageToText

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.huggingface.multimodal
from nodetool.workflows.base_node import BaseNode


class VisualQuestionAnswering(SingleOutputGraphNode[str], GraphNode[str]):
    """

    Answers natural language questions about image content using vision-language models.
    image, text, question-answering, multimodal, VQA

    Use cases:
    - Query image content with natural language questions
    - Extract specific information from photos and diagrams
    - Build interactive image exploration interfaces
    - Create accessibility tools for visual content understanding
    """

    model: (
        types.HFVisualQuestionAnswering | OutputHandle[types.HFVisualQuestionAnswering]
    ) = connect_field(
        default=types.HFVisualQuestionAnswering(
            type="hf.visual_question_answering",
            repo_id="",
            path=None,
            variant=None,
            allow_patterns=None,
            ignore_patterns=None,
        ),
        description="The visual question answering model. BLIP-VQA provides good general performance.",
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The image to ask questions about.",
    )
    question: str | OutputHandle[str] = connect_field(
        default="",
        description="Your question about the image content (e.g., 'What color is the car?').",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.huggingface.multimodal.VisualQuestionAnswering

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()
