# This file is auto-generated by nodetool.dsl.codegen.
# Please do not edit this file manually.

# Instead, edit the node class in the source module and run the following commands to regenerate the DSL:
# nodetool package scan
# nodetool codegen

from pydantic import BaseModel, Field
import typing
from typing import Any
import nodetool.metadata.types
import nodetool.metadata.types as types
from nodetool.dsl.graph import GraphNode, SingleOutputGraphNode

import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.huggingface.text_to_video
from nodetool.workflows.base_node import BaseNode


class CogVideoX(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

    Generates high-quality videos from text prompts using the CogVideoX diffusion transformer.
    video, generation, AI, text-to-video, transformer, diffusion, cinematic

    Use cases:
    - Create videos from detailed text descriptions
    - Generate cinematic content for creative projects
    - Produce animated scenes for storytelling and marketing
    - Build AI video generation applications
    - Create visual content for social media
    """

    prompt: str | OutputHandle[str] = connect_field(
        default="A detailed wooden toy ship with intricately carved masts and sails is seen gliding smoothly over a plush, blue carpet that mimics the waves of the sea. The ship's hull is painted a rich brown, with tiny windows. The carpet, soft and textured, provides a perfect backdrop, resembling an oceanic expanse. Surrounding the ship are various other toys and children's items, hinting at a playful environment. The scene captures the innocence and imagination of childhood, with the toy ship's journey symbolizing endless adventures in a whimsical, indoor setting.",
        description="Detailed text description of the video to generate. More descriptive prompts produce better results.",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="Describe what to avoid in the video (e.g., 'blurry, low quality, distorted').",
    )
    num_frames: int | OutputHandle[int] = connect_field(
        default=49,
        description="Total frames in the output. Must be 8n+1 (49, 81, 113). More frames = longer video.",
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=6.0,
        description="How strongly to follow the prompt. 5-8 is typical; higher = more adherence.",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=50,
        description="Denoising steps. 50 is recommended; lower for faster generation.",
    )
    height: int | OutputHandle[int] = connect_field(
        default=480, description="Output video height in pixels."
    )
    width: int | OutputHandle[int] = connect_field(
        default=720, description="Output video width in pixels."
    )
    fps: int | OutputHandle[int] = connect_field(
        default=8, description="Frames per second for the output video file."
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="Random seed for reproducible generation. Use -1 for random.",
    )
    max_sequence_length: int | OutputHandle[int] = connect_field(
        default=226,
        description="Maximum prompt encoding length. Higher allows longer prompts.",
    )
    enable_cpu_offload: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Offload model components to CPU to reduce VRAM usage.",
    )
    enable_vae_slicing: bool | OutputHandle[bool] = connect_field(
        default=True, description="Process VAE in slices to reduce peak memory usage."
    )
    enable_vae_tiling: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Process VAE in tiles for large videos. Reduces memory but may affect quality.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.huggingface.text_to_video.CogVideoX

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.huggingface.text_to_video
from nodetool.workflows.base_node import BaseNode


class Wan_T2V(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

    Generates videos from text prompts using Wan text-to-video diffusion models.
    video, generation, AI, text-to-video, diffusion, Wan, cinematic

    Use cases:
    - Create high-quality videos from text descriptions
    - Generate cinematic content for creative and commercial projects
    - Produce animated scenes for storytelling and marketing
    - Build AI video generation applications
    - Create visual content for social media and entertainment

    **Note:** Model variants offer different quality/resource tradeoffs. A14B is balanced; 14B offers maximum quality.
    """

    WanModel: typing.ClassVar[type] = (
        nodetool.nodes.huggingface.text_to_video.Wan_T2V.WanModel
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="A robot standing on a mountain top at sunset, cinematic lighting, high detail",
        description="Detailed text description of the video to generate.",
    )
    model_variant: nodetool.nodes.huggingface.text_to_video.Wan_T2V.WanModel = Field(
        default=nodetool.nodes.huggingface.text_to_video.Wan_T2V.WanModel.WAN_2_2_T2V_A14B,
        description="The Wan model variant. A14B is balanced; TI2V-5B is smaller/faster.",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="Describe what to avoid in the video (e.g., 'blurry, distorted').",
    )
    num_frames: int | OutputHandle[int] = connect_field(
        default=49,
        description="Total frames in the output video. More frames = longer duration.",
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=5.0, description="How strongly to follow the prompt. 4-7 is typical."
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=30, description="Denoising steps. 30 is fast; 50+ for higher quality."
    )
    height: int | OutputHandle[int] = connect_field(
        default=480, description="Output video height in pixels."
    )
    width: int | OutputHandle[int] = connect_field(
        default=720, description="Output video width in pixels."
    )
    fps: int | OutputHandle[int] = connect_field(
        default=16, description="Frames per second for the output video file."
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="Random seed for reproducible generation. Use -1 for random.",
    )
    max_sequence_length: int | OutputHandle[int] = connect_field(
        default=512,
        description="Maximum prompt encoding length. Higher allows longer prompts.",
    )
    enable_cpu_offload: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Offload model components to CPU to reduce VRAM usage.",
    )
    enable_vae_slicing: bool | OutputHandle[bool] = connect_field(
        default=True, description="Process VAE in slices to reduce peak memory usage."
    )
    enable_vae_tiling: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Process VAE in tiles for large videos. May affect quality.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.huggingface.text_to_video.Wan_T2V

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()
