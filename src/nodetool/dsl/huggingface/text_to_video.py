# This file is auto-generated by nodetool.dsl.codegen.
# Please do not edit this file manually.

# Instead, edit the node class in the source module and run the following commands to regenerate the DSL:
# nodetool package scan
# nodetool codegen

from pydantic import BaseModel, Field
import typing
from typing import Any
import nodetool.metadata.types
import nodetool.metadata.types as types
from nodetool.dsl.graph import GraphNode, SingleOutputGraphNode

import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.huggingface.text_to_video
from nodetool.workflows.base_node import BaseNode


class CogVideoX(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

    Generates videos from text prompts using CogVideoX, a large diffusion transformer model.
    video, generation, AI, text-to-video, transformer, diffusion

    Use cases:
    - Create high-quality videos from text descriptions
    - Generate longer and more consistent videos
    - Produce cinematic content for creative projects
    - Create animated scenes for storytelling
    - Generate video content for marketing and media
    """

    prompt: str | OutputHandle[str] = connect_field(
        default="A detailed wooden toy ship with intricately carved masts and sails is seen gliding smoothly over a plush, blue carpet that mimics the waves of the sea. The ship's hull is painted a rich brown, with tiny windows. The carpet, soft and textured, provides a perfect backdrop, resembling an oceanic expanse. Surrounding the ship are various other toys and children's items, hinting at a playful environment. The scene captures the innocence and imagination of childhood, with the toy ship's journey symbolizing endless adventures in a whimsical, indoor setting.",
        description="A text prompt describing the desired video.",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="", description="A text prompt describing what to avoid in the video."
    )
    num_frames: int | OutputHandle[int] = connect_field(
        default=49,
        description="The number of frames in the video. Must be divisible by 8 + 1 (e.g., 49, 81, 113).",
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=6.0, description="The scale for classifier-free guidance."
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=50, description="The number of denoising steps."
    )
    height: int | OutputHandle[int] = connect_field(
        default=480, description="The height of the generated video in pixels."
    )
    width: int | OutputHandle[int] = connect_field(
        default=720, description="The width of the generated video in pixels."
    )
    fps: int | OutputHandle[int] = connect_field(
        default=8, description="Frames per second for the output video."
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="Seed for the random number generator. Use -1 for a random seed.",
    )
    max_sequence_length: int | OutputHandle[int] = connect_field(
        default=226, description="Maximum sequence length in encoded prompt."
    )
    enable_cpu_offload: bool | OutputHandle[bool] = connect_field(
        default=True, description="Enable CPU offload to reduce VRAM usage."
    )
    enable_vae_slicing: bool | OutputHandle[bool] = connect_field(
        default=True, description="Enable VAE slicing to reduce VRAM usage."
    )
    enable_vae_tiling: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Enable VAE tiling to reduce VRAM usage for large videos.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.huggingface.text_to_video.CogVideoX

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.huggingface.text_to_video
from nodetool.workflows.base_node import BaseNode


class Wan_T2V(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

    Generates videos from text prompts using Wan text-to-video pipeline.
    video, generation, AI, text-to-video, diffusion, Wan

    Use cases:
    - Create high-quality videos from text descriptions
    - Efficient 1.3B model for consumer GPUs or 14B for maximum quality
    """

    WanModel: typing.ClassVar[type] = (
        nodetool.nodes.huggingface.text_to_video.Wan_T2V.WanModel
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="A robot standing on a mountain top at sunset, cinematic lighting, high detail",
        description="A text prompt describing the desired video.",
    )
    model_variant: nodetool.nodes.huggingface.text_to_video.Wan_T2V.WanModel = Field(
        default=nodetool.nodes.huggingface.text_to_video.Wan_T2V.WanModel.WAN_2_2_T2V_A14B,
        description="Select the Wan model to use.",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="", description="A text prompt describing what to avoid in the video."
    )
    num_frames: int | OutputHandle[int] = connect_field(
        default=49, description="The number of frames in the video."
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=5.0, description="The scale for classifier-free guidance."
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=30, description="The number of denoising steps."
    )
    height: int | OutputHandle[int] = connect_field(
        default=480, description="The height of the generated video in pixels."
    )
    width: int | OutputHandle[int] = connect_field(
        default=720, description="The width of the generated video in pixels."
    )
    fps: int | OutputHandle[int] = connect_field(
        default=16, description="Frames per second for the output video."
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="Seed for the random number generator. Use -1 for a random seed.",
    )
    max_sequence_length: int | OutputHandle[int] = connect_field(
        default=512, description="Maximum sequence length in encoded prompt."
    )
    enable_cpu_offload: bool | OutputHandle[bool] = connect_field(
        default=True, description="Enable CPU offload to reduce VRAM usage."
    )
    enable_vae_slicing: bool | OutputHandle[bool] = connect_field(
        default=True, description="Enable VAE slicing to reduce VRAM usage."
    )
    enable_vae_tiling: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Enable VAE tiling to reduce VRAM usage for large videos.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.huggingface.text_to_video.Wan_T2V

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()
