{
  "name": "nodetool-huggingface",
  "description": "HuggingFace nodes for Nodetool",
  "version": "0.6.2-rc.16",
  "authors": [
    "Matthias Georgi <matti.georgi@gmail.com>"
  ],
  "repo_id": "",
  "nodes": [
    {
      "title": "Audio LDM",
      "description": "Generates audio from text prompts using the AudioLDM latent diffusion model.\n    audio, generation, AI, text-to-audio, sound-effects\n\n    Use cases:\n    - Create custom music clips from text descriptions\n    - Generate sound effects for videos, games, and media\n    - Produce background audio for creative projects\n    - Explore AI-generated soundscapes and ambient audio",
      "namespace": "huggingface.text_to_audio",
      "node_type": "huggingface.text_to_audio.AudioLDM",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "Techno music with a strong, upbeat tempo and high melodic riffs",
          "title": "Prompt",
          "description": "Text description of the audio you want to generate."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 10,
          "title": "Num Inference Steps",
          "description": "Denoising steps. More steps = better quality but slower. 10-50 is typical.",
          "min": 1.0,
          "max": 100.0
        },
        {
          "name": "audio_length_in_s",
          "type": {
            "type": "float"
          },
          "default": 5.0,
          "title": "Audio Length In S",
          "description": "Duration of the generated audio in seconds (1-30).",
          "min": 1.0,
          "max": 30.0
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Seed",
          "description": "Random seed for reproducible generation. Use -1 for random.",
          "min": -1.0
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "recommended_models": [
        {
          "id": "cvssp/audioldm-s-full-v2",
          "type": "hf.text_to_audio",
          "name": "cvssp/audioldm-s-full-v2",
          "repo_id": "cvssp/audioldm-s-full-v2",
          "allow_patterns": [
            "*.safetensors",
            "*.json",
            "*.txt"
          ],
          "size_on_disk": 1687448107,
          "tags": [
            "diffusers",
            "safetensors",
            "arxiv:2301.12503",
            "license:cc-by-nc-sa-4.0",
            "diffusers:AudioLDMPipeline",
            "region:us"
          ],
          "has_model_index": true,
          "downloads": 1909,
          "likes": 20
        }
      ],
      "basic_fields": [
        "model",
        "prompt"
      ]
    },
    {
      "title": "Audio LDM 2",
      "description": "Generates audio from text prompts using the improved AudioLDM2 model.\n    audio, generation, AI, text-to-audio, sound-effects, sound-design\n\n    Use cases:\n    - Create realistic sound effects from text descriptions\n    - Generate background audio for videos and games\n    - Produce environmental soundscapes for multimedia\n    - Explore creative AI-generated audio for sound design",
      "namespace": "huggingface.text_to_audio",
      "node_type": "huggingface.text_to_audio.AudioLDM2",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "The sound of a hammer hitting a wooden surface.",
          "title": "Prompt",
          "description": "Text description of the audio you want to generate."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "Low quality.",
          "title": "Negative Prompt",
          "description": "Describe what to avoid in the generated audio (e.g., 'noise, distortion')."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 200,
          "title": "Num Inference Steps",
          "description": "Denoising steps. 200 is recommended for quality; lower for speed.",
          "min": 50.0,
          "max": 500.0
        },
        {
          "name": "audio_length_in_s",
          "type": {
            "type": "float"
          },
          "default": 10.0,
          "title": "Audio Length In S",
          "description": "Duration of the generated audio in seconds (1-30).",
          "min": 1.0,
          "max": 30.0
        },
        {
          "name": "num_waveforms_per_prompt",
          "type": {
            "type": "int"
          },
          "default": 3,
          "title": "Num Waveforms Per Prompt",
          "description": "Number of audio variations to generate. Best result is returned.",
          "min": 1.0,
          "max": 5.0
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Seed",
          "description": "Random seed for reproducible generation. Use -1 for random.",
          "min": -1.0
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "recommended_models": [
        {
          "id": "cvssp/audioldm2",
          "type": "hf.text_to_audio",
          "name": "cvssp/audioldm2",
          "repo_id": "cvssp/audioldm2",
          "allow_patterns": [
            "*.safetensors",
            "*.json",
            "*.txt"
          ],
          "size_on_disk": 4480160895,
          "tags": [
            "diffusers",
            "safetensors",
            "arxiv:2308.05734",
            "license:cc-by-nc-sa-4.0",
            "diffusers:AudioLDM2Pipeline",
            "region:us"
          ],
          "has_model_index": true,
          "downloads": 61987,
          "likes": 56
        }
      ],
      "basic_fields": [
        "model",
        "prompt"
      ]
    },
    {
      "title": "Dance Diffusion",
      "description": "Generates AI-composed music using the DanceDiffusion unconditional audio model.\n    audio, generation, AI, music, text-to-audio, unconditional\n\n    Use cases:\n    - Create AI-generated music samples and loops\n    - Produce background music for videos and games\n    - Generate experimental audio content\n    - Explore AI-composed musical ideas and patterns",
      "namespace": "huggingface.text_to_audio",
      "node_type": "huggingface.text_to_audio.DanceDiffusion",
      "properties": [
        {
          "name": "audio_length_in_s",
          "type": {
            "type": "float"
          },
          "default": 4.0,
          "title": "Audio Length In S",
          "description": "Duration of the generated audio in seconds (1-30).",
          "min": 1.0,
          "max": 30.0
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 50,
          "title": "Num Inference Steps",
          "description": "Denoising steps. More steps = better quality but slower. 50-200 is typical.",
          "min": 1.0,
          "max": 1000.0
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Seed",
          "description": "Random seed for reproducible generation. Use -1 for random.",
          "min": -1.0
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "recommended_models": [
        {
          "id": "harmonai/maestro-150k",
          "type": "hf.text_to_audio",
          "name": "harmonai/maestro-150k",
          "repo_id": "harmonai/maestro-150k",
          "allow_patterns": [
            "*.bin",
            "*.json",
            "*.txt"
          ],
          "size_on_disk": 885946620,
          "tags": [
            "diffusers",
            "audio-generation",
            "license:mit",
            "diffusers:DanceDiffusionPipeline",
            "region:us"
          ],
          "has_model_index": true,
          "downloads": 32,
          "likes": 17
        }
      ],
      "basic_fields": [
        "model",
        "prompt"
      ]
    },
    {
      "title": "Music Gen",
      "description": "Generates music and audio from text descriptions using Meta's MusicGen models.\n    audio, music, generation, huggingface, text-to-audio, soundtrack\n\n    Use cases:\n    - Create custom background music for videos, games, and podcasts\n    - Generate sound effects from textual descriptions\n    - Prototype musical ideas and compositions quickly\n    - Produce royalty-free audio content for creative projects\n    - Build AI-powered music generation applications",
      "namespace": "huggingface.text_to_audio",
      "node_type": "huggingface.text_to_audio.MusicGen",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.text_to_audio"
          },
          "default": {
            "type": "hf.text_to_audio",
            "repo_id": "",
            "path": null,
            "variant": null,
            "allow_patterns": null,
            "ignore_patterns": null
          },
          "title": "Model",
          "description": "The MusicGen model variant. Small is fastest; Large offers best quality; Melody can condition on audio input; Stereo produces 2-channel output."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Text Prompt",
          "description": "Describe the music you want to generate (e.g., 'upbeat jazz piano with drums' or 'calm ambient soundscape')."
        },
        {
          "name": "max_new_tokens",
          "type": {
            "type": "int"
          },
          "default": 1024,
          "title": "Max New Tokens",
          "description": "Controls audio length: ~256 tokens \u2248 5 seconds. Higher values produce longer audio."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "recommended_models": [
        {
          "id": "facebook/musicgen-small",
          "type": "hf.text_to_audio",
          "name": "facebook/musicgen-small",
          "repo_id": "facebook/musicgen-small",
          "allow_patterns": [
            "*.safetensors",
            "*.json",
            "*.model"
          ],
          "size_on_disk": 2367653971,
          "pipeline_tag": "text-to-audio",
          "tags": [
            "transformers",
            "pytorch",
            "safetensors",
            "musicgen",
            "text-to-audio",
            "arxiv:2306.05284",
            "license:cc-by-nc-4.0",
            "endpoints_compatible",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 99513,
          "likes": 466
        },
        {
          "id": "facebook/musicgen-medium",
          "type": "hf.text_to_audio",
          "name": "facebook/musicgen-medium",
          "repo_id": "facebook/musicgen-medium",
          "allow_patterns": [
            "*.safetensors",
            "*.json",
            "*.model"
          ],
          "size_on_disk": 3226683,
          "pipeline_tag": "text-to-audio",
          "tags": [
            "transformers",
            "pytorch",
            "musicgen",
            "text-to-audio",
            "arxiv:2306.05284",
            "license:cc-by-nc-4.0",
            "endpoints_compatible",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 1338649,
          "likes": 153
        },
        {
          "id": "facebook/musicgen-large",
          "type": "hf.text_to_audio",
          "name": "facebook/musicgen-large",
          "repo_id": "facebook/musicgen-large",
          "allow_patterns": [
            "*.safetensors",
            "*.json",
            "*.model"
          ],
          "size_on_disk": 3324787,
          "pipeline_tag": "text-to-audio",
          "tags": [
            "transformers",
            "pytorch",
            "musicgen",
            "text-to-audio",
            "arxiv:2306.05284",
            "license:cc-by-nc-4.0",
            "endpoints_compatible",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 11575,
          "likes": 498
        },
        {
          "id": "facebook/musicgen-melody",
          "type": "hf.text_to_audio",
          "name": "facebook/musicgen-melody",
          "repo_id": "facebook/musicgen-melody",
          "allow_patterns": [
            "*.safetensors",
            "*.json",
            "*.model"
          ],
          "size_on_disk": 6232963744,
          "pipeline_tag": "text-to-audio",
          "tags": [
            "transformers",
            "safetensors",
            "musicgen_melody",
            "text-to-audio",
            "musicgen",
            "arxiv:2306.05284",
            "license:cc-by-nc-4.0",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 3938,
          "likes": 247
        },
        {
          "id": "facebook/musicgen-stereo-small",
          "type": "hf.text_to_audio",
          "name": "facebook/musicgen-stereo-small",
          "repo_id": "facebook/musicgen-stereo-small",
          "allow_patterns": [
            "*.safetensors",
            "*.json",
            "*.model"
          ],
          "size_on_disk": 3650614378,
          "pipeline_tag": "text-to-audio",
          "tags": [
            "transformers",
            "pytorch",
            "safetensors",
            "musicgen",
            "text-to-audio",
            "audiocraft",
            "arxiv:2306.05284",
            "license:cc-by-nc-4.0",
            "endpoints_compatible",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 1565,
          "likes": 38
        },
        {
          "id": "facebook/musicgen-stereo-large",
          "type": "hf.text_to_audio",
          "name": "facebook/musicgen-stereo-large",
          "repo_id": "facebook/musicgen-stereo-large",
          "allow_patterns": [
            "*.safetensors",
            "*.json",
            "*.model"
          ],
          "size_on_disk": 6930444453,
          "pipeline_tag": "text-to-audio",
          "tags": [
            "transformers",
            "pytorch",
            "safetensors",
            "musicgen",
            "text-to-audio",
            "audiocraft",
            "arxiv:2306.05284",
            "license:cc-by-nc-4.0",
            "endpoints_compatible",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 417,
          "likes": 83
        }
      ],
      "basic_fields": [
        "model",
        "prompt"
      ]
    },
    {
      "title": "Music LDM",
      "description": "Generates music from text descriptions using latent diffusion models.\n    audio, music, generation, huggingface, text-to-audio, diffusion\n\n    Use cases:\n    - Create custom background music for videos and games\n    - Generate music clips based on textual mood descriptions\n    - Produce audio content for multimedia projects\n    - Explore AI-generated music for creative inspiration",
      "namespace": "huggingface.text_to_audio",
      "node_type": "huggingface.text_to_audio.MusicLDM",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.text_to_audio"
          },
          "default": {
            "type": "hf.text_to_audio",
            "repo_id": "",
            "path": null,
            "variant": null,
            "allow_patterns": null,
            "ignore_patterns": null
          },
          "title": "Model",
          "description": "The MusicLDM model to use for audio generation."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Text Prompt",
          "description": "Describe the music you want (e.g., 'electronic dance music with heavy bass')."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 10,
          "title": "Inference Steps",
          "description": "Number of denoising steps. More steps = better quality but slower generation."
        },
        {
          "name": "audio_length_in_s",
          "type": {
            "type": "float"
          },
          "default": 5.0,
          "title": "Audio Length",
          "description": "Duration of the generated audio in seconds."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "recommended_models": [
        {
          "id": "ucsd-reach/musicldm",
          "type": "hf.text_to_audio",
          "name": "ucsd-reach/musicldm",
          "repo_id": "ucsd-reach/musicldm",
          "allow_patterns": [
            "*.safetensors",
            "*.json",
            "*.txt"
          ],
          "size_on_disk": 1962535221,
          "tags": [
            "diffusers",
            "safetensors",
            "arxiv:2308.01546",
            "license:cc-by-nc-sa-4.0",
            "diffusers:MusicLDMPipeline",
            "region:us"
          ],
          "has_model_index": true,
          "downloads": 181,
          "likes": 7
        }
      ],
      "basic_fields": [
        "model",
        "prompt"
      ]
    },
    {
      "title": "Stable Audio",
      "description": "Generates high-quality, long-form audio from text prompts using Stability AI's Stable Audio.\n    audio, generation, synthesis, text-to-audio, music, sound-effects\n\n    Use cases:\n    - Create professional-quality music and soundtracks\n    - Generate ambient sounds and environmental audio\n    - Produce sound effects for multimedia projects\n    - Create experimental and artistic audio content\n    - Generate up to 5 minutes of continuous audio",
      "namespace": "huggingface.text_to_audio",
      "node_type": "huggingface.text_to_audio.StableAudio",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "A peaceful piano melody.",
          "title": "Prompt",
          "description": "Text description of the audio you want to generate."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "Low quality.",
          "title": "Negative Prompt",
          "description": "Describe what to avoid in the generated audio (e.g., 'noise, distortion')."
        },
        {
          "name": "duration",
          "type": {
            "type": "float"
          },
          "default": 10.0,
          "title": "Duration",
          "description": "Duration of the generated audio in seconds. Stable Audio supports up to 300 seconds.",
          "min": 1.0,
          "max": 300.0
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 200,
          "title": "Num Inference Steps",
          "description": "Denoising steps. 200 is recommended for quality; lower values for faster generation.",
          "min": 50.0,
          "max": 500.0
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Seed",
          "description": "Random seed for reproducible generation. Use -1 for random.",
          "min": -1.0
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "recommended_models": [
        {
          "id": "stabilityai/stable-audio-open-1.0",
          "type": "hf.text_to_audio",
          "name": "stabilityai/stable-audio-open-1.0",
          "repo_id": "stabilityai/stable-audio-open-1.0",
          "allow_patterns": [
            "*.safetensors",
            "*.json",
            "*.txt"
          ],
          "size_on_disk": 10148754510,
          "pipeline_tag": "text-to-audio",
          "tags": [
            "stable-audio-tools",
            "diffusers",
            "safetensors",
            "text-to-audio",
            "en",
            "arxiv:2407.14358",
            "license:other",
            "region:us"
          ],
          "has_model_index": true,
          "downloads": 65056,
          "likes": 1356
        }
      ],
      "basic_fields": [
        "model",
        "prompt"
      ]
    },
    {
      "title": "Question Answering",
      "description": "Extracts answers to questions from a given text context using extractive QA models.\n    text, question-answering, NLP, reading-comprehension\n\n    Use cases:\n    - Build automated FAQ and customer support systems\n    - Extract specific information from documents and articles\n    - Create reading comprehension and study tools\n    - Enable natural language queries over text databases\n    - Analyze contracts and legal documents for key details",
      "namespace": "huggingface.question_answering",
      "node_type": "huggingface.question_answering.QuestionAnswering",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.question_answering"
          },
          "default": {
            "type": "hf.question_answering",
            "repo_id": "",
            "path": null,
            "variant": null,
            "allow_patterns": null,
            "ignore_patterns": null
          },
          "title": "Model",
          "description": "The extractive QA model. DistilBERT is fast; BERT-large and RoBERTa offer higher accuracy."
        },
        {
          "name": "context",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Context",
          "description": "The text passage containing the information to answer questions from."
        },
        {
          "name": "question",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Question",
          "description": "The question to answer based on the provided context."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "str"
          },
          "name": "answer"
        },
        {
          "type": {
            "type": "float"
          },
          "name": "score"
        },
        {
          "type": {
            "type": "int"
          },
          "name": "start"
        },
        {
          "type": {
            "type": "int"
          },
          "name": "end"
        }
      ],
      "recommended_models": [
        {
          "id": "distilbert-base-cased-distilled-squad",
          "type": "hf.question_answering",
          "name": "distilbert-base-cased-distilled-squad",
          "repo_id": "distilbert-base-cased-distilled-squad",
          "allow_patterns": [
            "*.json",
            "*.txt",
            "*.safetensors"
          ],
          "size_on_disk": 261431925,
          "pipeline_tag": "question-answering",
          "tags": [
            "transformers",
            "pytorch",
            "tf",
            "rust",
            "safetensors",
            "openvino",
            "distilbert",
            "question-answering",
            "en",
            "dataset:squad",
            "arxiv:1910.01108",
            "arxiv:1910.09700",
            "license:apache-2.0",
            "model-index",
            "endpoints_compatible",
            "deploy:azure",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 190864,
          "likes": 265
        },
        {
          "id": "bert-large-uncased-whole-word-masking-finetuned-squad",
          "type": "hf.question_answering",
          "name": "bert-large-uncased-whole-word-masking-finetuned-squad",
          "repo_id": "bert-large-uncased-whole-word-masking-finetuned-squad",
          "allow_patterns": [
            "*.json",
            "*.txt",
            "*.safetensors"
          ],
          "size_on_disk": 1341320821,
          "pipeline_tag": "question-answering",
          "tags": [
            "transformers",
            "pytorch",
            "tf",
            "jax",
            "safetensors",
            "bert",
            "question-answering",
            "en",
            "dataset:bookcorpus",
            "dataset:wikipedia",
            "arxiv:1810.04805",
            "license:apache-2.0",
            "endpoints_compatible",
            "deploy:azure",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 275511,
          "likes": 180
        },
        {
          "id": "deepset/roberta-base-squad2",
          "type": "hf.question_answering",
          "name": "deepset/roberta-base-squad2",
          "repo_id": "deepset/roberta-base-squad2",
          "allow_patterns": [
            "*.json",
            "*.txt",
            "*.safetensors"
          ],
          "size_on_disk": 497611004,
          "pipeline_tag": "question-answering",
          "tags": [
            "transformers",
            "pytorch",
            "tf",
            "jax",
            "rust",
            "safetensors",
            "roberta",
            "question-answering",
            "en",
            "dataset:squad_v2",
            "base_model:FacebookAI/roberta-base",
            "base_model:finetune:FacebookAI/roberta-base",
            "license:cc-by-4.0",
            "model-index",
            "endpoints_compatible",
            "deploy:azure",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 726838,
          "likes": 931
        },
        {
          "id": "distilbert-base-uncased-distilled-squad",
          "type": "hf.question_answering",
          "name": "distilbert-base-uncased-distilled-squad",
          "repo_id": "distilbert-base-uncased-distilled-squad",
          "allow_patterns": [
            "*.json",
            "*.txt",
            "*.safetensors"
          ],
          "size_on_disk": 266168105,
          "pipeline_tag": "question-answering",
          "tags": [
            "transformers",
            "pytorch",
            "tf",
            "tflite",
            "coreml",
            "safetensors",
            "distilbert",
            "question-answering",
            "en",
            "dataset:squad",
            "arxiv:1910.01108",
            "arxiv:1910.09700",
            "license:apache-2.0",
            "endpoints_compatible",
            "deploy:azure",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 66358,
          "likes": 118
        }
      ],
      "basic_fields": [
        "model",
        "context",
        "question"
      ]
    },
    {
      "title": "Table Question Answering",
      "description": "Answers natural language questions about tabular data using table QA models.\n    table, question-answering, NLP, data-analysis\n\n    Use cases:\n    - Query spreadsheets and databases using natural language\n    - Extract insights from financial reports and data tables\n    - Build conversational interfaces for data exploration\n    - Automate data analysis with question-based queries\n    - Enable non-technical users to query structured data",
      "namespace": "huggingface.question_answering",
      "node_type": "huggingface.question_answering.TableQuestionAnswering",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.table_question_answering"
          },
          "default": {
            "type": "hf.table_question_answering",
            "repo_id": "",
            "path": null,
            "variant": null,
            "allow_patterns": null,
            "ignore_patterns": null
          },
          "title": "Model",
          "description": "The table QA model. TAPAS models handle complex queries; TAPEX offers fact verification."
        },
        {
          "name": "dataframe",
          "type": {
            "type": "dataframe"
          },
          "default": {
            "type": "dataframe",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "columns": null
          },
          "title": "Table",
          "description": "The table data to query. Columns should have clear, descriptive headers."
        },
        {
          "name": "question",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Question",
          "description": "Your question about the table data (e.g., 'What is the total revenue?' or 'Which product sold the most?')."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "str"
          },
          "name": "answer"
        },
        {
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "tuple",
                "type_args": [
                  {
                    "type": "int"
                  },
                  {
                    "type": "int"
                  }
                ]
              }
            ]
          },
          "name": "coordinates"
        },
        {
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "name": "cells"
        },
        {
          "type": {
            "type": "str"
          },
          "name": "aggregator"
        }
      ],
      "recommended_models": [
        {
          "id": "google/tapas-base-finetuned-wtq",
          "type": "hf.table_question_answering",
          "name": "google/tapas-base-finetuned-wtq",
          "repo_id": "google/tapas-base-finetuned-wtq",
          "allow_patterns": [
            "*.json",
            "*.txt",
            "*.safetensors"
          ],
          "size_on_disk": 264329,
          "pipeline_tag": "table-question-answering",
          "tags": [
            "transformers",
            "pytorch",
            "tf",
            "tapas",
            "table-question-answering",
            "en",
            "dataset:wikitablequestions",
            "arxiv:2004.02349",
            "arxiv:2010.00571",
            "arxiv:1508.00305",
            "license:apache-2.0",
            "endpoints_compatible",
            "deploy:azure",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 11571,
          "likes": 233
        },
        {
          "id": "google/tapas-large-finetuned-wtq",
          "type": "hf.table_question_answering",
          "name": "google/tapas-large-finetuned-wtq",
          "repo_id": "google/tapas-large-finetuned-wtq",
          "allow_patterns": [
            "*.json",
            "*.txt",
            "*.safetensors"
          ],
          "size_on_disk": 1347249613,
          "pipeline_tag": "table-question-answering",
          "tags": [
            "transformers",
            "pytorch",
            "tf",
            "safetensors",
            "tapas",
            "table-question-answering",
            "en",
            "dataset:wikitablequestions",
            "arxiv:2004.02349",
            "arxiv:2010.00571",
            "arxiv:1508.00305",
            "license:apache-2.0",
            "endpoints_compatible",
            "deploy:azure",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 2498,
          "likes": 148
        },
        {
          "id": "microsoft/tapex-large-finetuned-tabfact",
          "type": "hf.table_question_answering",
          "name": "microsoft/tapex-large-finetuned-tabfact",
          "repo_id": "microsoft/tapex-large-finetuned-tabfact",
          "allow_patterns": [
            "*.json",
            "*.txt",
            "*.safetensors"
          ],
          "size_on_disk": 1358708,
          "pipeline_tag": "table-question-answering",
          "tags": [
            "transformers",
            "pytorch",
            "bart",
            "text-classification",
            "tapex",
            "table-question-answering",
            "en",
            "dataset:tab_fact",
            "arxiv:2107.07653",
            "license:mit",
            "endpoints_compatible",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 1004,
          "likes": 8
        }
      ],
      "basic_fields": [
        "model",
        "dataframe",
        "question"
      ]
    },
    {
      "title": "Find Segment",
      "description": "Extracts a specific segment mask by label from a list of segmentation results.\n    image, segmentation, object-detection, mask, filter\n\n    Use cases:\n    - Extract a specific object mask (e.g., 'person', 'car') from segmentation output\n    - Filter segmentation results to focus on a single category\n    - Isolate regions for further processing or compositing",
      "namespace": "huggingface.image_segmentation",
      "node_type": "huggingface.image_segmentation.FindSegment",
      "properties": [
        {
          "name": "segments",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "image_segmentation_result"
              }
            ]
          },
          "default": [],
          "title": "Segmentation Masks",
          "description": "List of segmentation results from Segmentation or SAM2Segmentation nodes."
        },
        {
          "name": "segment_label",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Label",
          "description": "The exact label name to find (e.g., 'person', 'wall', 'sky'). Must match a label in the segments list."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "segments",
        "segment_label"
      ]
    },
    {
      "title": "SAM2 Segmentation",
      "description": "Performs automatic instance segmentation using Meta's Segment Anything Model 2 (SAM2).\n    image, segmentation, object-detection, scene-parsing, mask, SAM2\n\n    Use cases:\n    - Automatically segment all distinct objects in an image\n    - Generate high-quality masks without manual prompts\n    - Extract individual objects for image editing workflows\n    - Build interactive segmentation applications\n    - Enable precise object selection in creative tools",
      "namespace": "huggingface.image_segmentation",
      "node_type": "huggingface.image_segmentation.SAM2Segmentation",
      "properties": [
        {
          "name": "image",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Input Image",
          "description": "The image to segment. SAM2 automatically identifies and masks all distinct objects."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "image"
              }
            ]
          },
          "name": "output"
        }
      ],
      "recommended_models": [
        {
          "id": "facebook/sam2-hiera-large",
          "type": "hf.model",
          "name": "facebook/sam2-hiera-large",
          "repo_id": "facebook/sam2-hiera-large",
          "size_on_disk": 1795815850,
          "pipeline_tag": "mask-generation",
          "tags": [
            "transformers",
            "safetensors",
            "sam2_video",
            "feature-extraction",
            "mask-generation",
            "arxiv:2408.00714",
            "license:apache-2.0",
            "endpoints_compatible",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 13148,
          "likes": 115
        }
      ],
      "basic_fields": [
        "image"
      ]
    },
    {
      "title": "Image Segmentation",
      "description": "Performs semantic segmentation on images, identifying and labeling different regions with pixel-level precision.\n    image, segmentation, object-detection, scene-parsing, computer-vision\n\n    Use cases:\n    - Segment and identify objects, people, or regions in images\n    - Extract clothing items or body parts for fashion applications\n    - Parse indoor/outdoor scenes into semantic components\n    - Enable background removal or replacement in photos\n    - Build autonomous driving perception systems",
      "namespace": "huggingface.image_segmentation",
      "node_type": "huggingface.image_segmentation.Segmentation",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.image_segmentation"
          },
          "default": {
            "type": "hf.image_segmentation",
            "repo_id": "nvidia/segformer-b3-finetuned-ade-512-512",
            "path": null,
            "variant": null,
            "allow_patterns": null,
            "ignore_patterns": null
          },
          "title": "Model",
          "description": "The segmentation model. SegFormer-ADE for general scenes, specialized models for clothing or body parts."
        },
        {
          "name": "image",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image",
          "description": "The input image to segment. Larger images may require more processing time."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "image_segmentation_result"
              }
            ]
          },
          "name": "output"
        }
      ],
      "recommended_models": [
        {
          "id": "nvidia/segformer-b3-finetuned-ade-512-512",
          "type": "hf.image_segmentation",
          "name": "nvidia/segformer-b3-finetuned-ade-512-512",
          "repo_id": "nvidia/segformer-b3-finetuned-ade-512-512",
          "allow_patterns": [
            "README.md",
            "*.bin",
            "*.json",
            "**/*.json"
          ],
          "size_on_disk": 189618295,
          "pipeline_tag": "image-segmentation",
          "tags": [
            "transformers",
            "pytorch",
            "tf",
            "segformer",
            "vision",
            "image-segmentation",
            "dataset:scene_parse_150",
            "arxiv:2105.15203",
            "license:other",
            "endpoints_compatible",
            "deploy:azure",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 11231,
          "likes": 12
        },
        {
          "id": "mattmdjaga/segformer_b2_clothes",
          "type": "hf.image_segmentation",
          "name": "mattmdjaga/segformer_b2_clothes",
          "repo_id": "mattmdjaga/segformer_b2_clothes",
          "allow_patterns": [
            "README.md",
            "*.bin",
            "*.json",
            "**/*.json"
          ],
          "size_on_disk": 109887076,
          "pipeline_tag": "image-segmentation",
          "tags": [
            "transformers",
            "pytorch",
            "onnx",
            "safetensors",
            "segformer",
            "vision",
            "image-segmentation",
            "dataset:mattmdjaga/human_parsing_dataset",
            "arxiv:2105.15203",
            "license:other",
            "endpoints_compatible",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 131171,
          "likes": 471
        }
      ],
      "basic_fields": [
        "model",
        "image"
      ]
    },
    {
      "title": "Visualize Segmentation",
      "description": "Renders segmentation masks as colored overlays on the original image with labeled regions.\n    image, segmentation, visualization, mask, annotation\n\n    Use cases:\n    - Visualize and verify segmentation model results\n    - Create labeled images for documentation or presentations\n    - Compare different segmentation techniques visually\n    - Generate annotated images for training data review",
      "namespace": "huggingface.image_segmentation",
      "node_type": "huggingface.image_segmentation.VisualizeSegmentation",
      "properties": [
        {
          "name": "image",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image",
          "description": "The original image to overlay segmentation masks on."
        },
        {
          "name": "segments",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "image_segmentation_result"
              }
            ]
          },
          "default": [],
          "title": "Segmentation Masks",
          "description": "List of segmentation results to visualize. Each segment gets a distinct color and label."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "segments"
      ]
    },
    {
      "title": "Chroma",
      "description": "Generates high-quality images from text prompts using Chroma, a Flux-based architecture with enhanced color control.\n    image, generation, AI, text-to-image, flux, chroma, transformer, artistic\n\n    Use cases:\n    - Generate professional-quality images with precise color control\n    - Create artistic images with advanced attention mechanisms\n    - Produce images with optimized memory usage via CPU offload\n    - Build creative applications requiring high-fidelity output",
      "namespace": "huggingface.text_to_image",
      "node_type": "huggingface.text_to_image.Chroma",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "A high-fashion close-up portrait of a blonde woman in clear sunglasses. The image uses a bold teal and red color split for dramatic lighting. The background is a simple teal-green. The photo is sharp and well-composed, and is designed for viewing with anaglyph 3D glasses for optimal effect. It looks professionally done.",
          "title": "Prompt",
          "description": "Detailed text description of the image to generate."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "low quality, ugly, unfinished, out of focus, deformed, disfigure, blurry, smudged, restricted palette, flat colors",
          "title": "Negative Prompt",
          "description": "Describe what to avoid (e.g., 'blurry, low quality, distorted')."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.0,
          "title": "Guidance Scale",
          "description": "Prompt adherence strength. 2-5 is typical for Chroma.",
          "min": 0.0,
          "max": 30.0
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 40,
          "title": "Num Inference Steps",
          "description": "Denoising steps. 30-50 is typical; more = better quality but slower.",
          "min": 1.0,
          "max": 100.0
        },
        {
          "name": "height",
          "type": {
            "type": "int"
          },
          "default": 1024,
          "title": "Height",
          "description": "Output image height in pixels.",
          "min": 256.0,
          "max": 2048.0
        },
        {
          "name": "width",
          "type": {
            "type": "int"
          },
          "default": 1024,
          "title": "Width",
          "description": "Output image width in pixels.",
          "min": 256.0,
          "max": 2048.0
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducible generation. Use -1 for random.",
          "min": -1.0
        },
        {
          "name": "max_sequence_length",
          "type": {
            "type": "int"
          },
          "default": 512,
          "title": "Max Sequence Length",
          "description": "Maximum prompt length in tokens.",
          "min": 1.0,
          "max": 512.0
        },
        {
          "name": "enable_cpu_offload",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Cpu Offload",
          "description": "Offload model components to CPU to reduce VRAM usage."
        },
        {
          "name": "enable_attention_slicing",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Attention Slicing",
          "description": "Process attention in slices to reduce memory usage."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "recommended_models": [
        {
          "id": "lodestones/Chroma",
          "type": "hf.text_to_image",
          "name": "lodestones/Chroma",
          "repo_id": "lodestones/Chroma",
          "allow_patterns": [
            "**/*.safetensors",
            "**/*.json",
            "**/*.txt",
            "*.json"
          ],
          "size_on_disk": 205492973495,
          "pipeline_tag": "text-to-image",
          "tags": [
            "pytorch",
            "diffusers",
            "safetensors",
            "text-to-image",
            "image-generation",
            "chroma",
            "not-for-all-audiences",
            "en",
            "license:apache-2.0",
            "region:us"
          ],
          "has_model_index": true,
          "downloads": 0,
          "likes": 1291
        }
      ],
      "basic_fields": [
        "prompt",
        "height",
        "width",
        "seed"
      ]
    },
    {
      "title": "Flux",
      "description": "Generates high-quality images using Black Forest Labs' FLUX diffusion models with Nunchaku quantization.\n    image, generation, AI, text-to-image, flux, quantization, high-quality\n\n    Use cases:\n    - Generate high-fidelity images with excellent text rendering\n    - Create images with memory-efficient INT4/FP4 quantization\n    - Fast generation with FLUX.1-schnell (4 steps)\n    - High-quality generation with FLUX.1-dev\n    - Build production image generation systems",
      "namespace": "huggingface.text_to_image",
      "node_type": "huggingface.text_to_image.Flux",
      "properties": [
        {
          "name": "variant",
          "type": {
            "type": "enum",
            "values": [
              "schnell",
              "dev"
            ],
            "type_name": "nodetool.nodes.huggingface.text_to_image.FluxVariant"
          },
          "default": "dev",
          "title": "Variant",
          "description": "FLUX variant: 'schnell' for fast 4-step generation, 'dev' for higher quality with more steps."
        },
        {
          "name": "quantization",
          "type": {
            "type": "enum",
            "values": [
              "fp16",
              "fp4",
              "int4"
            ],
            "type_name": "nodetool.nodes.huggingface.text_to_image.FluxQuantization"
          },
          "default": "int4",
          "title": "Quantization",
          "description": "Quantization level: INT4/FP4 for lower VRAM, FP16 for full precision."
        },
        {
          "name": "enable_cpu_offload",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Cpu Offload",
          "description": "Offload model components to CPU to reduce VRAM usage."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "A cat holding a sign that says hello world",
          "title": "Prompt",
          "description": "Text description of the image to generate. FLUX excels at text rendering."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "Prompt adherence strength. Use 0.0 for schnell, 3-4 for dev.",
          "min": 0.0,
          "max": 20.0
        },
        {
          "name": "width",
          "type": {
            "type": "int"
          },
          "default": 1024,
          "title": "Width",
          "description": "Output image width in pixels. 1024 is recommended.",
          "min": 64.0,
          "max": 2048.0
        },
        {
          "name": "height",
          "type": {
            "type": "int"
          },
          "default": 1024,
          "title": "Height",
          "description": "Output image height in pixels. 1024 is recommended.",
          "min": 64.0,
          "max": 2048.0
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 20,
          "title": "Num Inference Steps",
          "description": "Denoising steps. Schnell uses 4 steps; dev uses 20-50.",
          "min": 1.0,
          "max": 100.0
        },
        {
          "name": "max_sequence_length",
          "type": {
            "type": "int"
          },
          "default": 512,
          "title": "Max Sequence Length",
          "description": "Maximum prompt length. Use 256 for schnell, 512 for dev.",
          "min": 1.0,
          "max": 512.0
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducible generation. Use -1 for random.",
          "min": -1.0
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "recommended_models": [
        {
          "id": "black-forest-labs/FLUX.1-schnell",
          "type": "hf.flux",
          "name": "black-forest-labs/FLUX.1-schnell",
          "repo_id": "black-forest-labs/FLUX.1-schnell",
          "allow_patterns": [
            "*.json",
            "*.txt",
            "scheduler/*",
            "vae/*",
            "text_encoder/*",
            "tokenizer/*",
            "tokenizer_2/*"
          ],
          "size_on_disk": 418780386,
          "pipeline_tag": "text-to-image",
          "tags": [
            "diffusers",
            "safetensors",
            "text-to-image",
            "image-generation",
            "flux",
            "en",
            "license:apache-2.0",
            "endpoints_compatible",
            "diffusers:FluxPipeline",
            "region:us"
          ],
          "has_model_index": true,
          "downloads": 771123,
          "likes": 4452
        },
        {
          "id": "black-forest-labs/FLUX.1-dev",
          "type": "hf.flux",
          "name": "black-forest-labs/FLUX.1-dev",
          "repo_id": "black-forest-labs/FLUX.1-dev",
          "allow_patterns": [
            "*.json",
            "*.txt",
            "scheduler/*",
            "vae/*",
            "text_encoder/*",
            "tokenizer/*",
            "tokenizer_2/*"
          ],
          "size_on_disk": 418780928,
          "pipeline_tag": "text-to-image",
          "tags": [
            "diffusers",
            "safetensors",
            "text-to-image",
            "image-generation",
            "flux",
            "en",
            "license:other",
            "endpoints_compatible",
            "diffusers:FluxPipeline",
            "region:us"
          ],
          "has_model_index": true,
          "downloads": 970417,
          "likes": 11997
        },
        {
          "id": "nunchaku-tech/nunchaku-flux.1-schnell:svdq-int4_r32-flux.1-schnell.safetensors",
          "type": "hf.flux",
          "name": "nunchaku-tech/nunchaku-flux.1-schnell",
          "repo_id": "nunchaku-tech/nunchaku-flux.1-schnell",
          "path": "svdq-int4_r32-flux.1-schnell.safetensors",
          "size_on_disk": 6747849760,
          "pipeline_tag": "text-to-image",
          "tags": [
            "diffusers",
            "text-to-image",
            "SVDQuant",
            "FLUX.1-schnell",
            "FLUX.1",
            "Diffusion",
            "Quantization",
            "ICLR2025",
            "en",
            "dataset:mit-han-lab/svdquant-datasets",
            "arxiv:2411.05007",
            "base_model:black-forest-labs/FLUX.1-schnell",
            "base_model:quantized:black-forest-labs/FLUX.1-schnell",
            "license:apache-2.0",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 3802,
          "likes": 8
        },
        {
          "id": "nunchaku-tech/nunchaku-flux.1-schnell:svdq-fp4_r32-flux.1-schnell.safetensors",
          "type": "hf.flux",
          "name": "nunchaku-tech/nunchaku-flux.1-schnell",
          "repo_id": "nunchaku-tech/nunchaku-flux.1-schnell",
          "path": "svdq-fp4_r32-flux.1-schnell.safetensors",
          "size_on_disk": 7018246824,
          "pipeline_tag": "text-to-image",
          "tags": [
            "diffusers",
            "text-to-image",
            "SVDQuant",
            "FLUX.1-schnell",
            "FLUX.1",
            "Diffusion",
            "Quantization",
            "ICLR2025",
            "en",
            "dataset:mit-han-lab/svdquant-datasets",
            "arxiv:2411.05007",
            "base_model:black-forest-labs/FLUX.1-schnell",
            "base_model:quantized:black-forest-labs/FLUX.1-schnell",
            "license:apache-2.0",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 3802,
          "likes": 8
        },
        {
          "id": "nunchaku-tech/nunchaku-flux.1-dev:svdq-int4_r32-flux.1-dev.safetensors",
          "type": "hf.flux",
          "name": "nunchaku-tech/nunchaku-flux.1-dev",
          "repo_id": "nunchaku-tech/nunchaku-flux.1-dev",
          "path": "svdq-int4_r32-flux.1-dev.safetensors",
          "size_on_disk": 6768309832,
          "pipeline_tag": "text-to-image",
          "tags": [
            "diffusers",
            "text-to-image",
            "SVDQuant",
            "FLUX.1-dev",
            "FLUX.1",
            "Diffusion",
            "Quantization",
            "ICLR2025",
            "en",
            "dataset:mit-han-lab/svdquant-datasets",
            "arxiv:2411.05007",
            "base_model:black-forest-labs/FLUX.1-dev",
            "base_model:quantized:black-forest-labs/FLUX.1-dev",
            "license:other",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 17429,
          "likes": 41
        },
        {
          "id": "nunchaku-tech/nunchaku-flux.1-dev:svdq-fp4_r32-flux.1-dev.safetensors",
          "type": "hf.flux",
          "name": "nunchaku-tech/nunchaku-flux.1-dev",
          "repo_id": "nunchaku-tech/nunchaku-flux.1-dev",
          "path": "svdq-fp4_r32-flux.1-dev.safetensors",
          "size_on_disk": 7038706888,
          "pipeline_tag": "text-to-image",
          "tags": [
            "diffusers",
            "text-to-image",
            "SVDQuant",
            "FLUX.1-dev",
            "FLUX.1",
            "Diffusion",
            "Quantization",
            "ICLR2025",
            "en",
            "dataset:mit-han-lab/svdquant-datasets",
            "arxiv:2411.05007",
            "base_model:black-forest-labs/FLUX.1-dev",
            "base_model:quantized:black-forest-labs/FLUX.1-dev",
            "license:other",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 17429,
          "likes": 41
        },
        {
          "id": "nunchaku-tech/nunchaku-t5:awq-int4-flux.1-t5xxl.safetensors",
          "type": "hf.t5",
          "name": "nunchaku-tech/nunchaku-t5",
          "repo_id": "nunchaku-tech/nunchaku-t5",
          "path": "awq-int4-flux.1-t5xxl.safetensors",
          "size_on_disk": 2986819952,
          "pipeline_tag": "text-generation",
          "tags": [
            "transformers",
            "text-generation",
            "AWQ",
            "Quantization",
            "en",
            "dataset:mit-han-lab/svdquant-datasets",
            "arxiv:2411.05007",
            "base_model:google/t5-v1_1-xxl",
            "base_model:quantized:google/t5-v1_1-xxl",
            "license:apache-2.0",
            "endpoints_compatible",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 25
        }
      ],
      "basic_fields": [
        "variant",
        "quantization",
        "prompt",
        "height",
        "width",
        "seed"
      ],
      "model_packs": [
        {
          "id": "flux_schnell_nunchaku_int4",
          "title": "Flux Schnell (Nunchaku INT4)",
          "description": "Fast 4-step Flux with INT4 quantization via Nunchaku. Requires base Schnell repo + quantized transformer + T5 encoder.",
          "tags": [
            "flux",
            "text-to-image",
            "int4",
            "nunchaku",
            "fast",
            "4-step"
          ],
          "models": [
            {
              "id": "black-forest-labs/FLUX.1-schnell",
              "type": "hf.flux",
              "name": "Flux Schnell Base (configs/VAE/tokenizer)",
              "repo_id": "black-forest-labs/FLUX.1-schnell",
              "allow_patterns": [
                "*.json",
                "*.txt",
                "scheduler/*",
                "vae/*",
                "text_encoder/*",
                "tokenizer/*",
                "tokenizer_2/*"
              ]
            },
            {
              "id": "nunchaku-tech/nunchaku-flux.1-schnell:svdq-int4_r32-flux.1-schnell.safetensors",
              "type": "hf.flux",
              "name": "Nunchaku Schnell Transformer (INT4)",
              "repo_id": "nunchaku-tech/nunchaku-flux.1-schnell",
              "path": "svdq-int4_r32-flux.1-schnell.safetensors",
              "size_on_disk": 6400000000
            },
            {
              "id": "nunchaku-tech/nunchaku-t5:awq-int4-flux.1-t5xxl.safetensors",
              "type": "hf.t5",
              "name": "Nunchaku T5-XXL Encoder (INT4)",
              "repo_id": "nunchaku-tech/nunchaku-t5",
              "path": "awq-int4-flux.1-t5xxl.safetensors",
              "size_on_disk": 5000000000
            }
          ],
          "total_size": 11400000000
        },
        {
          "id": "flux_dev_nunchaku_int4",
          "title": "Flux Dev (Nunchaku INT4)",
          "description": "High-quality Flux Dev with INT4 quantization via Nunchaku. Requires base Dev repo + quantized transformer + T5 encoder.",
          "tags": [
            "flux",
            "text-to-image",
            "int4",
            "nunchaku",
            "high-quality"
          ],
          "models": [
            {
              "id": "black-forest-labs/FLUX.1-dev",
              "type": "hf.flux",
              "name": "Flux Dev Base (configs/VAE/tokenizer)",
              "repo_id": "black-forest-labs/FLUX.1-dev",
              "allow_patterns": [
                "*.json",
                "*.txt",
                "scheduler/*",
                "vae/*",
                "text_encoder/*",
                "tokenizer/*",
                "tokenizer_2/*"
              ]
            },
            {
              "id": "nunchaku-tech/nunchaku-flux.1-dev:svdq-int4_r32-flux.1-dev.safetensors",
              "type": "hf.flux",
              "name": "Nunchaku Dev Transformer (INT4)",
              "repo_id": "nunchaku-tech/nunchaku-flux.1-dev",
              "path": "svdq-int4_r32-flux.1-dev.safetensors",
              "size_on_disk": 6400000000
            },
            {
              "id": "nunchaku-tech/nunchaku-t5:awq-int4-flux.1-t5xxl.safetensors",
              "type": "hf.t5",
              "name": "Nunchaku T5-XXL Encoder (INT4)",
              "repo_id": "nunchaku-tech/nunchaku-t5",
              "path": "awq-int4-flux.1-t5xxl.safetensors",
              "size_on_disk": 5000000000
            }
          ],
          "total_size": 11400000000
        }
      ]
    },
    {
      "title": "Flux Control",
      "description": "Generates images using FLUX Control models with depth or other control guidance.\n    image, generation, AI, text-to-image, flux, control, depth, guidance\n\n    Use cases:\n    - Generate images with depth-based control guidance\n    - Create images following structural guidance from control images\n    - High-quality controlled generation with FLUX models\n    - Depth-aware image generation",
      "namespace": "huggingface.text_to_image",
      "node_type": "huggingface.text_to_image.FluxControl",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.controlnet_flux"
          },
          "default": {
            "type": "hf.controlnet_flux",
            "repo_id": "black-forest-labs/FLUX.1-Depth-dev",
            "path": null,
            "variant": null,
            "allow_patterns": null,
            "ignore_patterns": null
          },
          "title": "Model",
          "description": "The FLUX Control model to use for controlled image generation."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "A robot made of exotic candies and chocolates of different kinds. The background is filled with confetti and celebratory gifts.",
          "title": "Prompt",
          "description": "A text prompt describing the desired image."
        },
        {
          "name": "control_image",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Control Image",
          "description": "The control image to guide the generation process."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 10.0,
          "title": "Guidance Scale",
          "description": "The scale for classifier-free guidance.",
          "min": 0.0,
          "max": 30.0
        },
        {
          "name": "width",
          "type": {
            "type": "int"
          },
          "default": 1024,
          "title": "Width",
          "description": "The width of the generated image.",
          "min": 64.0,
          "max": 2048.0
        },
        {
          "name": "height",
          "type": {
            "type": "int"
          },
          "default": 1024,
          "title": "Height",
          "description": "The height of the generated image.",
          "min": 64.0,
          "max": 2048.0
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Num Inference Steps",
          "description": "The number of denoising steps.",
          "min": 1.0,
          "max": 100.0
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for the random number generator. Use -1 for a random seed.",
          "min": -1.0
        },
        {
          "name": "enable_cpu_offload",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Cpu Offload",
          "description": "Enable CPU offload to reduce VRAM usage."
        },
        {
          "name": "quantization",
          "type": {
            "type": "enum",
            "values": [
              "fp16",
              "fp4",
              "int4"
            ],
            "type_name": "nodetool.nodes.huggingface.text_to_image.FluxControlQuantization"
          },
          "default": "int4",
          "title": "Quantization",
          "description": "Quantization level for the FLUX Control transformer."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "recommended_models": [
        {
          "id": "black-forest-labs/FLUX.1-Depth-dev",
          "type": "hf.controlnet_flux",
          "name": "black-forest-labs/FLUX.1-Depth-dev",
          "repo_id": "black-forest-labs/FLUX.1-Depth-dev",
          "allow_patterns": [
            "*.json",
            "*.txt",
            "scheduler/*",
            "vae/*",
            "text_encoder/*",
            "tokenizer/*",
            "tokenizer_2/*",
            "transformer/config.json"
          ],
          "size_on_disk": 832541209,
          "pipeline_tag": "text-to-image",
          "tags": [
            "diffusers",
            "safetensors",
            "image-generation",
            "flux",
            "diffusion-single-file",
            "en",
            "license:other",
            "diffusers:FluxControlPipeline",
            "region:us"
          ],
          "has_model_index": true,
          "downloads": 5283,
          "likes": 229
        },
        {
          "id": "black-forest-labs/FLUX.1-Canny-dev",
          "type": "hf.controlnet_flux",
          "name": "black-forest-labs/FLUX.1-Canny-dev",
          "repo_id": "black-forest-labs/FLUX.1-Canny-dev",
          "allow_patterns": [
            "*.json",
            "*.txt",
            "scheduler/*",
            "vae/*",
            "text_encoder/*",
            "tokenizer/*",
            "tokenizer_2/*",
            "transformer/config.json"
          ],
          "size_on_disk": 832541209,
          "pipeline_tag": "text-to-image",
          "tags": [
            "diffusers",
            "safetensors",
            "image-generation",
            "flux",
            "diffusion-single-file",
            "en",
            "license:other",
            "diffusers:FluxControlPipeline",
            "region:us"
          ],
          "has_model_index": true,
          "downloads": 1855,
          "likes": 234
        },
        {
          "id": "nunchaku-tech/nunchaku-flux.1-depth-dev:svdq-int4_r32-flux.1-depth-dev.safetensors",
          "type": "hf.controlnet_flux",
          "name": "nunchaku-tech/nunchaku-flux.1-depth-dev",
          "repo_id": "nunchaku-tech/nunchaku-flux.1-depth-dev",
          "path": "svdq-int4_r32-flux.1-depth-dev.safetensors",
          "size_on_disk": 6768703136,
          "pipeline_tag": "image-to-image",
          "tags": [
            "diffusers",
            "image-to-image",
            "SVDQuant",
            "FLUX.1-Depth-dev",
            "FLUX.1",
            "Diffusion",
            "Quantization",
            "ICLR2025",
            "en",
            "dataset:mit-han-lab/svdquant-datasets",
            "arxiv:2411.05007",
            "base_model:black-forest-labs/FLUX.1-Depth-dev",
            "base_model:quantized:black-forest-labs/FLUX.1-Depth-dev",
            "license:other",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 2440,
          "likes": 4
        },
        {
          "id": "nunchaku-tech/nunchaku-flux.1-depth-dev:svdq-fp4_r32-flux.1-depth-dev.safetensors",
          "type": "hf.controlnet_flux",
          "name": "nunchaku-tech/nunchaku-flux.1-depth-dev",
          "repo_id": "nunchaku-tech/nunchaku-flux.1-depth-dev",
          "path": "svdq-fp4_r32-flux.1-depth-dev.safetensors",
          "size_on_disk": 7039100192,
          "pipeline_tag": "image-to-image",
          "tags": [
            "diffusers",
            "image-to-image",
            "SVDQuant",
            "FLUX.1-Depth-dev",
            "FLUX.1",
            "Diffusion",
            "Quantization",
            "ICLR2025",
            "en",
            "dataset:mit-han-lab/svdquant-datasets",
            "arxiv:2411.05007",
            "base_model:black-forest-labs/FLUX.1-Depth-dev",
            "base_model:quantized:black-forest-labs/FLUX.1-Depth-dev",
            "license:other",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 2440,
          "likes": 4
        },
        {
          "id": "nunchaku-tech/nunchaku-flux.1-canny-dev:svdq-int4_r32-flux.1-canny-dev.safetensors",
          "type": "hf.controlnet_flux",
          "name": "nunchaku-tech/nunchaku-flux.1-canny-dev",
          "repo_id": "nunchaku-tech/nunchaku-flux.1-canny-dev",
          "path": "svdq-int4_r32-flux.1-canny-dev.safetensors",
          "size_on_disk": 6768703136,
          "pipeline_tag": "image-to-image",
          "tags": [
            "diffusers",
            "image-to-image",
            "SVDQuant",
            "FLUX.1-Canny-dev",
            "FLUX.1",
            "Diffusion",
            "Quantization",
            "ICLR2025",
            "en",
            "dataset:mit-han-lab/svdquant-datasets",
            "arxiv:2411.05007",
            "base_model:black-forest-labs/FLUX.1-Canny-dev",
            "base_model:quantized:black-forest-labs/FLUX.1-Canny-dev",
            "license:other",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 2327,
          "likes": 5
        },
        {
          "id": "nunchaku-tech/nunchaku-flux.1-canny-dev:svdq-fp4_r32-flux.1-canny-dev.safetensors",
          "type": "hf.controlnet_flux",
          "name": "nunchaku-tech/nunchaku-flux.1-canny-dev",
          "repo_id": "nunchaku-tech/nunchaku-flux.1-canny-dev",
          "path": "svdq-fp4_r32-flux.1-canny-dev.safetensors",
          "size_on_disk": 7039100192,
          "pipeline_tag": "image-to-image",
          "tags": [
            "diffusers",
            "image-to-image",
            "SVDQuant",
            "FLUX.1-Canny-dev",
            "FLUX.1",
            "Diffusion",
            "Quantization",
            "ICLR2025",
            "en",
            "dataset:mit-han-lab/svdquant-datasets",
            "arxiv:2411.05007",
            "base_model:black-forest-labs/FLUX.1-Canny-dev",
            "base_model:quantized:black-forest-labs/FLUX.1-Canny-dev",
            "license:other",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 2327,
          "likes": 5
        },
        {
          "id": "nunchaku-tech/nunchaku-t5:awq-int4-flux.1-t5xxl.safetensors",
          "type": "hf.t5",
          "name": "nunchaku-tech/nunchaku-t5",
          "repo_id": "nunchaku-tech/nunchaku-t5",
          "path": "awq-int4-flux.1-t5xxl.safetensors",
          "size_on_disk": 2986819952,
          "pipeline_tag": "text-generation",
          "tags": [
            "transformers",
            "text-generation",
            "AWQ",
            "Quantization",
            "en",
            "dataset:mit-han-lab/svdquant-datasets",
            "arxiv:2411.05007",
            "base_model:google/t5-v1_1-xxl",
            "base_model:quantized:google/t5-v1_1-xxl",
            "license:apache-2.0",
            "endpoints_compatible",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 25
        }
      ],
      "basic_fields": [
        "model",
        "quantization",
        "prompt",
        "control_image",
        "height",
        "width",
        "guidance_scale",
        "seed"
      ]
    },
    {
      "title": "Load Text To Image Model",
      "description": "Loads and validates a Hugging Face text-to-image model for use in downstream nodes.\n    model-loader, text-to-image, pipeline\n\n    Use cases:\n    - Pre-load text-to-image models before running pipelines\n    - Validate model availability and compatibility\n    - Configure model settings for Text2Image processing",
      "namespace": "huggingface.text_to_image",
      "node_type": "huggingface.text_to_image.LoadTextToImageModel",
      "properties": [
        {
          "name": "repo_id",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Repo Id",
          "description": "The Hugging Face repository ID for the text-to-image model."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "hf.text_to_image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "repo_id"
      ]
    },
    {
      "title": "Qwen-Image",
      "description": "Generates images from text prompts using Alibaba's Qwen-Image model with Nunchaku quantization support.\n    image, generation, AI, text-to-image, qwen, quantization, multilingual\n\n    Use cases:\n    - Generate high-quality images with strong multilingual prompt support\n    - Memory-efficient generation using INT4/FP4 quantization\n    - Create images with precise semantic understanding\n    - Build production image generation systems",
      "namespace": "huggingface.text_to_image",
      "node_type": "huggingface.text_to_image.QwenImage",
      "properties": [
        {
          "name": "quantization",
          "type": {
            "type": "enum",
            "values": [
              "fp16",
              "fp4",
              "int4"
            ],
            "type_name": "nodetool.nodes.huggingface.text_to_image.QwenQuantization"
          },
          "default": "int4",
          "title": "Quantization",
          "description": "Quantization level: INT4/FP4 for lower VRAM, FP16 for full precision."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "A cat holding a sign that says hello world",
          "title": "Prompt",
          "description": "Text description of the image to generate."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Describe what to avoid in the image (e.g., 'blurry, low quality')."
        },
        {
          "name": "true_cfg_scale",
          "type": {
            "type": "float"
          },
          "default": 1.0,
          "title": "True Cfg Scale",
          "description": "True CFG scale for enhanced prompt following.",
          "min": 0.0,
          "max": 10.0
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 50,
          "title": "Num Inference Steps",
          "description": "Denoising steps. 30-50 is typical.",
          "min": 1.0,
          "max": 100.0
        },
        {
          "name": "height",
          "type": {
            "type": "int"
          },
          "default": 1024,
          "title": "Height",
          "description": "Output image height in pixels.",
          "min": 256.0,
          "max": 2048.0
        },
        {
          "name": "width",
          "type": {
            "type": "int"
          },
          "default": 1024,
          "title": "Width",
          "description": "Output image width in pixels.",
          "min": 256.0,
          "max": 2048.0
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducible generation. Use -1 for random.",
          "min": -1.0
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "recommended_models": [
        {
          "id": "Qwen/Qwen-Image",
          "type": "hf.qwen_image",
          "name": "Qwen/Qwen-Image",
          "repo_id": "Qwen/Qwen-Image",
          "allow_patterns": [
            "*.json",
            "*.txt",
            "scheduler/*",
            "vae/*",
            "tokenizer/*",
            "tokenizer_2/*"
          ],
          "size_on_disk": 259132589,
          "pipeline_tag": "text-to-image",
          "tags": [
            "diffusers",
            "safetensors",
            "text-to-image",
            "en",
            "zh",
            "arxiv:2508.02324",
            "license:apache-2.0",
            "diffusers:QwenImagePipeline",
            "deploy:azure",
            "region:us"
          ],
          "has_model_index": true,
          "downloads": 295834,
          "likes": 2275
        },
        {
          "id": "nunchaku-tech/nunchaku-qwen-image:svdq-int4_r32-qwen-image.safetensors",
          "type": "hf.qwen_image",
          "name": "nunchaku-tech/nunchaku-qwen-image",
          "repo_id": "nunchaku-tech/nunchaku-qwen-image",
          "path": "svdq-int4_r32-qwen-image.safetensors",
          "size_on_disk": 11521979944,
          "pipeline_tag": "text-to-image",
          "tags": [
            "diffusers",
            "text-to-image",
            "SVDQuant",
            "Qwen-Image",
            "Diffusion",
            "Quantization",
            "ICLR2025",
            "en",
            "dataset:mit-han-lab/svdquant-datasets",
            "arxiv:2411.05007",
            "base_model:Qwen/Qwen-Image",
            "base_model:quantized:Qwen/Qwen-Image",
            "license:apache-2.0",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 63422,
          "likes": 237
        },
        {
          "id": "nunchaku-tech/nunchaku-qwen-image:svdq-fp4_r32-qwen-image.safetensors",
          "type": "hf.qwen_image",
          "name": "nunchaku-tech/nunchaku-qwen-image",
          "repo_id": "nunchaku-tech/nunchaku-qwen-image",
          "path": "svdq-fp4_r32-qwen-image.safetensors",
          "size_on_disk": 11948923656,
          "pipeline_tag": "text-to-image",
          "tags": [
            "diffusers",
            "text-to-image",
            "SVDQuant",
            "Qwen-Image",
            "Diffusion",
            "Quantization",
            "ICLR2025",
            "en",
            "dataset:mit-han-lab/svdquant-datasets",
            "arxiv:2411.05007",
            "base_model:Qwen/Qwen-Image",
            "base_model:quantized:Qwen/Qwen-Image",
            "license:apache-2.0",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 63422,
          "likes": 237
        },
        {
          "id": "Qwen/Qwen2.5-VL-7B-Instruct",
          "type": "hf.qwen2_5_vl",
          "name": "Qwen/Qwen2.5-VL-7B-Instruct",
          "repo_id": "Qwen/Qwen2.5-VL-7B-Instruct",
          "size_on_disk": 16595981281,
          "pipeline_tag": "image-text-to-text",
          "tags": [
            "transformers",
            "safetensors",
            "qwen2_5_vl",
            "image-to-text",
            "multimodal",
            "image-text-to-text",
            "conversational",
            "en",
            "arxiv:2309.00071",
            "arxiv:2409.12191",
            "arxiv:2308.12966",
            "license:apache-2.0",
            "text-generation-inference",
            "endpoints_compatible",
            "deploy:azure",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 3129340,
          "likes": 1390
        }
      ],
      "basic_fields": [
        "quantization",
        "prompt",
        "negative_prompt",
        "height",
        "width",
        "num_inference_steps"
      ],
      "model_packs": [
        {
          "id": "qwen_image_nunchaku_int4",
          "title": "Qwen-Image (Nunchaku INT4)",
          "description": "Qwen-Image with INT4 quantization via Nunchaku. Requires base Qwen-Image repo + quantized transformer.",
          "tags": [
            "qwen",
            "text-to-image",
            "int4",
            "nunchaku"
          ],
          "models": [
            {
              "id": "Qwen/Qwen-Image",
              "type": "hf.qwen_image",
              "name": "Qwen-Image Base (configs/VAE/tokenizer)",
              "repo_id": "Qwen/Qwen-Image",
              "allow_patterns": [
                "*.json",
                "*.txt",
                "scheduler/*",
                "vae/*",
                "tokenizer/*",
                "tokenizer_2/*"
              ]
            },
            {
              "id": "nunchaku-tech/nunchaku-qwen-image:svdq-int4_r32-qwen-image.safetensors",
              "type": "hf.qwen_image",
              "name": "Nunchaku Qwen Transformer (INT4)",
              "repo_id": "nunchaku-tech/nunchaku-qwen-image",
              "path": "svdq-int4_r32-qwen-image.safetensors",
              "size_on_disk": 6500000000
            },
            {
              "id": "Qwen/Qwen2.5-VL-7B-Instruct",
              "type": "hf.qwen_vl",
              "name": "Qwen2.5-VL-7B-Instruct (Text Encoder)",
              "repo_id": "Qwen/Qwen2.5-VL-7B-Instruct"
            }
          ],
          "total_size": 6500000000
        }
      ]
    },
    {
      "title": "Stable Diffusion",
      "description": "Generates images from text prompts using Stable Diffusion 1.x/2.x models.\n    image, generation, AI, text-to-image, SD, creative\n\n    Use cases:\n    - Create custom illustrations and artwork from text descriptions\n    - Generate concept art for games, films, and creative projects\n    - Produce unique visual content for marketing and media\n    - Explore AI-generated art with extensive community models\n    - Build image generation applications with well-understood architecture",
      "namespace": "huggingface.text_to_image",
      "node_type": "huggingface.text_to_image.StableDiffusion",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.stable_diffusion"
          },
          "default": {
            "type": "hf.stable_diffusion",
            "repo_id": "",
            "path": null,
            "variant": null,
            "allow_patterns": null,
            "ignore_patterns": null
          },
          "title": "Model",
          "description": "The model to use for image generation."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt for image generation."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to guide what should not appear in the generated image."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for the random number generator. Use -1 for a random seed.",
          "min": -1.0,
          "max": 4294967295.0
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 25,
          "title": "Num Inference Steps",
          "description": "Number of denoising steps.",
          "min": 1.0,
          "max": 100.0
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 7.5,
          "title": "Guidance Scale",
          "description": "Guidance scale for generation.",
          "min": 1.0,
          "max": 20.0
        },
        {
          "name": "scheduler",
          "type": {
            "type": "enum",
            "values": [
              "DPMSolverSDEScheduler",
              "EulerDiscreteScheduler",
              "LMSDiscreteScheduler",
              "DDIMScheduler",
              "DDPMScheduler",
              "HeunDiscreteScheduler",
              "DPMSolverMultistepScheduler",
              "DEISMultistepScheduler",
              "PNDMScheduler",
              "EulerAncestralDiscreteScheduler",
              "UniPCMultistepScheduler",
              "KDPM2DiscreteScheduler",
              "DPMSolverSinglestepScheduler",
              "KDPM2AncestralDiscreteScheduler"
            ],
            "type_name": "nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionBaseNode.StableDiffusionScheduler"
          },
          "default": "EulerDiscreteScheduler",
          "title": "Scheduler",
          "description": "The scheduler to use for the diffusion process."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "hf.lora_sd_config"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRA models to use for image processing"
        },
        {
          "name": "ip_adapter_model",
          "type": {
            "type": "hf.ip_adapter"
          },
          "default": {
            "type": "hf.ip_adapter",
            "repo_id": "",
            "path": null,
            "variant": null,
            "allow_patterns": null,
            "ignore_patterns": null
          },
          "title": "Ip Adapter Model",
          "description": "The IP adapter model to use for image processing"
        },
        {
          "name": "ip_adapter_image",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Ip Adapter Image",
          "description": "When provided the image will be fed into the IP adapter"
        },
        {
          "name": "ip_adapter_scale",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Ip Adapter Scale",
          "description": "The strength of the IP adapter",
          "min": 0.0,
          "max": 1.0
        },
        {
          "name": "latents",
          "type": {
            "type": "torch_tensor"
          },
          "default": {
            "type": "torch_tensor",
            "value": null,
            "dtype": "<i8",
            "shape": [
              1
            ]
          },
          "title": "Latents",
          "description": "Optional initial latents to start generation from."
        },
        {
          "name": "enable_attention_slicing",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Attention Slicing",
          "description": "Enable attention slicing for the pipeline. This can reduce VRAM usage."
        },
        {
          "name": "enable_tiling",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Tiling",
          "description": "Legacy VAE tiling flag (disabled in favor of PyTorch 2 attention optimizations)."
        },
        {
          "name": "enable_cpu_offload",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Cpu Offload",
          "description": "Enable CPU offload for the pipeline. This can reduce VRAM usage."
        },
        {
          "name": "output_type",
          "type": {
            "type": "enum",
            "values": [
              "Image",
              "Latent"
            ],
            "type_name": "nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionBaseNode.StableDiffusionOutputType"
          },
          "default": "Image",
          "title": "Output Type",
          "description": "The type of output to generate."
        },
        {
          "name": "width",
          "type": {
            "type": "int"
          },
          "default": 512,
          "title": "Width",
          "description": "Output image width in pixels. 512 is standard for SD 1.x.",
          "min": 256.0,
          "max": 1024.0
        },
        {
          "name": "height",
          "type": {
            "type": "int"
          },
          "default": 512,
          "title": "Height",
          "description": "Output image height in pixels. 512 is standard for SD 1.x.",
          "min": 256.0,
          "max": 1024.0
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "image"
        },
        {
          "type": {
            "type": "torch_tensor"
          },
          "name": "latent"
        }
      ],
      "recommended_models": [
        {
          "id": "h94/IP-Adapter:models/ip-adapter_sd15.bin",
          "type": "hf.ip_adapter",
          "name": "h94/IP-Adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "models/ip-adapter_sd15.bin",
          "size_on_disk": 44642825,
          "pipeline_tag": "text-to-image",
          "tags": [
            "diffusers",
            "safetensors",
            "text-to-image",
            "stable-diffusion",
            "en",
            "arxiv:2308.06721",
            "license:apache-2.0",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 1262
        },
        {
          "id": "h94/IP-Adapter:models/ip-adapter_sd15_light.bin",
          "type": "hf.ip_adapter",
          "name": "h94/IP-Adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "models/ip-adapter_sd15_light.bin",
          "size_on_disk": 44642819,
          "pipeline_tag": "text-to-image",
          "tags": [
            "diffusers",
            "safetensors",
            "text-to-image",
            "stable-diffusion",
            "en",
            "arxiv:2308.06721",
            "license:apache-2.0",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 1262
        },
        {
          "id": "h94/IP-Adapter:models/ip-adapter_sd15_vit-G.bin",
          "type": "hf.ip_adapter",
          "name": "h94/IP-Adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "models/ip-adapter_sd15_vit-G.bin",
          "size_on_disk": 46215689,
          "pipeline_tag": "text-to-image",
          "tags": [
            "diffusers",
            "safetensors",
            "text-to-image",
            "stable-diffusion",
            "en",
            "arxiv:2308.06721",
            "license:apache-2.0",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 1262
        },
        {
          "id": "Lykon/DreamShaper:DreamShaper_6.2_BakedVae_pruned.safetensors",
          "type": "hf.stable_diffusion",
          "name": "Lykon/DreamShaper",
          "repo_id": "Lykon/DreamShaper",
          "path": "DreamShaper_6.2_BakedVae_pruned.safetensors",
          "size_on_disk": 2132625894,
          "pipeline_tag": "text-to-image",
          "tags": [
            "diffusers",
            "stable-diffusion",
            "stable-diffusion-diffusers",
            "text-to-image",
            "art",
            "artistic",
            "anime",
            "en",
            "doi:10.57967/hf/0453",
            "license:other",
            "diffusers:StableDiffusionPipeline",
            "region:us"
          ],
          "has_model_index": true,
          "downloads": 86342,
          "likes": 996
        }
      ],
      "basic_fields": [
        "model",
        "prompt",
        "width",
        "height"
      ]
    },
    {
      "title": "Stable Diffusion XL",
      "description": "Generates high-resolution images from text prompts using Stable Diffusion XL.\n    image, generation, AI, text-to-image, SDXL, high-resolution\n\n    Use cases:\n    - Create detailed, high-resolution images (1024x1024) from text\n    - Generate marketing visuals and product imagery\n    - Produce concept art and illustrations with enhanced detail\n    - Create stock imagery and visual content for publications\n    - Build professional image generation applications",
      "namespace": "huggingface.text_to_image",
      "node_type": "huggingface.text_to_image.StableDiffusionXL",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.stable_diffusion_xl"
          },
          "default": {
            "type": "hf.stable_diffusion_xl",
            "repo_id": "",
            "path": null,
            "variant": null,
            "allow_patterns": null,
            "ignore_patterns": null
          },
          "title": "Model",
          "description": "The Stable Diffusion XL model to use for generation."
        },
        {
          "name": "quantization",
          "type": {
            "type": "enum",
            "values": [
              "fp16",
              "fp4",
              "int4"
            ],
            "type_name": "nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionXLQuantization"
          },
          "default": "fp16",
          "title": "Quantization",
          "description": "Quantization level for Stable Diffusion XL (enable INT4/FP4 to use a Nunchaku UNet)."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt for image generation."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to guide what should not appear in the generated image."
        },
        {
          "name": "width",
          "type": {
            "type": "int"
          },
          "default": 1024,
          "title": "Width",
          "description": "Width of the generated image.",
          "min": 64.0,
          "max": 2048.0
        },
        {
          "name": "height",
          "type": {
            "type": "int"
          },
          "default": 1024,
          "title": "Height",
          "description": "Height of the generated image",
          "min": 64.0,
          "max": 2048.0
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for the random number generator.",
          "min": -1.0,
          "max": 1000000.0
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 25,
          "title": "Num Inference Steps",
          "description": "Number of inference steps.",
          "min": 1.0,
          "max": 100.0
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 7.0,
          "title": "Guidance Scale",
          "description": "Guidance scale for generation.",
          "min": 0.0,
          "max": 20.0
        },
        {
          "name": "scheduler",
          "type": {
            "type": "enum",
            "values": [
              "DPMSolverSDEScheduler",
              "EulerDiscreteScheduler",
              "LMSDiscreteScheduler",
              "DDIMScheduler",
              "DDPMScheduler",
              "HeunDiscreteScheduler",
              "DPMSolverMultistepScheduler",
              "DEISMultistepScheduler",
              "PNDMScheduler",
              "EulerAncestralDiscreteScheduler",
              "UniPCMultistepScheduler",
              "KDPM2DiscreteScheduler",
              "DPMSolverSinglestepScheduler",
              "KDPM2AncestralDiscreteScheduler"
            ],
            "type_name": "nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionXLBase.StableDiffusionScheduler"
          },
          "default": "EulerDiscreteScheduler",
          "title": "Scheduler",
          "description": "The scheduler to use for the diffusion process."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "hf.lora_sdxl_config"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRA models to use for image processing"
        },
        {
          "name": "lora_scale",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Lora Scale",
          "description": "Strength of the LoRAs",
          "min": 0.0,
          "max": 1.0
        },
        {
          "name": "ip_adapter_model",
          "type": {
            "type": "hf.ip_adapter"
          },
          "default": {
            "type": "hf.ip_adapter",
            "repo_id": "",
            "path": null,
            "variant": null,
            "allow_patterns": null,
            "ignore_patterns": null
          },
          "title": "Ip Adapter Model",
          "description": "The IP adapter model to use for image processing"
        },
        {
          "name": "ip_adapter_image",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Ip Adapter Image",
          "description": "When provided the image will be fed into the IP adapter"
        },
        {
          "name": "ip_adapter_scale",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Ip Adapter Scale",
          "description": "Strength of the IP adapter image",
          "min": 0.0,
          "max": 1.0
        },
        {
          "name": "enable_attention_slicing",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Attention Slicing",
          "description": "Enable attention slicing for the pipeline. This can reduce VRAM usage."
        },
        {
          "name": "enable_tiling",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Tiling",
          "description": "Legacy VAE tiling flag (disabled in favor of PyTorch 2 attention optimizations)."
        },
        {
          "name": "enable_cpu_offload",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Cpu Offload",
          "description": "Enable CPU offload for the pipeline. This can reduce VRAM usage."
        },
        {
          "name": "output_type",
          "type": {
            "type": "enum",
            "values": [
              "Image",
              "Latent"
            ],
            "type_name": "nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionXLBase.StableDiffusionOutputType"
          },
          "default": "Image",
          "title": "Output Type",
          "description": "The type of output to generate."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "image"
        },
        {
          "type": {
            "type": "torch_tensor"
          },
          "name": "latent"
        }
      ],
      "recommended_models": [
        {
          "id": "h94/IP-Adapter:sdxl_models/ip-adapter_sdxl.bin",
          "type": "hf.ip_adapter",
          "name": "h94/IP-Adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "sdxl_models/ip-adapter_sdxl.bin",
          "size_on_disk": 702585097,
          "pipeline_tag": "text-to-image",
          "tags": [
            "diffusers",
            "safetensors",
            "text-to-image",
            "stable-diffusion",
            "en",
            "arxiv:2308.06721",
            "license:apache-2.0",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 1262
        },
        {
          "id": "h94/IP-Adapter:sdxl_models/ip-adapter_sdxl_vit-h.bin",
          "type": "hf.ip_adapter",
          "name": "h94/IP-Adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "sdxl_models/ip-adapter_sdxl_vit-h.bin",
          "size_on_disk": 698390793,
          "pipeline_tag": "text-to-image",
          "tags": [
            "diffusers",
            "safetensors",
            "text-to-image",
            "stable-diffusion",
            "en",
            "arxiv:2308.06721",
            "license:apache-2.0",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 1262
        },
        {
          "id": "h94/IP-Adapter:sdxl_models/ip-adapter-plus_sdxl_vit-h.bin",
          "type": "hf.ip_adapter",
          "name": "h94/IP-Adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "sdxl_models/ip-adapter-plus_sdxl_vit-h.bin",
          "size_on_disk": 1013454427,
          "pipeline_tag": "text-to-image",
          "tags": [
            "diffusers",
            "safetensors",
            "text-to-image",
            "stable-diffusion",
            "en",
            "arxiv:2308.06721",
            "license:apache-2.0",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 1262
        },
        {
          "id": "stabilityai/stable-diffusion-xl-base-1.0:sd_xl_base_1.0.safetensors",
          "type": "hf.stable_diffusion_xl",
          "name": "stabilityai/stable-diffusion-xl-base-1.0",
          "repo_id": "stabilityai/stable-diffusion-xl-base-1.0",
          "path": "sd_xl_base_1.0.safetensors",
          "size_on_disk": 6938078334,
          "pipeline_tag": "text-to-image",
          "tags": [
            "diffusers",
            "onnx",
            "safetensors",
            "text-to-image",
            "stable-diffusion",
            "arxiv:2307.01952",
            "arxiv:2211.01324",
            "arxiv:2108.01073",
            "arxiv:2112.10752",
            "license:openrail++",
            "endpoints_compatible",
            "diffusers:StableDiffusionXLPipeline",
            "deploy:azure",
            "region:us"
          ],
          "has_model_index": true,
          "downloads": 2122049,
          "likes": 7232
        },
        {
          "id": "Lykon/dreamshaper-xl-v2-turbo:DreamShaperXL_Turbo_v2_1.safetensors",
          "type": "hf.stable_diffusion_xl",
          "name": "Lykon/dreamshaper-xl-v2-turbo",
          "repo_id": "Lykon/dreamshaper-xl-v2-turbo",
          "path": "DreamShaperXL_Turbo_v2_1.safetensors",
          "size_on_disk": 6939220250,
          "pipeline_tag": "text-to-image",
          "tags": [
            "diffusers",
            "safetensors",
            "stable-diffusion",
            "stable-diffusion-diffusers",
            "stable-diffusion-xl",
            "stable-diffusion-xl-turbo",
            "text-to-image",
            "art",
            "artistic",
            "anime",
            "dreamshaper",
            "turbo",
            "lcm",
            "en",
            "license:openrail++",
            "endpoints_compatible",
            "diffusers:StableDiffusionXLPipeline",
            "region:us"
          ],
          "has_model_index": true,
          "downloads": 62403,
          "likes": 70
        },
        {
          "id": "RunDiffusion/Juggernaut-XL-v9:Juggernaut-XL_v9_RunDiffusionPhoto_v2.safetensors",
          "type": "hf.stable_diffusion_xl",
          "name": "RunDiffusion/Juggernaut-XL-v9",
          "repo_id": "RunDiffusion/Juggernaut-XL-v9",
          "path": "Juggernaut-XL_v9_RunDiffusionPhoto_v2.safetensors",
          "size_on_disk": 7105348188,
          "pipeline_tag": "text-to-image",
          "tags": [
            "diffusers",
            "art",
            "people",
            "diffusion",
            "Cinematic",
            "Photography",
            "Landscape",
            "Interior",
            "Food",
            "Car",
            "Wildlife",
            "Architecture",
            "text-to-image",
            "en",
            "base_model:stabilityai/stable-diffusion-xl-base-1.0",
            "base_model:finetune:stabilityai/stable-diffusion-xl-base-1.0",
            "license:creativeml-openrail-m",
            "endpoints_compatible",
            "diffusers:StableDiffusionXLPipeline",
            "region:us"
          ],
          "has_model_index": true,
          "downloads": 201043,
          "likes": 277
        },
        {
          "id": "dataautogpt3/ProteusV0.3:ProteusV0.3.safetensors",
          "type": "hf.stable_diffusion_xl",
          "name": "dataautogpt3/ProteusV0.3",
          "repo_id": "dataautogpt3/ProteusV0.3",
          "path": "ProteusV0.3.safetensors",
          "size_on_disk": 6938040736,
          "pipeline_tag": "text-to-image",
          "tags": [
            "diffusers",
            "text-to-image",
            "license:gpl-3.0",
            "endpoints_compatible",
            "diffusers:StableDiffusionXLPipeline",
            "region:us"
          ],
          "has_model_index": true,
          "downloads": 159694,
          "likes": 94
        },
        {
          "id": "John6666/prefect-illustrious-xl-v3-sdxl",
          "type": "hf.stable_diffusion_xl",
          "name": "John6666/prefect-illustrious-xl-v3-sdxl",
          "repo_id": "John6666/prefect-illustrious-xl-v3-sdxl",
          "size_on_disk": 6941387072,
          "pipeline_tag": "text-to-image",
          "tags": [
            "diffusers",
            "safetensors",
            "text-to-image",
            "stable-diffusion",
            "stable-diffusion-xl",
            "anime",
            "girls",
            "styles",
            "lighting",
            "texture",
            "clean",
            "color balance",
            "prompt understanding",
            "merge",
            "noobai",
            "Illustrious XL v1.0",
            "illustrious",
            "en",
            "base_model:Laxhar/noobai-XL-1.1",
            "base_model:merge:Laxhar/noobai-XL-1.1",
            "base_model:OnomaAIResearch/Illustrious-XL-v1.0",
            "base_model:merge:OnomaAIResearch/Illustrious-XL-v1.0",
            "license:other",
            "endpoints_compatible",
            "diffusers:StableDiffusionXLPipeline",
            "region:us"
          ],
          "has_model_index": true,
          "downloads": 278268,
          "likes": 0
        },
        {
          "id": "cagliostrolab/animagine-xl-4.0:animagine-xl-4.0-opt.safetensors",
          "type": "hf.stable_diffusion_xl",
          "name": "cagliostrolab/animagine-xl-4.0",
          "repo_id": "cagliostrolab/animagine-xl-4.0",
          "path": "animagine-xl-4.0-opt.safetensors",
          "size_on_disk": 6938350040,
          "pipeline_tag": "text-to-image",
          "tags": [
            "diffusers",
            "safetensors",
            "text-to-image",
            "stable-diffusion",
            "stable-diffusion-xl",
            "en",
            "base_model:stabilityai/stable-diffusion-xl-base-1.0",
            "base_model:finetune:stabilityai/stable-diffusion-xl-base-1.0",
            "license:openrail++",
            "endpoints_compatible",
            "diffusers:StableDiffusionXLPipeline",
            "region:us"
          ],
          "has_model_index": true,
          "downloads": 249066,
          "likes": 363
        },
        {
          "id": "SG161222/RealVisXL_V5.0:RealVisXL_V5.0_fp16.safetensors",
          "type": "hf.stable_diffusion_xl",
          "name": "SG161222/RealVisXL_V5.0",
          "repo_id": "SG161222/RealVisXL_V5.0",
          "path": "RealVisXL_V5.0_fp16.safetensors",
          "size_on_disk": 6938065488,
          "pipeline_tag": "text-to-image",
          "tags": [
            "diffusers",
            "safetensors",
            "license:openrail++",
            "endpoints_compatible",
            "diffusers:StableDiffusionXLPipeline",
            "region:us"
          ],
          "has_model_index": true,
          "downloads": 55543,
          "likes": 121
        },
        {
          "id": "nunchaku-tech/nunchaku-sdxl:svdq-int4_r32-sdxl.safetensors",
          "type": "hf.stable_diffusion_xl",
          "name": "nunchaku-tech/nunchaku-sdxl",
          "repo_id": "nunchaku-tech/nunchaku-sdxl",
          "path": "svdq-int4_r32-sdxl.safetensors",
          "size_on_disk": 2559021560,
          "pipeline_tag": "text-to-image",
          "tags": [
            "diffusers",
            "text-to-image",
            "SVDQuant",
            "SDXL",
            "Diffusion",
            "Quantization",
            "stable-diffusion",
            "en",
            "dataset:mit-han-lab/svdquant-datasets",
            "arxiv:2411.05007",
            "base_model:stabilityai/stable-diffusion-xl-base-1.0",
            "base_model:quantized:stabilityai/stable-diffusion-xl-base-1.0",
            "license:openrail++",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 819,
          "likes": 26
        },
        {
          "id": "nunchaku-tech/nunchaku-sdxl-turbo:svdq-int4_r32-sdxl-turbo.safetensors",
          "type": "hf.stable_diffusion_xl",
          "name": "nunchaku-tech/nunchaku-sdxl-turbo",
          "repo_id": "nunchaku-tech/nunchaku-sdxl-turbo",
          "path": "svdq-int4_r32-sdxl-turbo.safetensors",
          "size_on_disk": 2559021776,
          "pipeline_tag": "text-to-image",
          "tags": [
            "diffusers",
            "text-to-image",
            "SVDQuant",
            "SDXL-Turbo",
            "Diffusion",
            "Quantization",
            "stable-diffusion",
            "en",
            "dataset:mit-han-lab/svdquant-datasets",
            "arxiv:2411.05007",
            "base_model:stabilityai/stable-diffusion-xl-base-1.0",
            "base_model:quantized:stabilityai/stable-diffusion-xl-base-1.0",
            "license:other",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 746,
          "likes": 10
        }
      ],
      "basic_fields": [
        "model",
        "quantization",
        "prompt",
        "width",
        "height"
      ]
    },
    {
      "title": "Text to Image",
      "description": "Generates images from text prompts using AutoPipeline for automatic model detection.\n    image, generation, AI, text-to-image, auto, flexible\n\n    Use cases:\n    - Generate images with automatic pipeline selection for any supported model\n    - Quickly prototype with various text-to-image architectures\n    - Build flexible workflows that adapt to different model types\n    - Create images without needing pipeline-specific configuration",
      "namespace": "huggingface.text_to_image",
      "node_type": "huggingface.text_to_image.Text2Image",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.text_to_image"
          },
          "default": {
            "type": "hf.text_to_image",
            "repo_id": "",
            "path": null,
            "variant": null,
            "allow_patterns": null,
            "ignore_patterns": null
          },
          "title": "Model",
          "description": "The text-to-image model. AutoPipeline automatically selects the correct pipeline type."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "A cat holding a sign that says hello world",
          "title": "Prompt",
          "description": "Text description of the image to generate. Be specific for better results."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Describe what to avoid in the image (e.g., 'blurry, low quality')."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 50,
          "title": "Num Inference Steps",
          "description": "Denoising steps. 20-50 is typical; more steps = better quality but slower.",
          "min": 1.0,
          "max": 100.0
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 7.5,
          "title": "Guidance Scale",
          "description": "How strongly to follow the prompt. 7-9 is typical for SD models.",
          "min": 1.0,
          "max": 20.0
        },
        {
          "name": "width",
          "type": {
            "type": "int"
          },
          "default": 512,
          "title": "Width",
          "description": "Output image width in pixels.",
          "min": 64.0,
          "max": 2048.0
        },
        {
          "name": "height",
          "type": {
            "type": "int"
          },
          "default": 512,
          "title": "Height",
          "description": "Output image height in pixels.",
          "min": 64.0,
          "max": 2048.0
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducible generation. Use -1 for random.",
          "min": -1.0
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "image"
        },
        {
          "type": {
            "type": "torch_tensor"
          },
          "name": "latent"
        }
      ],
      "basic_fields": [
        "model",
        "prompt",
        "height",
        "width",
        "seed"
      ]
    },
    {
      "title": "CogVideoX",
      "description": "Generates high-quality videos from text prompts using the CogVideoX diffusion transformer.\n    video, generation, AI, text-to-video, transformer, diffusion, cinematic\n\n    Use cases:\n    - Create videos from detailed text descriptions\n    - Generate cinematic content for creative projects\n    - Produce animated scenes for storytelling and marketing\n    - Build AI video generation applications\n    - Create visual content for social media",
      "namespace": "huggingface.text_to_video",
      "node_type": "huggingface.text_to_video.CogVideoX",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "A detailed wooden toy ship with intricately carved masts and sails is seen gliding smoothly over a plush, blue carpet that mimics the waves of the sea. The ship's hull is painted a rich brown, with tiny windows. The carpet, soft and textured, provides a perfect backdrop, resembling an oceanic expanse. Surrounding the ship are various other toys and children's items, hinting at a playful environment. The scene captures the innocence and imagination of childhood, with the toy ship's journey symbolizing endless adventures in a whimsical, indoor setting.",
          "title": "Prompt",
          "description": "Detailed text description of the video to generate. More descriptive prompts produce better results."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Describe what to avoid in the video (e.g., 'blurry, low quality, distorted')."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 49,
          "title": "Num Frames",
          "description": "Total frames in the output. Must be 8n+1 (49, 81, 113). More frames = longer video.",
          "min": 49.0,
          "max": 113.0
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 6.0,
          "title": "Guidance Scale",
          "description": "How strongly to follow the prompt. 5-8 is typical; higher = more adherence.",
          "min": 1.0,
          "max": 20.0
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 50,
          "title": "Num Inference Steps",
          "description": "Denoising steps. 50 is recommended; lower for faster generation.",
          "min": 1.0,
          "max": 100.0
        },
        {
          "name": "height",
          "type": {
            "type": "int"
          },
          "default": 480,
          "title": "Height",
          "description": "Output video height in pixels.",
          "min": 256.0,
          "max": 1024.0
        },
        {
          "name": "width",
          "type": {
            "type": "int"
          },
          "default": 720,
          "title": "Width",
          "description": "Output video width in pixels.",
          "min": 256.0,
          "max": 1024.0
        },
        {
          "name": "fps",
          "type": {
            "type": "int"
          },
          "default": 8,
          "title": "Fps",
          "description": "Frames per second for the output video file.",
          "min": 1.0,
          "max": 30.0
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducible generation. Use -1 for random.",
          "min": -1.0
        },
        {
          "name": "max_sequence_length",
          "type": {
            "type": "int"
          },
          "default": 226,
          "title": "Max Sequence Length",
          "description": "Maximum prompt encoding length. Higher allows longer prompts.",
          "min": 1.0,
          "max": 512.0
        },
        {
          "name": "enable_cpu_offload",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Cpu Offload",
          "description": "Offload model components to CPU to reduce VRAM usage."
        },
        {
          "name": "enable_vae_slicing",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Vae Slicing",
          "description": "Process VAE in slices to reduce peak memory usage."
        },
        {
          "name": "enable_vae_tiling",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Vae Tiling",
          "description": "Process VAE in tiles for large videos. Reduces memory but may affect quality."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "recommended_models": [
        {
          "id": "THUDM/CogVideoX-2b",
          "type": "hf.text_to_video",
          "name": "THUDM/CogVideoX-2b",
          "repo_id": "THUDM/CogVideoX-2b",
          "allow_patterns": [
            "**/*.safetensors",
            "**/*.json",
            "**/*.txt",
            "*.json"
          ],
          "size_on_disk": 13774736163,
          "pipeline_tag": "text-to-video",
          "tags": [
            "diffusers",
            "safetensors",
            "cogvideox",
            "video-generation",
            "thudm",
            "text-to-video",
            "en",
            "arxiv:2408.06072",
            "license:apache-2.0",
            "diffusers:CogVideoXPipeline",
            "region:us"
          ],
          "has_model_index": true,
          "downloads": 53188,
          "likes": 340
        },
        {
          "id": "THUDM/CogVideoX-5b",
          "type": "hf.text_to_video",
          "name": "THUDM/CogVideoX-5b",
          "repo_id": "THUDM/CogVideoX-5b",
          "allow_patterns": [
            "**/*.safetensors",
            "**/*.json",
            "**/*.txt",
            "*.json"
          ],
          "size_on_disk": 21527871871,
          "pipeline_tag": "text-to-video",
          "tags": [
            "diffusers",
            "safetensors",
            "cogvideox",
            "video-generation",
            "thudm",
            "text-to-video",
            "en",
            "arxiv:2408.06072",
            "license:other",
            "diffusers:CogVideoXPipeline",
            "region:us"
          ],
          "has_model_index": true,
          "downloads": 24656,
          "likes": 656
        }
      ],
      "basic_fields": [
        "prompt",
        "num_frames",
        "height",
        "width"
      ]
    },
    {
      "title": "Wan (Text-to-Video)",
      "description": "Generates videos from text prompts using Wan text-to-video diffusion models.\n    video, generation, AI, text-to-video, diffusion, Wan, cinematic\n\n    Use cases:\n    - Create high-quality videos from text descriptions\n    - Generate cinematic content for creative and commercial projects\n    - Produce animated scenes for storytelling and marketing\n    - Build AI video generation applications\n    - Create visual content for social media and entertainment\n\n    **Note:** Model variants offer different quality/resource tradeoffs. A14B is balanced; 14B offers maximum quality.",
      "namespace": "huggingface.text_to_video",
      "node_type": "huggingface.text_to_video.Wan_T2V",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "A robot standing on a mountain top at sunset, cinematic lighting, high detail",
          "title": "Prompt",
          "description": "Detailed text description of the video to generate."
        },
        {
          "name": "model_variant",
          "type": {
            "type": "enum",
            "values": [
              "Wan-AI/Wan2.2-T2V-A14B-Diffusers",
              "Wan-AI/Wan2.1-T2V-14B-Diffusers",
              "Wan-AI/Wan2.2-TI2V-5B-Diffusers"
            ],
            "type_name": "nodetool.nodes.huggingface.text_to_video.Wan_T2V.WanModel"
          },
          "default": "Wan-AI/Wan2.2-T2V-A14B-Diffusers",
          "title": "Model Variant",
          "description": "The Wan model variant. A14B is balanced; TI2V-5B is smaller/faster."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Describe what to avoid in the video (e.g., 'blurry, distorted')."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 49,
          "title": "Num Frames",
          "description": "Total frames in the output video. More frames = longer duration.",
          "min": 16.0,
          "max": 129.0
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 5.0,
          "title": "Guidance Scale",
          "description": "How strongly to follow the prompt. 4-7 is typical.",
          "min": 1.0,
          "max": 20.0
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Num Inference Steps",
          "description": "Denoising steps. 30 is fast; 50+ for higher quality.",
          "min": 1.0,
          "max": 100.0
        },
        {
          "name": "height",
          "type": {
            "type": "int"
          },
          "default": 480,
          "title": "Height",
          "description": "Output video height in pixels.",
          "min": 256.0,
          "max": 1080.0
        },
        {
          "name": "width",
          "type": {
            "type": "int"
          },
          "default": 720,
          "title": "Width",
          "description": "Output video width in pixels.",
          "min": 256.0,
          "max": 1920.0
        },
        {
          "name": "fps",
          "type": {
            "type": "int"
          },
          "default": 16,
          "title": "Fps",
          "description": "Frames per second for the output video file.",
          "min": 1.0,
          "max": 60.0
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducible generation. Use -1 for random.",
          "min": -1.0
        },
        {
          "name": "max_sequence_length",
          "type": {
            "type": "int"
          },
          "default": 512,
          "title": "Max Sequence Length",
          "description": "Maximum prompt encoding length. Higher allows longer prompts.",
          "min": 64.0,
          "max": 1024.0
        },
        {
          "name": "enable_cpu_offload",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Cpu Offload",
          "description": "Offload model components to CPU to reduce VRAM usage."
        },
        {
          "name": "enable_vae_slicing",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Vae Slicing",
          "description": "Process VAE in slices to reduce peak memory usage."
        },
        {
          "name": "enable_vae_tiling",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Vae Tiling",
          "description": "Process VAE in tiles for large videos. May affect quality."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "recommended_models": [
        {
          "id": "Wan-AI/Wan2.2-T2V-A14B-Diffusers",
          "type": "hf.text_to_video",
          "name": "Wan-AI/Wan2.2-T2V-A14B-Diffusers",
          "repo_id": "Wan-AI/Wan2.2-T2V-A14B-Diffusers",
          "allow_patterns": [
            "**/*.safetensors",
            "**/*.json",
            "**/*.txt",
            "*.json"
          ],
          "size_on_disk": 126194725893,
          "pipeline_tag": "text-to-video",
          "tags": [
            "diffusers",
            "safetensors",
            "text-to-video",
            "arxiv:2503.20314",
            "arxiv:2309.14509",
            "license:apache-2.0",
            "diffusers:WanPipeline",
            "region:us"
          ],
          "has_model_index": true,
          "downloads": 181844,
          "likes": 92
        },
        {
          "id": "Wan-AI/Wan2.1-T2V-14B-Diffusers",
          "type": "hf.text_to_video",
          "name": "Wan-AI/Wan2.1-T2V-14B-Diffusers",
          "repo_id": "Wan-AI/Wan2.1-T2V-14B-Diffusers",
          "allow_patterns": [
            "**/*.safetensors",
            "**/*.json",
            "**/*.txt",
            "*.json"
          ],
          "size_on_disk": 80402370670,
          "pipeline_tag": "text-to-video",
          "tags": [
            "diffusers",
            "safetensors",
            "video generation",
            "text-to-video",
            "en",
            "zh",
            "license:apache-2.0",
            "diffusers:WanPipeline",
            "region:us"
          ],
          "has_model_index": true,
          "downloads": 17976,
          "likes": 46
        },
        {
          "id": "Wan-AI/Wan2.2-TI2V-5B-Diffusers",
          "type": "hf.text_to_video",
          "name": "Wan-AI/Wan2.2-TI2V-5B-Diffusers",
          "repo_id": "Wan-AI/Wan2.2-TI2V-5B-Diffusers",
          "allow_patterns": [
            "**/*.safetensors",
            "**/*.json",
            "**/*.txt",
            "*.json"
          ],
          "size_on_disk": 34196870087,
          "pipeline_tag": "text-to-video",
          "tags": [
            "diffusers",
            "safetensors",
            "text-to-video",
            "en",
            "zh",
            "arxiv:2503.20314",
            "license:apache-2.0",
            "diffusers:WanPipeline",
            "region:us"
          ],
          "has_model_index": true,
          "downloads": 53482,
          "likes": 90
        }
      ],
      "basic_fields": [
        "prompt",
        "num_frames",
        "height",
        "width",
        "model_variant"
      ]
    },
    {
      "title": "Image Classifier",
      "description": "Classifies images into predefined categories using vision transformer models.\n    image, classification, labeling, categorization, computer-vision\n\n    Use cases:\n    - Automatically tag and organize photo libraries\n    - Detect inappropriate or NSFW content for moderation\n    - Classify product images in e-commerce catalogs\n    - Identify age, gender, or other attributes in photos\n    - Sort images by scene type, object presence, or style",
      "namespace": "huggingface.image_classification",
      "node_type": "huggingface.image_classification.ImageClassifier",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.image_classification"
          },
          "default": {
            "type": "hf.image_classification",
            "repo_id": "",
            "path": null,
            "variant": null,
            "allow_patterns": null,
            "ignore_patterns": null
          },
          "title": "Model",
          "description": "The image classification model. ViT and ResNet models offer general classification; specialized models exist for NSFW detection, age estimation, etc."
        },
        {
          "name": "image",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image",
          "description": "The image to classify. Supports common formats like JPEG, PNG, WebP."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "float"
              }
            ]
          },
          "name": "output"
        }
      ],
      "recommended_models": [
        {
          "id": "google/vit-base-patch16-224",
          "type": "hf.image_classification",
          "name": "google/vit-base-patch16-224",
          "repo_id": "google/vit-base-patch16-224",
          "allow_patterns": [
            "README.md",
            "*.safetensors",
            "*.json",
            "**/*.json"
          ],
          "size_on_disk": 346369365,
          "pipeline_tag": "image-classification",
          "tags": [
            "transformers",
            "pytorch",
            "tf",
            "jax",
            "safetensors",
            "vit",
            "image-classification",
            "vision",
            "dataset:imagenet-1k",
            "dataset:imagenet-21k",
            "arxiv:2010.11929",
            "arxiv:2006.03677",
            "license:apache-2.0",
            "endpoints_compatible",
            "deploy:azure",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 4082533,
          "likes": 908
        },
        {
          "id": "microsoft/resnet-50",
          "type": "hf.image_classification",
          "name": "microsoft/resnet-50",
          "repo_id": "microsoft/resnet-50",
          "allow_patterns": [
            "README.md",
            "*.safetensors",
            "*.json",
            "**/*.json"
          ],
          "size_on_disk": 102555318,
          "pipeline_tag": "image-classification",
          "tags": [
            "transformers",
            "pytorch",
            "tf",
            "jax",
            "safetensors",
            "resnet",
            "image-classification",
            "vision",
            "dataset:imagenet-1k",
            "arxiv:1512.03385",
            "license:apache-2.0",
            "endpoints_compatible",
            "deploy:azure",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 267478,
          "likes": 468
        },
        {
          "id": "microsoft/resnet-18",
          "type": "hf.image_classification",
          "name": "microsoft/resnet-18",
          "repo_id": "microsoft/resnet-18",
          "allow_patterns": [
            "README.md",
            "*.safetensors",
            "*.json",
            "**/*.json"
          ],
          "size_on_disk": 46884396,
          "pipeline_tag": "image-classification",
          "tags": [
            "transformers",
            "pytorch",
            "tf",
            "safetensors",
            "resnet",
            "image-classification",
            "vision",
            "dataset:imagenet-1k",
            "arxiv:1512.03385",
            "license:apache-2.0",
            "endpoints_compatible",
            "deploy:azure",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 136949,
          "likes": 62
        },
        {
          "id": "apple/mobilevit-small",
          "type": "hf.image_classification",
          "name": "apple/mobilevit-small",
          "repo_id": "apple/mobilevit-small",
          "allow_patterns": [
            "README.md",
            "*.bin",
            "*.json",
            "**/*.json"
          ],
          "size_on_disk": 44860946,
          "pipeline_tag": "image-classification",
          "tags": [
            "transformers",
            "pytorch",
            "tf",
            "coreml",
            "mobilevit",
            "image-classification",
            "vision",
            "dataset:imagenet-1k",
            "arxiv:2110.02178",
            "license:other",
            "endpoints_compatible",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 1533962,
          "likes": 82
        },
        {
          "id": "apple/mobilevit-xx-small",
          "type": "hf.image_classification",
          "name": "apple/mobilevit-xx-small",
          "repo_id": "apple/mobilevit-xx-small",
          "allow_patterns": [
            "README.md",
            "*.bin",
            "*.json",
            "**/*.json"
          ],
          "size_on_disk": 10391397,
          "pipeline_tag": "image-classification",
          "tags": [
            "transformers",
            "pytorch",
            "tf",
            "coreml",
            "mobilevit",
            "image-classification",
            "vision",
            "dataset:imagenet-1k",
            "arxiv:2110.02178",
            "license:other",
            "endpoints_compatible",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 5688,
          "likes": 20
        },
        {
          "id": "nateraw/vit-age-classifier",
          "type": "hf.image_classification",
          "name": "nateraw/vit-age-classifier",
          "repo_id": "nateraw/vit-age-classifier",
          "allow_patterns": [
            "README.md",
            "*.safetensors",
            "*.json",
            "**/*.json"
          ],
          "size_on_disk": 343247506,
          "pipeline_tag": "image-classification",
          "tags": [
            "transformers",
            "pytorch",
            "safetensors",
            "vit",
            "image-classification",
            "dataset:nateraw/fairface",
            "doi:10.57967/hf/1259",
            "endpoints_compatible",
            "deploy:azure",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 325581,
          "likes": 142
        },
        {
          "id": "Falconsai/nsfw_image_detection",
          "type": "hf.image_classification",
          "name": "Falconsai/nsfw_image_detection",
          "repo_id": "Falconsai/nsfw_image_detection",
          "allow_patterns": [
            "README.md",
            "*.safetensors",
            "*.json",
            "**/*.json"
          ],
          "size_on_disk": 343234197,
          "pipeline_tag": "image-classification",
          "tags": [
            "transformers",
            "pytorch",
            "safetensors",
            "vit",
            "image-classification",
            "arxiv:2010.11929",
            "license:apache-2.0",
            "endpoints_compatible",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 88981519,
          "likes": 917
        },
        {
          "id": "rizvandwiki/gender-classification-2",
          "type": "hf.image_classification",
          "name": "rizvandwiki/gender-classification-2",
          "repo_id": "rizvandwiki/gender-classification-2",
          "allow_patterns": [
            "README.md",
            "*.safetensors",
            "*.json",
            "**/*.json"
          ],
          "size_on_disk": 343225777,
          "pipeline_tag": "image-classification",
          "tags": [
            "transformers",
            "pytorch",
            "tensorboard",
            "safetensors",
            "vit",
            "image-classification",
            "huggingpics",
            "model-index",
            "endpoints_compatible",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 395965,
          "likes": 36
        }
      ],
      "basic_fields": [
        "model",
        "image"
      ]
    },
    {
      "title": "Zero-Shot Image Classifier",
      "description": "Classifies images into custom categories without requiring task-specific training data.\n    image, classification, labeling, categorization, zero-shot, flexible\n\n    Use cases:\n    - Categorize images with custom, user-defined labels on the fly\n    - Quickly prototype image classification systems without training\n    - Identify objects or scenes without predefined model categories\n    - Build flexible image tagging workflows with dynamic categories\n    - Test hypotheses about image content using natural language labels",
      "namespace": "huggingface.image_classification",
      "node_type": "huggingface.image_classification.ZeroShotImageClassifier",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.zero_shot_image_classification"
          },
          "default": {
            "type": "hf.zero_shot_image_classification",
            "repo_id": "",
            "path": null,
            "variant": null,
            "allow_patterns": null,
            "ignore_patterns": null
          },
          "title": "Model",
          "description": "The zero-shot classification model. CLIP-based models (OpenAI, LAION) enable flexible label matching using vision-language understanding."
        },
        {
          "name": "image",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image",
          "description": "The image to classify. Supports common formats like JPEG, PNG, WebP."
        },
        {
          "name": "candidate_labels",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Candidate Labels",
          "description": "Comma-separated list of labels to classify against (e.g., 'cat,dog,bird,fish'). Use descriptive labels for better results."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "float"
              }
            ]
          },
          "name": "output"
        }
      ],
      "recommended_models": [
        {
          "id": "openai/clip-vit-base-patch32",
          "type": "hf.zero_shot_image_classification",
          "name": "openai/clip-vit-base-patch32",
          "repo_id": "openai/clip-vit-base-patch32",
          "allow_patterns": [
            "README.md",
            "pytorch_model.bin",
            "*.json",
            "*.txt"
          ],
          "size_on_disk": 608871522,
          "pipeline_tag": "zero-shot-image-classification",
          "tags": [
            "transformers",
            "pytorch",
            "tf",
            "jax",
            "clip",
            "zero-shot-image-classification",
            "vision",
            "arxiv:2103.00020",
            "arxiv:1908.04913",
            "endpoints_compatible",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 17750912,
          "likes": 826
        },
        {
          "id": "laion/CLIP-ViT-H-14-laion2B-s32B-b79K",
          "type": "hf.zero_shot_image_classification",
          "name": "laion/CLIP-ViT-H-14-laion2B-s32B-b79K",
          "repo_id": "laion/CLIP-ViT-H-14-laion2B-s32B-b79K",
          "allow_patterns": [
            "README.md",
            "pytorch_model.bin",
            "*.json",
            "*.txt"
          ],
          "size_on_disk": 3948362264,
          "pipeline_tag": "zero-shot-image-classification",
          "tags": [
            "open_clip",
            "pytorch",
            "safetensors",
            "clip",
            "zero-shot-image-classification",
            "arxiv:1910.04867",
            "license:mit",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 669638,
          "likes": 426
        },
        {
          "id": "laion/CLIP-ViT-g-14-laion2B-s12B-b42K",
          "type": "hf.zero_shot_image_classification",
          "name": "laion/CLIP-ViT-g-14-laion2B-s12B-b42K",
          "repo_id": "laion/CLIP-ViT-g-14-laion2B-s12B-b42K",
          "allow_patterns": [
            "README.md",
            "pytorch_model.bin",
            "*.json",
            "*.txt"
          ],
          "size_on_disk": 5470681040,
          "tags": [
            "open_clip",
            "pytorch",
            "safetensors",
            "clip",
            "arxiv:1910.04867",
            "license:mit",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 75844,
          "likes": 44
        }
      ],
      "basic_fields": [
        "model",
        "image",
        "candidate_labels"
      ]
    },
    {
      "title": "Fill Mask",
      "description": "Predicts the most likely words to fill masked positions in text using language models.\n    text, fill-mask, NLP, language-modeling, word-prediction\n\n    Use cases:\n    - Complete sentences with contextually appropriate words\n    - Generate word suggestions for text editing tools\n    - Test language understanding and word associations\n    - Build autocomplete and text prediction features\n    - Explore semantic relationships between words in context",
      "namespace": "huggingface.fill_mask",
      "node_type": "huggingface.fill_mask.FillMask",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.fill_mask"
          },
          "default": {
            "type": "hf.fill_mask",
            "repo_id": "",
            "path": null,
            "variant": null,
            "allow_patterns": null,
            "ignore_patterns": null
          },
          "title": "Model",
          "description": "The masked language model to use. BERT, RoBERTa, and DistilBERT variants are supported."
        },
        {
          "name": "inputs",
          "type": {
            "type": "str"
          },
          "default": "The capital of France is [MASK].",
          "title": "Input Text",
          "description": "Text containing [MASK] token(s) to be predicted. Different models may use different mask tokens (e.g., BERT uses [MASK], RoBERTa uses <mask>)."
        },
        {
          "name": "top_k",
          "type": {
            "type": "int"
          },
          "default": 5,
          "title": "Top K",
          "description": "Number of most likely predictions to return for each masked position."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "recommended_models": [
        {
          "id": "bert-base-uncased",
          "type": "hf.fill_mask",
          "name": "bert-base-uncased",
          "repo_id": "bert-base-uncased",
          "allow_patterns": [
            "*.json",
            "*.txt",
            "*.safetensors"
          ],
          "size_on_disk": 441148573,
          "pipeline_tag": "fill-mask",
          "tags": [
            "transformers",
            "pytorch",
            "tf",
            "jax",
            "rust",
            "coreml",
            "onnx",
            "safetensors",
            "bert",
            "fill-mask",
            "exbert",
            "en",
            "dataset:bookcorpus",
            "dataset:wikipedia",
            "arxiv:1810.04805",
            "license:apache-2.0",
            "endpoints_compatible",
            "deploy:azure",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 60303708,
          "likes": 2510
        },
        {
          "id": "roberta-base",
          "type": "hf.fill_mask",
          "name": "roberta-base",
          "repo_id": "roberta-base",
          "allow_patterns": [
            "*.json",
            "*.txt",
            "*.safetensors"
          ],
          "size_on_disk": 502132854,
          "pipeline_tag": "fill-mask",
          "tags": [
            "transformers",
            "pytorch",
            "tf",
            "jax",
            "rust",
            "safetensors",
            "roberta",
            "fill-mask",
            "exbert",
            "en",
            "dataset:bookcorpus",
            "dataset:wikipedia",
            "arxiv:1907.11692",
            "arxiv:1806.02847",
            "license:mit",
            "endpoints_compatible",
            "deploy:azure",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 8790703,
          "likes": 535
        },
        {
          "id": "distilbert-base-uncased",
          "type": "hf.fill_mask",
          "name": "distilbert-base-uncased",
          "repo_id": "distilbert-base-uncased",
          "allow_patterns": [
            "*.json",
            "*.txt",
            "*.safetensors"
          ],
          "size_on_disk": 268652869,
          "pipeline_tag": "fill-mask",
          "tags": [
            "transformers",
            "pytorch",
            "tf",
            "jax",
            "rust",
            "safetensors",
            "distilbert",
            "fill-mask",
            "exbert",
            "en",
            "dataset:bookcorpus",
            "dataset:wikipedia",
            "arxiv:1910.01108",
            "license:apache-2.0",
            "endpoints_compatible",
            "deploy:azure",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 9723848,
          "likes": 804
        },
        {
          "id": "albert-base-v2",
          "type": "hf.fill_mask",
          "name": "albert-base-v2",
          "repo_id": "albert-base-v2",
          "allow_patterns": [
            "*.json",
            "*.txt",
            "*.safetensors"
          ],
          "size_on_disk": 48686272,
          "pipeline_tag": "fill-mask",
          "tags": [
            "transformers",
            "pytorch",
            "tf",
            "jax",
            "rust",
            "safetensors",
            "albert",
            "fill-mask",
            "en",
            "dataset:bookcorpus",
            "dataset:wikipedia",
            "arxiv:1909.11942",
            "license:apache-2.0",
            "endpoints_compatible",
            "deploy:azure",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 486172,
          "likes": 135
        }
      ],
      "basic_fields": [
        "model",
        "inputs",
        "top_k"
      ]
    },
    {
      "title": "Image To Text",
      "description": "Generates textual descriptions and captions from images using vision-language models.\n    image, captioning, OCR, image-to-text, accessibility\n\n    Use cases:\n    - Automatically generate captions for photos and artwork\n    - Extract visible text from images (OCR-style functionality)\n    - Create alt-text descriptions for web accessibility\n    - Build image search engines with text-based queries\n    - Generate descriptions for visually impaired users",
      "namespace": "huggingface.image_to_text",
      "node_type": "huggingface.image_to_text.ImageToText",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.image_to_text"
          },
          "default": {
            "type": "hf.image_to_text",
            "repo_id": "Salesforce/blip-image-captioning-base",
            "path": null,
            "variant": null,
            "allow_patterns": [
              "*.safetensors",
              "*.json",
              "*.txt",
              "*.model"
            ],
            "ignore_patterns": null
          },
          "title": "Model",
          "description": "The image captioning model. BLIP models offer good quality; BLIP2 provides enhanced understanding; GIT is faster."
        },
        {
          "name": "image",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Input Image",
          "description": "The image to generate text from. Supports JPEG, PNG, WebP and other common formats."
        },
        {
          "name": "max_new_tokens",
          "type": {
            "type": "int"
          },
          "default": 1024,
          "title": "Max New Tokens",
          "description": "Maximum length of the generated caption in tokens. Higher values allow longer descriptions."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "str"
          },
          "name": "output"
        }
      ],
      "recommended_models": [
        {
          "id": "Salesforce/blip-image-captioning-base",
          "type": "hf.image_to_text",
          "name": "Salesforce/blip-image-captioning-base",
          "repo_id": "Salesforce/blip-image-captioning-base",
          "allow_patterns": [
            "*.safetensors",
            "*.json",
            "*.txt",
            "*.model"
          ],
          "size_on_disk": 948385,
          "pipeline_tag": "image-to-text",
          "tags": [
            "transformers",
            "pytorch",
            "tf",
            "blip",
            "image-to-text",
            "image-captioning",
            "arxiv:2201.12086",
            "license:bsd-3-clause",
            "endpoints_compatible",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 1908664,
          "likes": 823
        },
        {
          "id": "Salesforce/blip2-opt-2.7b",
          "type": "hf.image_to_text",
          "name": "Salesforce/blip2-opt-2.7b",
          "repo_id": "Salesforce/blip2-opt-2.7b",
          "allow_patterns": [
            "*.safetensors",
            "*.json",
            "*.txt",
            "*.model"
          ],
          "size_on_disk": 14984267092,
          "pipeline_tag": "image-text-to-text",
          "tags": [
            "transformers",
            "pytorch",
            "safetensors",
            "blip-2",
            "image-to-text",
            "vision",
            "image-captioning",
            "visual-question-answering",
            "image-text-to-text",
            "en",
            "arxiv:2301.12597",
            "license:mit",
            "endpoints_compatible",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 631122,
          "likes": 424
        },
        {
          "id": "microsoft/git-base",
          "type": "hf.image_to_text",
          "name": "microsoft/git-base",
          "repo_id": "microsoft/git-base",
          "allow_patterns": [
            "*.safetensors",
            "*.json",
            "*.txt",
            "*.model"
          ],
          "size_on_disk": 707472970,
          "pipeline_tag": "image-to-text",
          "tags": [
            "transformers",
            "pytorch",
            "safetensors",
            "git",
            "image-to-text",
            "vision",
            "image-captioning",
            "en",
            "arxiv:2205.14100",
            "license:mit",
            "endpoints_compatible",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 15406,
          "likes": 106
        },
        {
          "id": "nlpconnect/vit-gpt2-image-captioning",
          "type": "hf.image_to_text",
          "name": "nlpconnect/vit-gpt2-image-captioning",
          "repo_id": "nlpconnect/vit-gpt2-image-captioning",
          "allow_patterns": [
            "*.safetensors",
            "*.json",
            "*.txt",
            "*.model"
          ],
          "size_on_disk": 2615156,
          "pipeline_tag": "image-to-text",
          "tags": [
            "transformers",
            "pytorch",
            "vision-encoder-decoder",
            "image-to-text",
            "image-captioning",
            "doi:10.57967/hf/0222",
            "license:apache-2.0",
            "endpoints_compatible",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 1349965,
          "likes": 921
        }
      ],
      "basic_fields": [
        "model",
        "image"
      ]
    },
    {
      "title": "Load Image To Text Model",
      "description": "Loads and validates a Hugging Face image-to-text model for use in downstream nodes.\n    model-loader, captioning, OCR, image-to-text\n\n    Use cases:\n    - Pre-load image captioning models before running pipelines\n    - Validate model availability and compatibility\n    - Configure model settings for ImageToText processing",
      "namespace": "huggingface.image_to_text",
      "node_type": "huggingface.image_to_text.LoadImageToTextModel",
      "properties": [
        {
          "name": "repo_id",
          "type": {
            "type": "str"
          },
          "default": "Salesforce/blip-image-captioning-base",
          "title": "Model ID",
          "description": "The Hugging Face repository ID for the image-to-text model (e.g., BLIP, GIT, ViT-GPT2)."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "hf.image_to_text"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "repo_id"
      ]
    },
    {
      "title": "Image To Text",
      "description": "Generates descriptive text captions from images using vision-language models.\n    image, text, captioning, vision-language, accessibility\n\n    Use cases:\n    - Generate natural language descriptions of image content\n    - Create alt-text for web accessibility compliance\n    - Build automatic image cataloging and search systems\n    - Enable content discovery through text-based image queries",
      "namespace": "huggingface.multimodal",
      "node_type": "huggingface.multimodal.ImageToText",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.image_to_text"
          },
          "default": {
            "type": "hf.image_to_text",
            "repo_id": "",
            "path": null,
            "variant": null,
            "allow_patterns": null,
            "ignore_patterns": null
          },
          "title": "Model",
          "description": "The image captioning model. BLIP variants offer good quality/speed balance."
        },
        {
          "name": "image",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Input Image",
          "description": "The image to generate a caption for."
        },
        {
          "name": "max_new_tokens",
          "type": {
            "type": "int"
          },
          "default": 1024,
          "title": "Max New Tokens",
          "description": "Maximum length of the generated caption in tokens."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "str"
          },
          "name": "output"
        }
      ],
      "recommended_models": [
        {
          "id": "Salesforce/blip-image-captioning-base",
          "type": "hf.image_to_text",
          "name": "Salesforce/blip-image-captioning-base",
          "repo_id": "Salesforce/blip-image-captioning-base",
          "allow_patterns": [
            "*.bin",
            "*.json",
            "*.txt    "
          ],
          "size_on_disk": 990537726,
          "pipeline_tag": "image-to-text",
          "tags": [
            "transformers",
            "pytorch",
            "tf",
            "blip",
            "image-to-text",
            "image-captioning",
            "arxiv:2201.12086",
            "license:bsd-3-clause",
            "endpoints_compatible",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 1908664,
          "likes": 823
        },
        {
          "id": "Salesforce/blip-image-captioning-large",
          "type": "hf.image_to_text",
          "name": "Salesforce/blip-image-captioning-large",
          "repo_id": "Salesforce/blip-image-captioning-large",
          "allow_patterns": [
            "*.bin",
            "*.json",
            "*.txt"
          ],
          "size_on_disk": 1880092519,
          "pipeline_tag": "image-to-text",
          "tags": [
            "transformers",
            "pytorch",
            "tf",
            "safetensors",
            "blip",
            "image-to-text",
            "image-captioning",
            "arxiv:2201.12086",
            "license:bsd-3-clause",
            "endpoints_compatible",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 1167096,
          "likes": 1439
        },
        {
          "id": "nlpconnect/vit-gpt2-image-captioning",
          "type": "hf.image_to_text",
          "name": "nlpconnect/vit-gpt2-image-captioning",
          "repo_id": "nlpconnect/vit-gpt2-image-captioning",
          "allow_patterns": [
            "*.bin",
            "*.json",
            "*.txt"
          ],
          "size_on_disk": 984757149,
          "pipeline_tag": "image-to-text",
          "tags": [
            "transformers",
            "pytorch",
            "vision-encoder-decoder",
            "image-to-text",
            "image-captioning",
            "doi:10.57967/hf/0222",
            "license:apache-2.0",
            "endpoints_compatible",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 1349965,
          "likes": 921
        },
        {
          "id": "microsoft/git-base-coco",
          "type": "hf.image_to_text",
          "name": "microsoft/git-base-coco",
          "repo_id": "microsoft/git-base-coco",
          "allow_patterns": [
            "*.bin",
            "*.json",
            "*.txt"
          ],
          "size_on_disk": 707534943,
          "pipeline_tag": "image-to-text",
          "tags": [
            "transformers",
            "pytorch",
            "git",
            "image-to-text",
            "vision",
            "image-captioning",
            "en",
            "arxiv:2205.14100",
            "license:mit",
            "endpoints_compatible",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 97206,
          "likes": 19
        }
      ],
      "basic_fields": [
        "model",
        "image",
        "max_new_tokens"
      ]
    },
    {
      "title": "Visual Question Answering",
      "description": "Answers natural language questions about image content using vision-language models.\n    image, text, question-answering, multimodal, VQA\n\n    Use cases:\n    - Query image content with natural language questions\n    - Extract specific information from photos and diagrams\n    - Build interactive image exploration interfaces\n    - Create accessibility tools for visual content understanding",
      "namespace": "huggingface.multimodal",
      "node_type": "huggingface.multimodal.VisualQuestionAnswering",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.visual_question_answering"
          },
          "default": {
            "type": "hf.visual_question_answering",
            "repo_id": "",
            "path": null,
            "variant": null,
            "allow_patterns": null,
            "ignore_patterns": null
          },
          "title": "Model",
          "description": "The visual question answering model. BLIP-VQA provides good general performance."
        },
        {
          "name": "image",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image",
          "description": "The image to ask questions about."
        },
        {
          "name": "question",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Question",
          "description": "Your question about the image content (e.g., 'What color is the car?')."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "str"
          },
          "name": "output"
        }
      ],
      "recommended_models": [
        {
          "id": "Salesforce/blip-vqa-base",
          "type": "hf.visual_question_answering",
          "name": "Salesforce/blip-vqa-base",
          "repo_id": "Salesforce/blip-vqa-base",
          "allow_patterns": [
            "*.bin",
            "*.json",
            "*.txt"
          ],
          "size_on_disk": 1539915254,
          "pipeline_tag": "visual-question-answering",
          "tags": [
            "transformers",
            "pytorch",
            "tf",
            "safetensors",
            "blip",
            "visual-question-answering",
            "arxiv:2201.12086",
            "license:bsd-3-clause",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 257402,
          "likes": 184
        }
      ],
      "basic_fields": [
        "model",
        "image",
        "question"
      ]
    },
    {
      "title": "Audio Classifier",
      "description": "Classifies audio into predefined categories using pretrained transformer models.\n    audio, classification, labeling, categorization, sound-recognition\n\n    Use cases:\n    - Classify music by genre or mood\n    - Detect speech vs. non-speech audio segments\n    - Identify environmental sounds (e.g., car horn, dog bark)\n    - Recognize emotions in speech recordings\n    - Content moderation for audio platforms",
      "namespace": "huggingface.audio_classification",
      "node_type": "huggingface.audio_classification.AudioClassifier",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.audio_classification"
          },
          "default": {
            "type": "hf.audio_classification",
            "repo_id": "",
            "path": null,
            "variant": null,
            "allow_patterns": null,
            "ignore_patterns": null
          },
          "title": "Model",
          "description": "The Hugging Face model for audio classification. Recommended: MIT/ast-finetuned-audioset for general sounds, wav2vec2-lg-xlsr-en-speech-emotion-recognition for speech emotions."
        },
        {
          "name": "audio",
          "type": {
            "type": "audio"
          },
          "default": {
            "type": "audio",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Audio",
          "description": "The audio file to classify. Supports common formats like WAV, MP3, FLAC."
        },
        {
          "name": "top_k",
          "type": {
            "type": "int"
          },
          "default": 10,
          "title": "Top K",
          "description": "Number of top classification results to return, ranked by confidence score."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "float"
              }
            ]
          },
          "name": "output"
        }
      ],
      "recommended_models": [
        {
          "id": "MIT/ast-finetuned-audioset-10-10-0.4593",
          "type": "hf.audio_classification",
          "name": "MIT/ast-finetuned-audioset-10-10-0.4593",
          "repo_id": "MIT/ast-finetuned-audioset-10-10-0.4593",
          "allow_patterns": [
            "*.safetensors",
            "*.json"
          ],
          "size_on_disk": 346432008,
          "pipeline_tag": "audio-classification",
          "tags": [
            "transformers",
            "pytorch",
            "safetensors",
            "audio-spectrogram-transformer",
            "audio-classification",
            "arxiv:2104.01778",
            "license:bsd-3-clause",
            "endpoints_compatible",
            "deploy:azure",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 665957,
          "likes": 332
        },
        {
          "id": "ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition",
          "type": "hf.audio_classification",
          "name": "ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition",
          "repo_id": "ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition",
          "allow_patterns": [
            "pytorch_model.bin",
            "*.json"
          ],
          "size_on_disk": 1266158125,
          "pipeline_tag": "audio-classification",
          "tags": [
            "transformers",
            "pytorch",
            "tensorboard",
            "safetensors",
            "wav2vec2",
            "audio-classification",
            "generated_from_trainer",
            "doi:10.57967/hf/2045",
            "license:apache-2.0",
            "endpoints_compatible",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 39371,
          "likes": 236
        }
      ],
      "basic_fields": [
        "model",
        "audio",
        "top_k"
      ]
    },
    {
      "title": "Zero Shot Audio Classifier",
      "description": "Classifies audio into custom categories without requiring task-specific training data.\n    audio, classification, labeling, categorization, zero-shot, flexible\n\n    Use cases:\n    - Categorize audio with custom, user-defined labels on the fly\n    - Identify sounds or music genres without predefined model training\n    - Quickly prototype audio classification systems\n    - Automate tagging for large audio datasets with dynamic categories",
      "namespace": "huggingface.audio_classification",
      "node_type": "huggingface.audio_classification.ZeroShotAudioClassifier",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.zero_shot_audio_classification"
          },
          "default": {
            "type": "hf.zero_shot_audio_classification",
            "repo_id": "",
            "path": null,
            "variant": null,
            "allow_patterns": null,
            "ignore_patterns": null
          },
          "title": "Model",
          "description": "The Hugging Face model for zero-shot audio classification. Uses CLIP-based models for flexible label matching."
        },
        {
          "name": "audio",
          "type": {
            "type": "audio"
          },
          "default": {
            "type": "audio",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Audio",
          "description": "The audio file to classify. Supports common formats like WAV, MP3, FLAC."
        },
        {
          "name": "candidate_labels",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Candidate Labels",
          "description": "Comma-separated list of labels to classify against (e.g., 'music,speech,noise,silence')."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "float"
              }
            ]
          },
          "name": "output"
        }
      ],
      "recommended_models": [
        {
          "id": "laion/clap-htsat-unfused",
          "type": "hf.zero_shot_audio_classification",
          "name": "laion/clap-htsat-unfused",
          "repo_id": "laion/clap-htsat-unfused",
          "allow_patterns": [
            "model.safetensors",
            "*.json",
            "*.txt"
          ],
          "size_on_disk": 3369990,
          "pipeline_tag": "feature-extraction",
          "tags": [
            "transformers",
            "pytorch",
            "clap",
            "feature-extraction",
            "arxiv:2211.06687",
            "license:apache-2.0",
            "endpoints_compatible",
            "deploy:azure",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 471056,
          "likes": 62
        }
      ],
      "basic_fields": [
        "model",
        "audio",
        "candidate_labels"
      ]
    },
    {
      "title": "Wan (Image-to-Video)",
      "description": "Transforms a static image into a dynamic video clip using Wan image-to-video diffusion models.\n    video, generation, AI, image-to-video, diffusion, Wan, animation\n\n    Use cases:\n    - Animate photographs and artwork into short video clips\n    - Create motion from still images with text-guided direction\n    - Generate video content for social media from static images\n    - Bring product images to life with realistic movement\n    - Create dynamic visual effects from single frames\n\n    **Note:** Model variants offer different quality/speed tradeoffs. A14B is balanced; 720P provides higher resolution.",
      "namespace": "huggingface.image_to_video",
      "node_type": "huggingface.image_to_video.Wan_I2V",
      "properties": [
        {
          "name": "input_image",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Input Image",
          "description": "The source image to animate. Image content guides the video's appearance."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "An astronaut walking on the moon, cinematic lighting, high detail",
          "title": "Prompt",
          "description": "Text description guiding how the image should animate and move."
        },
        {
          "name": "model_variant",
          "type": {
            "type": "enum",
            "values": [
              "Wan-AI/Wan2.2-I2V-A14B-Diffusers",
              "Wan-AI/Wan2.1-I2V-14B-480P-Diffusers",
              "Wan-AI/Wan2.1-I2V-14B-720P-Diffusers"
            ],
            "type_name": "nodetool.nodes.huggingface.image_to_video.Wan_I2V.WanI2VModel"
          },
          "default": "Wan-AI/Wan2.2-I2V-A14B-Diffusers",
          "title": "Model Variant",
          "description": "The Wan I2V model variant. A14B is balanced; 720P offers higher resolution output."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Describe what to avoid in the generated video (e.g., 'blurry, distorted')."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 81,
          "title": "Num Frames",
          "description": "Total frames in the output video. More frames = longer duration.",
          "min": 16.0,
          "max": 129.0
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 5.0,
          "title": "Guidance Scale",
          "description": "How strongly to follow the prompt. Higher values = more prompt adherence.",
          "min": 1.0,
          "max": 20.0
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 50,
          "title": "Num Inference Steps",
          "description": "Denoising steps. More steps = better quality but slower generation.",
          "min": 1.0,
          "max": 100.0
        },
        {
          "name": "height",
          "type": {
            "type": "int"
          },
          "default": 480,
          "title": "Height",
          "description": "Output video height in pixels.",
          "min": 256.0,
          "max": 1080.0
        },
        {
          "name": "width",
          "type": {
            "type": "int"
          },
          "default": 832,
          "title": "Width",
          "description": "Output video width in pixels.",
          "min": 256.0,
          "max": 1920.0
        },
        {
          "name": "fps",
          "type": {
            "type": "int"
          },
          "default": 16,
          "title": "Fps",
          "description": "Frames per second for the output video file.",
          "min": 1.0,
          "max": 60.0
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducibility. Use -1 for random generation.",
          "min": -1.0
        },
        {
          "name": "max_sequence_length",
          "type": {
            "type": "int"
          },
          "default": 512,
          "title": "Max Sequence Length",
          "description": "Maximum prompt encoding length. Higher allows longer prompts.",
          "min": 64.0,
          "max": 1024.0
        },
        {
          "name": "enable_cpu_offload",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Cpu Offload",
          "description": "Offload model components to CPU to reduce VRAM usage."
        },
        {
          "name": "enable_vae_slicing",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Vae Slicing",
          "description": "Process VAE in slices to reduce peak memory usage."
        },
        {
          "name": "enable_vae_tiling",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Vae Tiling",
          "description": "Process VAE in tiles for very large videos. May affect quality."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "recommended_models": [
        {
          "id": "Wan-AI/Wan2.2-I2V-A14B-Diffusers",
          "type": "hf.text_to_video",
          "name": "Wan-AI/Wan2.2-I2V-A14B-Diffusers",
          "repo_id": "Wan-AI/Wan2.2-I2V-A14B-Diffusers",
          "allow_patterns": [
            "**/*.safetensors",
            "**/*.json",
            "**/*.txt",
            "*.json"
          ],
          "size_on_disk": 126198002795,
          "pipeline_tag": "image-to-video",
          "tags": [
            "diffusers",
            "safetensors",
            "image-to-video",
            "en",
            "zh",
            "arxiv:2503.20314",
            "license:apache-2.0",
            "diffusers:WanImageToVideoPipeline",
            "region:us"
          ],
          "has_model_index": true,
          "downloads": 257837,
          "likes": 173
        },
        {
          "id": "Wan-AI/Wan2.1-I2V-14B-480P-Diffusers",
          "type": "hf.text_to_video",
          "name": "Wan-AI/Wan2.1-I2V-14B-480P-Diffusers",
          "repo_id": "Wan-AI/Wan2.1-I2V-14B-480P-Diffusers",
          "allow_patterns": [
            "**/*.safetensors",
            "**/*.json",
            "**/*.txt",
            "*.json"
          ],
          "size_on_disk": 90093008067,
          "pipeline_tag": "image-to-video",
          "tags": [
            "diffusers",
            "safetensors",
            "video",
            "video-generation",
            "image-to-video",
            "en",
            "zh",
            "license:apache-2.0",
            "diffusers:WanImageToVideoPipeline",
            "region:us"
          ],
          "has_model_index": true,
          "downloads": 37659,
          "likes": 57
        },
        {
          "id": "Wan-AI/Wan2.1-I2V-14B-720P-Diffusers",
          "type": "hf.text_to_video",
          "name": "Wan-AI/Wan2.1-I2V-14B-720P-Diffusers",
          "repo_id": "Wan-AI/Wan2.1-I2V-14B-720P-Diffusers",
          "allow_patterns": [
            "**/*.safetensors",
            "**/*.json",
            "**/*.txt",
            "*.json"
          ],
          "size_on_disk": 90093008067,
          "pipeline_tag": "image-to-video",
          "tags": [
            "diffusers",
            "safetensors",
            "video",
            "video genration",
            "image-to-video",
            "en",
            "zh",
            "license:apache-2.0",
            "diffusers:WanImageToVideoPipeline",
            "region:us"
          ],
          "has_model_index": true,
          "downloads": 10615,
          "likes": 47
        }
      ],
      "basic_fields": [
        "input_image",
        "prompt",
        "num_frames",
        "height",
        "width",
        "model_variant"
      ]
    },
    {
      "title": "Flux Fill",
      "description": "Performs image inpainting/filling using FLUX Fill models with support for GGUF quantization.\n    image, inpainting, fill, flux, quantization, mask\n\n    Use cases:\n    - Fill masked regions in images with high-quality content\n    - Remove unwanted objects from images\n    - Complete missing parts of images\n    - Memory-efficient inpainting using GGUF quantization\n    - High-quality image editing with FLUX models",
      "namespace": "huggingface.image_to_image",
      "node_type": "huggingface.image_to_image.FluxFill",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.inpainting"
          },
          "default": {
            "type": "hf.inpainting",
            "repo_id": "black-forest-labs/FLUX.1-Fill-dev",
            "path": null,
            "variant": null,
            "allow_patterns": null,
            "ignore_patterns": null
          },
          "title": "Model",
          "description": "The FLUX Fill model to use for image inpainting."
        },
        {
          "name": "quantization",
          "type": {
            "type": "enum",
            "values": [
              "fp16",
              "fp4",
              "int4"
            ],
            "type_name": "nodetool.nodes.huggingface.image_to_image.FluxFillQuantization"
          },
          "default": "fp16",
          "title": "Quantization",
          "description": "Quantization level for the FLUX Fill transformer."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "a white paper cup",
          "title": "Prompt",
          "description": "A text prompt describing what should fill the masked area."
        },
        {
          "name": "image",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Input Image",
          "description": "The input image to fill/inpaint"
        },
        {
          "name": "mask_image",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Mask Image",
          "description": "The mask image indicating areas to be filled (white areas will be filled)"
        },
        {
          "name": "height",
          "type": {
            "type": "int"
          },
          "default": 1024,
          "title": "Height",
          "description": "The height of the generated image.",
          "min": 64.0,
          "max": 2048.0
        },
        {
          "name": "width",
          "type": {
            "type": "int"
          },
          "default": 1024,
          "title": "Width",
          "description": "The width of the generated image.",
          "min": 64.0,
          "max": 2048.0
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 30.0,
          "title": "Guidance Scale",
          "description": "Guidance scale for generation. Higher values follow the prompt more closely",
          "min": 0.0,
          "max": 50.0
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 50,
          "title": "Num Inference Steps",
          "description": "Number of denoising steps",
          "min": 1.0,
          "max": 100.0
        },
        {
          "name": "max_sequence_length",
          "type": {
            "type": "int"
          },
          "default": 512,
          "title": "Max Sequence Length",
          "description": "Maximum sequence length for the prompt.",
          "min": 1.0,
          "max": 512.0
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for the random number generator. Use -1 for a random seed.",
          "min": -1.0
        },
        {
          "name": "enable_cpu_offload",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Cpu Offload",
          "description": "Enable CPU offload to reduce VRAM usage."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "recommended_models": [
        {
          "id": "black-forest-labs/FLUX.1-Fill-dev",
          "type": "hf.inpainting",
          "name": "black-forest-labs/FLUX.1-Fill-dev",
          "repo_id": "black-forest-labs/FLUX.1-Fill-dev",
          "allow_patterns": [
            "*.json",
            "*.txt",
            "scheduler/*",
            "vae/*",
            "text_encoder/*",
            "tokenizer/*",
            "tokenizer_2/*",
            "transformer/config.json"
          ],
          "size_on_disk": 586420144,
          "tags": [
            "diffusers",
            "safetensors",
            "image-generation",
            "flux",
            "diffusion-single-file",
            "en",
            "license:other",
            "diffusers:FluxFillPipeline",
            "region:us"
          ],
          "has_model_index": true,
          "downloads": 178454,
          "likes": 962
        },
        {
          "id": "nunchaku-tech/nunchaku-flux.1-fill-dev:svdq-int4_r32-flux.1-fill-dev.safetensors",
          "type": "hf.inpainting",
          "name": "nunchaku-tech/nunchaku-flux.1-fill-dev",
          "repo_id": "nunchaku-tech/nunchaku-flux.1-fill-dev",
          "path": "svdq-int4_r32-flux.1-fill-dev.safetensors",
          "size_on_disk": 6770275936,
          "pipeline_tag": "image-to-image",
          "tags": [
            "diffusers",
            "image-to-image",
            "SVDQuant",
            "FLUX.1-Fill-dev",
            "FLUX.1",
            "Diffusion",
            "Quantization",
            "ICLR2025",
            "en",
            "dataset:mit-han-lab/svdquant-datasets",
            "arxiv:2411.05007",
            "base_model:black-forest-labs/FLUX.1-Fill-dev",
            "base_model:quantized:black-forest-labs/FLUX.1-Fill-dev",
            "license:other",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 8141,
          "likes": 19
        },
        {
          "id": "nunchaku-tech/nunchaku-flux.1-fill-dev:svdq-fp4_r32-flux.1-fill-dev.safetensors",
          "type": "hf.inpainting",
          "name": "nunchaku-tech/nunchaku-flux.1-fill-dev",
          "repo_id": "nunchaku-tech/nunchaku-flux.1-fill-dev",
          "path": "svdq-fp4_r32-flux.1-fill-dev.safetensors",
          "size_on_disk": 7040672992,
          "pipeline_tag": "image-to-image",
          "tags": [
            "diffusers",
            "image-to-image",
            "SVDQuant",
            "FLUX.1-Fill-dev",
            "FLUX.1",
            "Diffusion",
            "Quantization",
            "ICLR2025",
            "en",
            "dataset:mit-han-lab/svdquant-datasets",
            "arxiv:2411.05007",
            "base_model:black-forest-labs/FLUX.1-Fill-dev",
            "base_model:quantized:black-forest-labs/FLUX.1-Fill-dev",
            "license:other",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 8141,
          "likes": 19
        }
      ],
      "basic_fields": [
        "model",
        "quantization",
        "image",
        "mask_image",
        "prompt",
        "height",
        "width",
        "guidance_scale",
        "num_inference_steps",
        "seed"
      ]
    },
    {
      "title": "Flux Kontext",
      "description": "Performs image editing using FLUX Kontext models for context-aware image generation.\n    image, editing, flux, kontext, context-aware, generation\n\n    Use cases:\n    - Edit images based on reference context\n    - Add elements to images guided by prompts\n    - Context-aware image modifications\n    - High-quality image editing with FLUX models",
      "namespace": "huggingface.image_to_image",
      "node_type": "huggingface.image_to_image.FluxKontext",
      "properties": [
        {
          "name": "image",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Input Image",
          "description": "The input image to edit"
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "Add a hat to the cat",
          "title": "Prompt",
          "description": "Text description of the desired edit to apply to the image"
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 2.5,
          "title": "Guidance Scale",
          "description": "Guidance scale for editing. Higher values follow the prompt more closely",
          "min": 0.0,
          "max": 30.0
        },
        {
          "name": "quantization",
          "type": {
            "type": "enum",
            "values": [
              "fp16",
              "fp4",
              "int4"
            ],
            "type_name": "nodetool.nodes.huggingface.image_to_image.FluxKontextQuantization"
          },
          "default": "int4",
          "title": "Quantization",
          "description": "Quantization level for the FLUX Kontext transformer."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for the random number generator. Use -1 for a random seed",
          "min": -1.0
        },
        {
          "name": "enable_cpu_offload",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Cpu Offload",
          "description": "Enable CPU offload to reduce VRAM usage."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "recommended_models": [
        {
          "id": "black-forest-labs/FLUX.1-Kontext-dev",
          "type": "hf.flux_kontext",
          "name": "black-forest-labs/FLUX.1-Kontext-dev",
          "repo_id": "black-forest-labs/FLUX.1-Kontext-dev",
          "allow_patterns": [
            "*.json",
            "*.txt",
            "scheduler/*",
            "vae/*",
            "text_encoder/*",
            "tokenizer/*",
            "tokenizer_2/*"
          ],
          "size_on_disk": 418781428,
          "pipeline_tag": "image-to-image",
          "tags": [
            "diffusers",
            "safetensors",
            "image-generation",
            "flux",
            "diffusion-single-file",
            "image-to-image",
            "en",
            "arxiv:2506.15742",
            "license:other",
            "diffusers:FluxKontextPipeline",
            "region:us"
          ],
          "has_model_index": true,
          "downloads": 328544,
          "likes": 2448
        },
        {
          "id": "nunchaku-tech/nunchaku-flux.1-kontext-dev:svdq-int4_r32-flux.1-kontext-dev.safetensors",
          "type": "hf.flux_kontext",
          "name": "nunchaku-tech/nunchaku-flux.1-kontext-dev",
          "repo_id": "nunchaku-tech/nunchaku-flux.1-kontext-dev",
          "path": "svdq-int4_r32-flux.1-kontext-dev.safetensors",
          "size_on_disk": 6768310048,
          "pipeline_tag": "image-to-image",
          "tags": [
            "diffusers",
            "image-to-image",
            "SVDQuant",
            "FLUX.1-Kontext-dev",
            "FLUX.1",
            "Diffusion",
            "Quantization",
            "ICLR2025",
            "en",
            "dataset:mit-han-lab/svdquant-datasets",
            "arxiv:2411.05007",
            "base_model:black-forest-labs/FLUX.1-Kontext-dev",
            "base_model:quantized:black-forest-labs/FLUX.1-Kontext-dev",
            "license:other",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 38626,
          "likes": 56
        },
        {
          "id": "nunchaku-tech/nunchaku-flux.1-kontext-dev:svdq-fp4_r32-flux.1-kontext-dev.safetensors",
          "type": "hf.flux_kontext",
          "name": "nunchaku-tech/nunchaku-flux.1-kontext-dev",
          "repo_id": "nunchaku-tech/nunchaku-flux.1-kontext-dev",
          "path": "svdq-fp4_r32-flux.1-kontext-dev.safetensors",
          "size_on_disk": 7038707104,
          "pipeline_tag": "image-to-image",
          "tags": [
            "diffusers",
            "image-to-image",
            "SVDQuant",
            "FLUX.1-Kontext-dev",
            "FLUX.1",
            "Diffusion",
            "Quantization",
            "ICLR2025",
            "en",
            "dataset:mit-han-lab/svdquant-datasets",
            "arxiv:2411.05007",
            "base_model:black-forest-labs/FLUX.1-Kontext-dev",
            "base_model:quantized:black-forest-labs/FLUX.1-Kontext-dev",
            "license:other",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 38626,
          "likes": 56
        },
        {
          "id": "nunchaku-tech/nunchaku-t5:awq-int4-flux.1-t5xxl.safetensors",
          "type": "hf.t5",
          "name": "nunchaku-tech/nunchaku-t5",
          "repo_id": "nunchaku-tech/nunchaku-t5",
          "path": "awq-int4-flux.1-t5xxl.safetensors",
          "size_on_disk": 2986819952,
          "pipeline_tag": "text-generation",
          "tags": [
            "transformers",
            "text-generation",
            "AWQ",
            "Quantization",
            "en",
            "dataset:mit-han-lab/svdquant-datasets",
            "arxiv:2411.05007",
            "base_model:google/t5-v1_1-xxl",
            "base_model:quantized:google/t5-v1_1-xxl",
            "license:apache-2.0",
            "endpoints_compatible",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 25
        }
      ],
      "basic_fields": [
        "image",
        "prompt",
        "guidance_scale",
        "quantization"
      ]
    },
    {
      "title": "Image to Image",
      "description": "Transforms existing images based on text prompts using AutoPipeline for Image-to-Image.\n    This node automatically detects the appropriate pipeline class based on the model used.\n    image, generation, image-to-image, autopipeline\n\n    Use cases:\n    - Transform existing images with any compatible model (Stable Diffusion, SDXL, Kandinsky, etc.)\n    - Apply specific styles or concepts to photographs or artwork\n    - Modify existing images based on text descriptions\n    - Create variations of existing visual content with automatic pipeline selection",
      "namespace": "huggingface.image_to_image",
      "node_type": "huggingface.image_to_image.ImageToImage",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.image_to_image"
          },
          "default": {
            "type": "hf.image_to_image",
            "repo_id": "runwayml/stable-diffusion-v1-5",
            "path": null,
            "variant": null,
            "allow_patterns": [
              "*.safetensors",
              "*.bin",
              "*.json",
              "**/*.json"
            ],
            "ignore_patterns": null
          },
          "title": "Model",
          "description": "The HuggingFace model to use for image-to-image generation."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "A beautiful landscape with mountains and a lake at sunset",
          "title": "Prompt",
          "description": "Text prompt describing the desired image transformation."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Text prompt describing what should not appear in the generated image."
        },
        {
          "name": "image",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Input Image",
          "description": "The input image to transform"
        },
        {
          "name": "strength",
          "type": {
            "type": "float"
          },
          "default": 0.8,
          "title": "Strength",
          "description": "Strength of the transformation. Higher values allow for more deviation from the original image.",
          "min": 0.0,
          "max": 1.0
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 25,
          "title": "Num Inference Steps",
          "description": "Number of denoising steps.",
          "min": 1.0,
          "max": 100.0
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 7.5,
          "title": "Guidance Scale",
          "description": "Guidance scale for generation. Higher values follow the prompt more closely.",
          "min": 1.0,
          "max": 20.0
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for the random number generator. Use -1 for a random seed.",
          "min": -1.0
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "model",
        "image",
        "prompt",
        "negative_prompt",
        "strength"
      ]
    },
    {
      "title": "Load Image To Image Model",
      "description": "Load HuggingFace model for image-to-image generation from a repo_id.\n\n    Use cases:\n    - Loads a pipeline directly from a repo_id\n    - Used for ImageToImage node",
      "namespace": "huggingface.image_to_image",
      "node_type": "huggingface.image_to_image.LoadImageToImageModel",
      "properties": [
        {
          "name": "repo_id",
          "type": {
            "type": "str"
          },
          "default": "runwayml/stable-diffusion-v1-5",
          "title": "Repo Id",
          "description": "The repository ID of the model to use for image-to-image generation."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "hf.image_to_image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "repo_id"
      ]
    },
    {
      "title": "OmniGen",
      "description": "Generates and edits images using the OmniGen model, supporting multimodal inputs.\n    image, generation, text-to-image, image-editing, multimodal, omnigen\n\n    Use cases:\n    - Generate images from text prompts\n    - Edit existing images with text instructions\n    - Controllable image generation with reference images\n    - Visual reasoning and image manipulation\n    - ID and object preserving generation",
      "namespace": "huggingface.image_to_image",
      "node_type": "huggingface.image_to_image.OmniGen",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "A realistic photo of a young woman sitting on a sofa, holding a book and facing the camera.",
          "title": "Prompt",
          "description": "The text prompt for image generation. Use <img><|image_1|></img> placeholders to reference input images."
        },
        {
          "name": "input_images",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "image"
              }
            ]
          },
          "default": [],
          "title": "Input Images",
          "description": "List of input images to use for editing or as reference. Referenced in prompt using <img><|image_1|></img>, <img><|image_2|></img>, etc."
        },
        {
          "name": "height",
          "type": {
            "type": "int"
          },
          "default": 1024,
          "title": "Height",
          "description": "Height of the generated image.",
          "min": 64.0,
          "max": 2048.0
        },
        {
          "name": "width",
          "type": {
            "type": "int"
          },
          "default": 1024,
          "title": "Width",
          "description": "Width of the generated image.",
          "min": 64.0,
          "max": 2048.0
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 2.5,
          "title": "Guidance Scale",
          "description": "Guidance scale for generation. Higher values follow the prompt more closely.",
          "min": 1.0,
          "max": 20.0
        },
        {
          "name": "img_guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 1.6,
          "title": "Img Guidance Scale",
          "description": "Image guidance scale when using input images.",
          "min": 1.0,
          "max": 20.0
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 25,
          "title": "Num Inference Steps",
          "description": "Number of denoising steps.",
          "min": 1.0,
          "max": 100.0
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for the random number generator. Use -1 for a random seed.",
          "min": -1.0
        },
        {
          "name": "use_input_image_size_as_output",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Use Input Image Size As Output",
          "description": "If True, use the input image size as output size. Recommended for image editing."
        },
        {
          "name": "max_input_image_size",
          "type": {
            "type": "int"
          },
          "default": 1024,
          "title": "Max Input Image Size",
          "description": "Maximum input image size. Smaller values reduce memory usage but may affect quality.",
          "min": 256.0,
          "max": 2048.0
        },
        {
          "name": "enable_model_cpu_offload",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Model Cpu Offload",
          "description": "Enable CPU offload to reduce memory usage when using multiple images."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "recommended_models": [
        {
          "id": "Shitao/OmniGen-v1-diffusers",
          "type": "hf.model",
          "name": "Shitao/OmniGen-v1-diffusers",
          "repo_id": "Shitao/OmniGen-v1-diffusers",
          "allow_patterns": [
            "README.md",
            "*.safetensors",
            "*.json",
            "**/*.json"
          ],
          "size_on_disk": 8088954905,
          "tags": [
            "diffusers",
            "safetensors",
            "license:mit",
            "diffusers:OmniGenPipeline",
            "region:us"
          ],
          "has_model_index": true,
          "downloads": 827,
          "likes": 4
        }
      ],
      "basic_fields": [
        "prompt",
        "input_images",
        "height",
        "width",
        "guidance_scale"
      ]
    },
    {
      "title": "Qwen Image Edit",
      "description": "Performs image editing using the Qwen Image Edit model with support for Nunchaku quantization.\n    image, editing, semantic, appearance, qwen, multimodal, quantization\n\n    Use cases:\n    - Semantic editing (object rotation, style transfer)\n    - Appearance editing (adding/removing elements)\n    - Precise text modifications in images\n    - Background and clothing changes\n    - Complex image transformations guided by text\n    - Memory-efficient editing using Nunchaku quantization",
      "namespace": "huggingface.image_to_image",
      "node_type": "huggingface.image_to_image.QwenImageEdit",
      "properties": [
        {
          "name": "image",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Input Image",
          "description": "The input image to edit"
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "Change the object's color to blue",
          "title": "Prompt",
          "description": "Text description of the desired edit to apply to the image"
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Text describing what should not appear in the edited image"
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 50,
          "title": "Num Inference Steps",
          "description": "Number of denoising steps for the editing process",
          "min": 1.0,
          "max": 100.0
        },
        {
          "name": "true_cfg_scale",
          "type": {
            "type": "float"
          },
          "default": 4.0,
          "title": "True Cfg Scale",
          "description": "Guidance scale for editing. Higher values follow the prompt more closely",
          "min": 1.0,
          "max": 20.0
        },
        {
          "name": "quantization",
          "type": {
            "type": "enum",
            "values": [
              "fp16",
              "fp4",
              "int4"
            ],
            "type_name": "nodetool.nodes.huggingface.image_to_image.QwenImageEditQuantization"
          },
          "default": "int4",
          "title": "Quantization",
          "description": "Quantization level for the Qwen Image Edit transformer."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for the random number generator. Use -1 for a random seed",
          "min": -1.0
        },
        {
          "name": "enable_memory_efficient_attention",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Memory Efficient Attention",
          "description": "Enable memory efficient attention to reduce VRAM usage."
        },
        {
          "name": "enable_cpu_offload",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Cpu Offload",
          "description": "Enable CPU offload to reduce VRAM usage."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "recommended_models": [
        {
          "id": "Qwen/Qwen-Image-Edit",
          "type": "hf.qwen_image_edit",
          "name": "Qwen/Qwen-Image-Edit",
          "repo_id": "Qwen/Qwen-Image-Edit",
          "allow_patterns": [
            "*.json",
            "*.txt",
            "scheduler/*",
            "vae/*",
            "text_encoder/*",
            "text_encoder_2/*",
            "tokenizer/*",
            "tokenizer_2/*"
          ],
          "size_on_disk": 16859425389,
          "pipeline_tag": "image-to-image",
          "tags": [
            "diffusers",
            "safetensors",
            "image-to-image",
            "en",
            "zh",
            "arxiv:2508.02324",
            "license:apache-2.0",
            "diffusers:QwenImageEditPipeline",
            "region:us"
          ],
          "has_model_index": true,
          "downloads": 82354,
          "likes": 2180
        },
        {
          "id": "nunchaku-tech/nunchaku-qwen-image-edit:svdq-int4_r32-qwen-image-edit.safetensors",
          "type": "hf.qwen_image_edit",
          "name": "nunchaku-tech/nunchaku-qwen-image-edit",
          "repo_id": "nunchaku-tech/nunchaku-qwen-image-edit",
          "path": "svdq-int4_r32-qwen-image-edit.safetensors",
          "size_on_disk": 11521979944,
          "pipeline_tag": "text-to-image",
          "tags": [
            "diffusers",
            "image-editing",
            "SVDQuant",
            "Qwen-Image-Edit",
            "Diffusion",
            "Quantization",
            "ICLR2025",
            "text-to-image",
            "en",
            "dataset:mit-han-lab/svdquant-datasets",
            "arxiv:2411.05007",
            "base_model:Qwen/Qwen-Image-Edit",
            "base_model:quantized:Qwen/Qwen-Image-Edit",
            "license:apache-2.0",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 14337,
          "likes": 105
        },
        {
          "id": "nunchaku-tech/nunchaku-qwen-image-edit:svdq-fp4_r32-qwen-image-edit.safetensors",
          "type": "hf.qwen_image_edit",
          "name": "nunchaku-tech/nunchaku-qwen-image-edit",
          "repo_id": "nunchaku-tech/nunchaku-qwen-image-edit",
          "path": "svdq-fp4_r32-qwen-image-edit.safetensors",
          "size_on_disk": 11948923656,
          "pipeline_tag": "text-to-image",
          "tags": [
            "diffusers",
            "image-editing",
            "SVDQuant",
            "Qwen-Image-Edit",
            "Diffusion",
            "Quantization",
            "ICLR2025",
            "text-to-image",
            "en",
            "dataset:mit-han-lab/svdquant-datasets",
            "arxiv:2411.05007",
            "base_model:Qwen/Qwen-Image-Edit",
            "base_model:quantized:Qwen/Qwen-Image-Edit",
            "license:apache-2.0",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 14337,
          "likes": 105
        },
        {
          "id": "nunchaku-tech/nunchaku-qwen-image-edit-2509:svdq-int4_r32-qwen-image-edit-2509.safetensors",
          "type": "hf.qwen_image_edit",
          "name": "nunchaku-tech/nunchaku-qwen-image-edit-2509",
          "repo_id": "nunchaku-tech/nunchaku-qwen-image-edit-2509",
          "path": "svdq-int4_r32-qwen-image-edit-2509.safetensors",
          "size_on_disk": 11521979944,
          "pipeline_tag": "text-to-image",
          "tags": [
            "diffusers",
            "image-editing",
            "SVDQuant",
            "Qwen-Image-Edit-2509",
            "Diffusion",
            "Quantization",
            "ICLR2025",
            "text-to-image",
            "en",
            "dataset:mit-han-lab/svdquant-datasets",
            "arxiv:2411.05007",
            "base_model:Qwen/Qwen-Image-Edit-2509",
            "base_model:quantized:Qwen/Qwen-Image-Edit-2509",
            "license:apache-2.0",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 152335,
          "likes": 250
        },
        {
          "id": "nunchaku-tech/nunchaku-qwen-image-edit-2509:svdq-fp4_r32-qwen-image-edit-2509.safetensors",
          "type": "hf.qwen_image_edit",
          "name": "nunchaku-tech/nunchaku-qwen-image-edit-2509",
          "repo_id": "nunchaku-tech/nunchaku-qwen-image-edit-2509",
          "path": "svdq-fp4_r32-qwen-image-edit-2509.safetensors",
          "size_on_disk": 11948923656,
          "pipeline_tag": "text-to-image",
          "tags": [
            "diffusers",
            "image-editing",
            "SVDQuant",
            "Qwen-Image-Edit-2509",
            "Diffusion",
            "Quantization",
            "ICLR2025",
            "text-to-image",
            "en",
            "dataset:mit-han-lab/svdquant-datasets",
            "arxiv:2411.05007",
            "base_model:Qwen/Qwen-Image-Edit-2509",
            "base_model:quantized:Qwen/Qwen-Image-Edit-2509",
            "license:apache-2.0",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 152335,
          "likes": 250
        }
      ],
      "basic_fields": [
        "image",
        "prompt",
        "negative_prompt",
        "true_cfg_scale",
        "quantization"
      ]
    },
    {
      "title": "Real ESRGAN",
      "description": "Performs image super-resolution using the RealESRGAN model.\n    image, super-resolution, enhancement, huggingface\n\n    Use cases:\n    - Enhance low-resolution images\n    - Improve image quality for printing or display\n    - Upscale images for better detail",
      "namespace": "huggingface.image_to_image",
      "node_type": "huggingface.image_to_image.RealESRGAN",
      "properties": [
        {
          "name": "image",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Input Image",
          "description": "The input image to transform"
        },
        {
          "name": "model",
          "type": {
            "type": "hf.real_esrgan"
          },
          "default": {
            "type": "hf.real_esrgan",
            "repo_id": "",
            "path": null,
            "variant": null,
            "allow_patterns": null,
            "ignore_patterns": null
          },
          "title": "RealESRGAN Model",
          "description": "The RealESRGAN model to use for image super-resolution"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "recommended_models": [
        {
          "id": "ai-forever/Real-ESRGAN:RealESRGAN_x2.pth",
          "type": "hf.real_esrgan",
          "name": "ai-forever/Real-ESRGAN",
          "repo_id": "ai-forever/Real-ESRGAN",
          "path": "RealESRGAN_x2.pth",
          "size_on_disk": 67061725,
          "tags": [
            "PyTorch",
            "ru",
            "en",
            "arxiv:2107.10833",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 217
        },
        {
          "id": "ai-forever/Real-ESRGAN:RealESRGAN_x4.pth",
          "type": "hf.real_esrgan",
          "name": "ai-forever/Real-ESRGAN",
          "repo_id": "ai-forever/Real-ESRGAN",
          "path": "RealESRGAN_x4.pth",
          "size_on_disk": 67040989,
          "tags": [
            "PyTorch",
            "ru",
            "en",
            "arxiv:2107.10833",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 217
        },
        {
          "id": "ai-forever/Real-ESRGAN:RealESRGAN_x8.pth",
          "type": "hf.real_esrgan",
          "name": "ai-forever/Real-ESRGAN",
          "repo_id": "ai-forever/Real-ESRGAN",
          "path": "RealESRGAN_x8.pth",
          "size_on_disk": 67189359,
          "tags": [
            "PyTorch",
            "ru",
            "en",
            "arxiv:2107.10833",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 217
        },
        {
          "id": "ximso/RealESRGAN_x4plus_anime_6B:RealESRGAN_x4plus_anime_6B.pth",
          "type": "hf.real_esrgan",
          "name": "ximso/RealESRGAN_x4plus_anime_6B",
          "repo_id": "ximso/RealESRGAN_x4plus_anime_6B",
          "path": "RealESRGAN_x4plus_anime_6B.pth",
          "size_on_disk": 17938799,
          "tags": [
            "license:openrail",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 5
        }
      ],
      "basic_fields": [
        "image",
        "model"
      ]
    },
    {
      "title": "Stable Diffusion ControlNet",
      "description": "Generates images using Stable Diffusion with ControlNet guidance.\n    image, generation, text-to-image, controlnet, SD\n\n    Use cases:\n    - Generate images with precise control over composition and structure\n    - Create variations of existing images while maintaining specific features\n    - Artistic image generation with guided outputs",
      "namespace": "huggingface.image_to_image",
      "node_type": "huggingface.image_to_image.StableDiffusionControlNet",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.stable_diffusion"
          },
          "default": {
            "type": "hf.stable_diffusion",
            "repo_id": "",
            "path": null,
            "variant": null,
            "allow_patterns": null,
            "ignore_patterns": null
          },
          "title": "Model",
          "description": "The model to use for image generation."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt for image generation."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to guide what should not appear in the generated image."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for the random number generator. Use -1 for a random seed.",
          "min": -1.0,
          "max": 4294967295.0
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 25,
          "title": "Num Inference Steps",
          "description": "Number of denoising steps.",
          "min": 1.0,
          "max": 100.0
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 7.5,
          "title": "Guidance Scale",
          "description": "Guidance scale for generation.",
          "min": 1.0,
          "max": 20.0
        },
        {
          "name": "scheduler",
          "type": {
            "type": "enum",
            "values": [
              "DPMSolverSDEScheduler",
              "EulerDiscreteScheduler",
              "LMSDiscreteScheduler",
              "DDIMScheduler",
              "DDPMScheduler",
              "HeunDiscreteScheduler",
              "DPMSolverMultistepScheduler",
              "DEISMultistepScheduler",
              "PNDMScheduler",
              "EulerAncestralDiscreteScheduler",
              "UniPCMultistepScheduler",
              "KDPM2DiscreteScheduler",
              "DPMSolverSinglestepScheduler",
              "KDPM2AncestralDiscreteScheduler"
            ],
            "type_name": "nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionBaseNode.StableDiffusionScheduler"
          },
          "default": "EulerDiscreteScheduler",
          "title": "Scheduler",
          "description": "The scheduler to use for the diffusion process."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "hf.lora_sd_config"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRA models to use for image processing"
        },
        {
          "name": "ip_adapter_model",
          "type": {
            "type": "hf.ip_adapter"
          },
          "default": {
            "type": "hf.ip_adapter",
            "repo_id": "",
            "path": null,
            "variant": null,
            "allow_patterns": null,
            "ignore_patterns": null
          },
          "title": "Ip Adapter Model",
          "description": "The IP adapter model to use for image processing"
        },
        {
          "name": "ip_adapter_image",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Ip Adapter Image",
          "description": "When provided the image will be fed into the IP adapter"
        },
        {
          "name": "ip_adapter_scale",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Ip Adapter Scale",
          "description": "The strength of the IP adapter",
          "min": 0.0,
          "max": 1.0
        },
        {
          "name": "latents",
          "type": {
            "type": "torch_tensor"
          },
          "default": {
            "type": "torch_tensor",
            "value": null,
            "dtype": "<i8",
            "shape": [
              1
            ]
          },
          "title": "Latents",
          "description": "Optional initial latents to start generation from."
        },
        {
          "name": "enable_attention_slicing",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Attention Slicing",
          "description": "Enable attention slicing for the pipeline. This can reduce VRAM usage."
        },
        {
          "name": "enable_tiling",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Tiling",
          "description": "Legacy VAE tiling flag (disabled in favor of PyTorch 2 attention optimizations)."
        },
        {
          "name": "enable_cpu_offload",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Cpu Offload",
          "description": "Enable CPU offload for the pipeline. This can reduce VRAM usage."
        },
        {
          "name": "output_type",
          "type": {
            "type": "enum",
            "values": [
              "Image",
              "Latent"
            ],
            "type_name": "nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionBaseNode.StableDiffusionOutputType"
          },
          "default": "Image",
          "title": "Output Type",
          "description": "The type of output to generate."
        },
        {
          "name": "controlnet",
          "type": {
            "type": "hf.controlnet"
          },
          "default": {
            "type": "hf.controlnet",
            "repo_id": "",
            "path": null,
            "variant": null,
            "allow_patterns": null,
            "ignore_patterns": null
          },
          "title": "Controlnet",
          "description": "The ControlNet model to use for guidance."
        },
        {
          "name": "control_image",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Control Image",
          "description": "The control image to guide the generation process."
        },
        {
          "name": "controlnet_conditioning_scale",
          "type": {
            "type": "float"
          },
          "default": 1.0,
          "title": "Controlnet Conditioning Scale",
          "description": "The scale for ControlNet conditioning.",
          "min": 0.0,
          "max": 2.0
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "image"
        },
        {
          "type": {
            "type": "torch_tensor"
          },
          "name": "latent"
        }
      ],
      "recommended_models": [
        {
          "id": "lllyasviel/control_v11p_sd15_canny:diffusion_pytorch_model.fp16.safetensors",
          "type": "hf.controlnet",
          "name": "lllyasviel/control_v11p_sd15_canny",
          "repo_id": "lllyasviel/control_v11p_sd15_canny",
          "path": "diffusion_pytorch_model.fp16.safetensors",
          "size_on_disk": 722598642,
          "pipeline_tag": "image-to-image",
          "tags": [
            "diffusers",
            "safetensors",
            "art",
            "controlnet",
            "stable-diffusion",
            "controlnet-v1-1",
            "image-to-image",
            "arxiv:2302.05543",
            "base_model:runwayml/stable-diffusion-v1-5",
            "base_model:adapter:runwayml/stable-diffusion-v1-5",
            "license:openrail",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 13691,
          "likes": 55
        },
        {
          "id": "lllyasviel/control_v11p_sd15_inpaint:diffusion_pytorch_model.fp16.safetensors",
          "type": "hf.controlnet",
          "name": "lllyasviel/control_v11p_sd15_inpaint",
          "repo_id": "lllyasviel/control_v11p_sd15_inpaint",
          "path": "diffusion_pytorch_model.fp16.safetensors",
          "size_on_disk": 722598642,
          "pipeline_tag": "image-to-image",
          "tags": [
            "diffusers",
            "safetensors",
            "art",
            "controlnet",
            "stable-diffusion",
            "controlnet-v1-1",
            "image-to-image",
            "arxiv:2302.05543",
            "base_model:runwayml/stable-diffusion-v1-5",
            "base_model:adapter:runwayml/stable-diffusion-v1-5",
            "license:openrail",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 27026,
          "likes": 132
        },
        {
          "id": "lllyasviel/control_v11p_sd15_mlsd:diffusion_pytorch_model.fp16.safetensors",
          "type": "hf.controlnet",
          "name": "lllyasviel/control_v11p_sd15_mlsd",
          "repo_id": "lllyasviel/control_v11p_sd15_mlsd",
          "path": "diffusion_pytorch_model.fp16.safetensors",
          "size_on_disk": 722598642,
          "pipeline_tag": "image-to-image",
          "tags": [
            "diffusers",
            "safetensors",
            "art",
            "controlnet",
            "stable-diffusion",
            "controlnet-v1-1",
            "image-to-image",
            "arxiv:2302.05543",
            "base_model:runwayml/stable-diffusion-v1-5",
            "base_model:adapter:runwayml/stable-diffusion-v1-5",
            "license:openrail",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 11665,
          "likes": 17
        },
        {
          "id": "lllyasviel/control_v11p_sd15_lineart:diffusion_pytorch_model.fp16.safetensors",
          "type": "hf.controlnet",
          "name": "lllyasviel/control_v11p_sd15_lineart",
          "repo_id": "lllyasviel/control_v11p_sd15_lineart",
          "path": "diffusion_pytorch_model.fp16.safetensors",
          "size_on_disk": 722598642,
          "pipeline_tag": "image-to-image",
          "tags": [
            "diffusers",
            "safetensors",
            "art",
            "controlnet",
            "stable-diffusion",
            "controlnet-v1-1",
            "image-to-image",
            "arxiv:2302.05543",
            "base_model:runwayml/stable-diffusion-v1-5",
            "base_model:adapter:runwayml/stable-diffusion-v1-5",
            "license:openrail",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 9524,
          "likes": 72
        },
        {
          "id": "lllyasviel/control_v11p_sd15_scribble:diffusion_pytorch_model.fp16.safetensors",
          "type": "hf.controlnet",
          "name": "lllyasviel/control_v11p_sd15_scribble",
          "repo_id": "lllyasviel/control_v11p_sd15_scribble",
          "path": "diffusion_pytorch_model.fp16.safetensors",
          "size_on_disk": 722598642,
          "pipeline_tag": "image-to-image",
          "tags": [
            "diffusers",
            "safetensors",
            "art",
            "controlnet",
            "stable-diffusion",
            "controlnet-v1-1",
            "image-to-image",
            "arxiv:2302.05543",
            "base_model:runwayml/stable-diffusion-v1-5",
            "base_model:adapter:runwayml/stable-diffusion-v1-5",
            "license:openrail",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 7183,
          "likes": 29
        },
        {
          "id": "lllyasviel/control_v11p_sd15_openpose:diffusion_pytorch_model.fp16.safetensors",
          "type": "hf.controlnet",
          "name": "lllyasviel/control_v11p_sd15_openpose",
          "repo_id": "lllyasviel/control_v11p_sd15_openpose",
          "path": "diffusion_pytorch_model.fp16.safetensors",
          "size_on_disk": 722598642,
          "pipeline_tag": "image-to-image",
          "tags": [
            "diffusers",
            "safetensors",
            "art",
            "controlnet",
            "stable-diffusion",
            "controlnet-v1-1",
            "image-to-image",
            "arxiv:2302.05543",
            "base_model:runwayml/stable-diffusion-v1-5",
            "base_model:adapter:runwayml/stable-diffusion-v1-5",
            "license:openrail",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 13419,
          "likes": 130
        },
        {
          "id": "lllyasviel/sd_control_collection:ip-adapter_sd15_plus.pth",
          "type": "hf.controlnet",
          "name": "lllyasviel/sd_control_collection",
          "repo_id": "lllyasviel/sd_control_collection",
          "path": "ip-adapter_sd15_plus.pth",
          "size_on_disk": 158030471,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 2074
        },
        {
          "id": "lllyasviel/sd_control_collection:ip-adapter_sd15.pth",
          "type": "hf.controlnet",
          "name": "lllyasviel/sd_control_collection",
          "repo_id": "lllyasviel/sd_control_collection",
          "path": "ip-adapter_sd15.pth",
          "size_on_disk": 44642819,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 2074
        },
        {
          "id": "lllyasviel/sd_control_collection:ioclab_sd15_recolor.safetensors",
          "type": "hf.controlnet",
          "name": "lllyasviel/sd_control_collection",
          "repo_id": "lllyasviel/sd_control_collection",
          "path": "ioclab_sd15_recolor.safetensors",
          "size_on_disk": 722598616,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 2074
        },
        {
          "id": "h94/IP-Adapter:models/ip-adapter_sd15.bin",
          "type": "hf.ip_adapter",
          "name": "h94/IP-Adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "models/ip-adapter_sd15.bin",
          "size_on_disk": 44642825,
          "pipeline_tag": "text-to-image",
          "tags": [
            "diffusers",
            "safetensors",
            "text-to-image",
            "stable-diffusion",
            "en",
            "arxiv:2308.06721",
            "license:apache-2.0",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 1262
        },
        {
          "id": "h94/IP-Adapter:models/ip-adapter_sd15_light.bin",
          "type": "hf.ip_adapter",
          "name": "h94/IP-Adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "models/ip-adapter_sd15_light.bin",
          "size_on_disk": 44642819,
          "pipeline_tag": "text-to-image",
          "tags": [
            "diffusers",
            "safetensors",
            "text-to-image",
            "stable-diffusion",
            "en",
            "arxiv:2308.06721",
            "license:apache-2.0",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 1262
        },
        {
          "id": "h94/IP-Adapter:models/ip-adapter_sd15_vit-G.bin",
          "type": "hf.ip_adapter",
          "name": "h94/IP-Adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "models/ip-adapter_sd15_vit-G.bin",
          "size_on_disk": 46215689,
          "pipeline_tag": "text-to-image",
          "tags": [
            "diffusers",
            "safetensors",
            "text-to-image",
            "stable-diffusion",
            "en",
            "arxiv:2308.06721",
            "license:apache-2.0",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 1262
        },
        {
          "id": "Lykon/DreamShaper:DreamShaper_6.2_BakedVae_pruned.safetensors",
          "type": "hf.stable_diffusion",
          "name": "Lykon/DreamShaper",
          "repo_id": "Lykon/DreamShaper",
          "path": "DreamShaper_6.2_BakedVae_pruned.safetensors",
          "size_on_disk": 2132625894,
          "pipeline_tag": "text-to-image",
          "tags": [
            "diffusers",
            "stable-diffusion",
            "stable-diffusion-diffusers",
            "text-to-image",
            "art",
            "artistic",
            "anime",
            "en",
            "doi:10.57967/hf/0453",
            "license:other",
            "diffusers:StableDiffusionPipeline",
            "region:us"
          ],
          "has_model_index": true,
          "downloads": 86342,
          "likes": 996
        }
      ],
      "basic_fields": [
        "model",
        "prompt",
        "controlnet",
        "control_image",
        "controlnet_conditioning_scale"
      ]
    },
    {
      "title": "Stable Diffusion ControlNet (Img2Img)",
      "description": "Transforms existing images using Stable Diffusion with ControlNet guidance.\n    image, generation, image-to-image, controlnet, SD, style-transfer\n\n    Use cases:\n    - Modify existing images with precise control over composition and structure\n    - Apply specific styles or concepts to photographs or artwork with guided transformations\n    - Create variations of existing visual content while maintaining certain features\n    - Enhance image editing capabilities with AI-guided transformations",
      "namespace": "huggingface.image_to_image",
      "node_type": "huggingface.image_to_image.StableDiffusionControlNetImg2Img",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.stable_diffusion"
          },
          "default": {
            "type": "hf.stable_diffusion",
            "repo_id": "",
            "path": null,
            "variant": null,
            "allow_patterns": null,
            "ignore_patterns": null
          },
          "title": "Model",
          "description": "The model to use for image generation."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt for image generation."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to guide what should not appear in the generated image."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for the random number generator. Use -1 for a random seed.",
          "min": -1.0,
          "max": 4294967295.0
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 25,
          "title": "Num Inference Steps",
          "description": "Number of denoising steps.",
          "min": 1.0,
          "max": 100.0
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 7.5,
          "title": "Guidance Scale",
          "description": "Guidance scale for generation.",
          "min": 1.0,
          "max": 20.0
        },
        {
          "name": "scheduler",
          "type": {
            "type": "enum",
            "values": [
              "DPMSolverSDEScheduler",
              "EulerDiscreteScheduler",
              "LMSDiscreteScheduler",
              "DDIMScheduler",
              "DDPMScheduler",
              "HeunDiscreteScheduler",
              "DPMSolverMultistepScheduler",
              "DEISMultistepScheduler",
              "PNDMScheduler",
              "EulerAncestralDiscreteScheduler",
              "UniPCMultistepScheduler",
              "KDPM2DiscreteScheduler",
              "DPMSolverSinglestepScheduler",
              "KDPM2AncestralDiscreteScheduler"
            ],
            "type_name": "nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionBaseNode.StableDiffusionScheduler"
          },
          "default": "EulerDiscreteScheduler",
          "title": "Scheduler",
          "description": "The scheduler to use for the diffusion process."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "hf.lora_sd_config"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRA models to use for image processing"
        },
        {
          "name": "ip_adapter_model",
          "type": {
            "type": "hf.ip_adapter"
          },
          "default": {
            "type": "hf.ip_adapter",
            "repo_id": "",
            "path": null,
            "variant": null,
            "allow_patterns": null,
            "ignore_patterns": null
          },
          "title": "Ip Adapter Model",
          "description": "The IP adapter model to use for image processing"
        },
        {
          "name": "ip_adapter_image",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Ip Adapter Image",
          "description": "When provided the image will be fed into the IP adapter"
        },
        {
          "name": "ip_adapter_scale",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Ip Adapter Scale",
          "description": "The strength of the IP adapter",
          "min": 0.0,
          "max": 1.0
        },
        {
          "name": "latents",
          "type": {
            "type": "torch_tensor"
          },
          "default": {
            "type": "torch_tensor",
            "value": null,
            "dtype": "<i8",
            "shape": [
              1
            ]
          },
          "title": "Latents",
          "description": "Optional initial latents to start generation from."
        },
        {
          "name": "enable_attention_slicing",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Attention Slicing",
          "description": "Enable attention slicing for the pipeline. This can reduce VRAM usage."
        },
        {
          "name": "enable_tiling",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Tiling",
          "description": "Legacy VAE tiling flag (disabled in favor of PyTorch 2 attention optimizations)."
        },
        {
          "name": "enable_cpu_offload",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Cpu Offload",
          "description": "Enable CPU offload for the pipeline. This can reduce VRAM usage."
        },
        {
          "name": "output_type",
          "type": {
            "type": "enum",
            "values": [
              "Image",
              "Latent"
            ],
            "type_name": "nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionBaseNode.StableDiffusionOutputType"
          },
          "default": "Image",
          "title": "Output Type",
          "description": "The type of output to generate."
        },
        {
          "name": "image",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image",
          "description": "The input image to be transformed."
        },
        {
          "name": "strength",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Strength",
          "description": "Similarity to the input image",
          "min": 0.0,
          "max": 1.0
        },
        {
          "name": "controlnet",
          "type": {
            "type": "hf.controlnet"
          },
          "default": {
            "type": "hf.controlnet",
            "repo_id": "",
            "path": null,
            "variant": null,
            "allow_patterns": null,
            "ignore_patterns": null
          },
          "title": "Controlnet",
          "description": "The ControlNet model to use for guidance."
        },
        {
          "name": "control_image",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Control Image",
          "description": "The control image to guide the transformation."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "image"
        },
        {
          "type": {
            "type": "torch_tensor"
          },
          "name": "latent"
        }
      ],
      "recommended_models": [
        {
          "id": "lllyasviel/control_v11p_sd15_canny:diffusion_pytorch_model.fp16.safetensors",
          "type": "hf.controlnet",
          "name": "lllyasviel/control_v11p_sd15_canny",
          "repo_id": "lllyasviel/control_v11p_sd15_canny",
          "path": "diffusion_pytorch_model.fp16.safetensors",
          "size_on_disk": 722598642,
          "pipeline_tag": "image-to-image",
          "tags": [
            "diffusers",
            "safetensors",
            "art",
            "controlnet",
            "stable-diffusion",
            "controlnet-v1-1",
            "image-to-image",
            "arxiv:2302.05543",
            "base_model:runwayml/stable-diffusion-v1-5",
            "base_model:adapter:runwayml/stable-diffusion-v1-5",
            "license:openrail",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 13691,
          "likes": 55
        },
        {
          "id": "lllyasviel/control_v11p_sd15_inpaint:diffusion_pytorch_model.fp16.safetensors",
          "type": "hf.controlnet",
          "name": "lllyasviel/control_v11p_sd15_inpaint",
          "repo_id": "lllyasviel/control_v11p_sd15_inpaint",
          "path": "diffusion_pytorch_model.fp16.safetensors",
          "size_on_disk": 722598642,
          "pipeline_tag": "image-to-image",
          "tags": [
            "diffusers",
            "safetensors",
            "art",
            "controlnet",
            "stable-diffusion",
            "controlnet-v1-1",
            "image-to-image",
            "arxiv:2302.05543",
            "base_model:runwayml/stable-diffusion-v1-5",
            "base_model:adapter:runwayml/stable-diffusion-v1-5",
            "license:openrail",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 27026,
          "likes": 132
        },
        {
          "id": "lllyasviel/control_v11p_sd15_mlsd:diffusion_pytorch_model.fp16.safetensors",
          "type": "hf.controlnet",
          "name": "lllyasviel/control_v11p_sd15_mlsd",
          "repo_id": "lllyasviel/control_v11p_sd15_mlsd",
          "path": "diffusion_pytorch_model.fp16.safetensors",
          "size_on_disk": 722598642,
          "pipeline_tag": "image-to-image",
          "tags": [
            "diffusers",
            "safetensors",
            "art",
            "controlnet",
            "stable-diffusion",
            "controlnet-v1-1",
            "image-to-image",
            "arxiv:2302.05543",
            "base_model:runwayml/stable-diffusion-v1-5",
            "base_model:adapter:runwayml/stable-diffusion-v1-5",
            "license:openrail",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 11665,
          "likes": 17
        },
        {
          "id": "lllyasviel/control_v11p_sd15_lineart:diffusion_pytorch_model.fp16.safetensors",
          "type": "hf.controlnet",
          "name": "lllyasviel/control_v11p_sd15_lineart",
          "repo_id": "lllyasviel/control_v11p_sd15_lineart",
          "path": "diffusion_pytorch_model.fp16.safetensors",
          "size_on_disk": 722598642,
          "pipeline_tag": "image-to-image",
          "tags": [
            "diffusers",
            "safetensors",
            "art",
            "controlnet",
            "stable-diffusion",
            "controlnet-v1-1",
            "image-to-image",
            "arxiv:2302.05543",
            "base_model:runwayml/stable-diffusion-v1-5",
            "base_model:adapter:runwayml/stable-diffusion-v1-5",
            "license:openrail",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 9524,
          "likes": 72
        },
        {
          "id": "lllyasviel/control_v11p_sd15_scribble:diffusion_pytorch_model.fp16.safetensors",
          "type": "hf.controlnet",
          "name": "lllyasviel/control_v11p_sd15_scribble",
          "repo_id": "lllyasviel/control_v11p_sd15_scribble",
          "path": "diffusion_pytorch_model.fp16.safetensors",
          "size_on_disk": 722598642,
          "pipeline_tag": "image-to-image",
          "tags": [
            "diffusers",
            "safetensors",
            "art",
            "controlnet",
            "stable-diffusion",
            "controlnet-v1-1",
            "image-to-image",
            "arxiv:2302.05543",
            "base_model:runwayml/stable-diffusion-v1-5",
            "base_model:adapter:runwayml/stable-diffusion-v1-5",
            "license:openrail",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 7183,
          "likes": 29
        },
        {
          "id": "lllyasviel/control_v11p_sd15_openpose:diffusion_pytorch_model.fp16.safetensors",
          "type": "hf.controlnet",
          "name": "lllyasviel/control_v11p_sd15_openpose",
          "repo_id": "lllyasviel/control_v11p_sd15_openpose",
          "path": "diffusion_pytorch_model.fp16.safetensors",
          "size_on_disk": 722598642,
          "pipeline_tag": "image-to-image",
          "tags": [
            "diffusers",
            "safetensors",
            "art",
            "controlnet",
            "stable-diffusion",
            "controlnet-v1-1",
            "image-to-image",
            "arxiv:2302.05543",
            "base_model:runwayml/stable-diffusion-v1-5",
            "base_model:adapter:runwayml/stable-diffusion-v1-5",
            "license:openrail",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 13419,
          "likes": 130
        },
        {
          "id": "lllyasviel/sd_control_collection:ip-adapter_sd15_plus.pth",
          "type": "hf.controlnet",
          "name": "lllyasviel/sd_control_collection",
          "repo_id": "lllyasviel/sd_control_collection",
          "path": "ip-adapter_sd15_plus.pth",
          "size_on_disk": 158030471,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 2074
        },
        {
          "id": "lllyasviel/sd_control_collection:ip-adapter_sd15.pth",
          "type": "hf.controlnet",
          "name": "lllyasviel/sd_control_collection",
          "repo_id": "lllyasviel/sd_control_collection",
          "path": "ip-adapter_sd15.pth",
          "size_on_disk": 44642819,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 2074
        },
        {
          "id": "lllyasviel/sd_control_collection:ioclab_sd15_recolor.safetensors",
          "type": "hf.controlnet",
          "name": "lllyasviel/sd_control_collection",
          "repo_id": "lllyasviel/sd_control_collection",
          "path": "ioclab_sd15_recolor.safetensors",
          "size_on_disk": 722598616,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 2074
        },
        {
          "id": "h94/IP-Adapter:models/ip-adapter_sd15.bin",
          "type": "hf.ip_adapter",
          "name": "h94/IP-Adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "models/ip-adapter_sd15.bin",
          "size_on_disk": 44642825,
          "pipeline_tag": "text-to-image",
          "tags": [
            "diffusers",
            "safetensors",
            "text-to-image",
            "stable-diffusion",
            "en",
            "arxiv:2308.06721",
            "license:apache-2.0",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 1262
        },
        {
          "id": "h94/IP-Adapter:models/ip-adapter_sd15_light.bin",
          "type": "hf.ip_adapter",
          "name": "h94/IP-Adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "models/ip-adapter_sd15_light.bin",
          "size_on_disk": 44642819,
          "pipeline_tag": "text-to-image",
          "tags": [
            "diffusers",
            "safetensors",
            "text-to-image",
            "stable-diffusion",
            "en",
            "arxiv:2308.06721",
            "license:apache-2.0",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 1262
        },
        {
          "id": "h94/IP-Adapter:models/ip-adapter_sd15_vit-G.bin",
          "type": "hf.ip_adapter",
          "name": "h94/IP-Adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "models/ip-adapter_sd15_vit-G.bin",
          "size_on_disk": 46215689,
          "pipeline_tag": "text-to-image",
          "tags": [
            "diffusers",
            "safetensors",
            "text-to-image",
            "stable-diffusion",
            "en",
            "arxiv:2308.06721",
            "license:apache-2.0",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 1262
        },
        {
          "id": "Lykon/DreamShaper:DreamShaper_6.2_BakedVae_pruned.safetensors",
          "type": "hf.stable_diffusion",
          "name": "Lykon/DreamShaper",
          "repo_id": "Lykon/DreamShaper",
          "path": "DreamShaper_6.2_BakedVae_pruned.safetensors",
          "size_on_disk": 2132625894,
          "pipeline_tag": "text-to-image",
          "tags": [
            "diffusers",
            "stable-diffusion",
            "stable-diffusion-diffusers",
            "text-to-image",
            "art",
            "artistic",
            "anime",
            "en",
            "doi:10.57967/hf/0453",
            "license:other",
            "diffusers:StableDiffusionPipeline",
            "region:us"
          ],
          "has_model_index": true,
          "downloads": 86342,
          "likes": 996
        }
      ],
      "basic_fields": [
        "model",
        "prompt",
        "image",
        "controlnet",
        "control_image"
      ]
    },
    {
      "title": "Stable Diffusion ControlNet Inpaint",
      "description": "Performs inpainting on images using Stable Diffusion with ControlNet guidance.\n    image, inpainting, controlnet, SD, style-transfer\n\n    Use cases:\n    - Remove unwanted objects from images with precise control\n    - Fill in missing parts of images guided by control images\n    - Modify specific areas of images while preserving the rest and maintaining structure",
      "namespace": "huggingface.image_to_image",
      "node_type": "huggingface.image_to_image.StableDiffusionControlNetInpaint",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.stable_diffusion"
          },
          "default": {
            "type": "hf.stable_diffusion",
            "repo_id": "",
            "path": null,
            "variant": null,
            "allow_patterns": null,
            "ignore_patterns": null
          },
          "title": "Model",
          "description": "The model to use for image generation."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt for image generation."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to guide what should not appear in the generated image."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for the random number generator. Use -1 for a random seed.",
          "min": -1.0,
          "max": 4294967295.0
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 25,
          "title": "Num Inference Steps",
          "description": "Number of denoising steps.",
          "min": 1.0,
          "max": 100.0
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 7.5,
          "title": "Guidance Scale",
          "description": "Guidance scale for generation.",
          "min": 1.0,
          "max": 20.0
        },
        {
          "name": "scheduler",
          "type": {
            "type": "enum",
            "values": [
              "DPMSolverSDEScheduler",
              "EulerDiscreteScheduler",
              "LMSDiscreteScheduler",
              "DDIMScheduler",
              "DDPMScheduler",
              "HeunDiscreteScheduler",
              "DPMSolverMultistepScheduler",
              "DEISMultistepScheduler",
              "PNDMScheduler",
              "EulerAncestralDiscreteScheduler",
              "UniPCMultistepScheduler",
              "KDPM2DiscreteScheduler",
              "DPMSolverSinglestepScheduler",
              "KDPM2AncestralDiscreteScheduler"
            ],
            "type_name": "nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionBaseNode.StableDiffusionScheduler"
          },
          "default": "EulerDiscreteScheduler",
          "title": "Scheduler",
          "description": "The scheduler to use for the diffusion process."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "hf.lora_sd_config"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRA models to use for image processing"
        },
        {
          "name": "ip_adapter_model",
          "type": {
            "type": "hf.ip_adapter"
          },
          "default": {
            "type": "hf.ip_adapter",
            "repo_id": "",
            "path": null,
            "variant": null,
            "allow_patterns": null,
            "ignore_patterns": null
          },
          "title": "Ip Adapter Model",
          "description": "The IP adapter model to use for image processing"
        },
        {
          "name": "ip_adapter_image",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Ip Adapter Image",
          "description": "When provided the image will be fed into the IP adapter"
        },
        {
          "name": "ip_adapter_scale",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Ip Adapter Scale",
          "description": "The strength of the IP adapter",
          "min": 0.0,
          "max": 1.0
        },
        {
          "name": "latents",
          "type": {
            "type": "torch_tensor"
          },
          "default": {
            "type": "torch_tensor",
            "value": null,
            "dtype": "<i8",
            "shape": [
              1
            ]
          },
          "title": "Latents",
          "description": "Optional initial latents to start generation from."
        },
        {
          "name": "enable_attention_slicing",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Attention Slicing",
          "description": "Enable attention slicing for the pipeline. This can reduce VRAM usage."
        },
        {
          "name": "enable_tiling",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Tiling",
          "description": "Legacy VAE tiling flag (disabled in favor of PyTorch 2 attention optimizations)."
        },
        {
          "name": "enable_cpu_offload",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Cpu Offload",
          "description": "Enable CPU offload for the pipeline. This can reduce VRAM usage."
        },
        {
          "name": "output_type",
          "type": {
            "type": "enum",
            "values": [
              "Image",
              "Latent"
            ],
            "type_name": "nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionBaseNode.StableDiffusionOutputType"
          },
          "default": "Image",
          "title": "Output Type",
          "description": "The type of output to generate."
        },
        {
          "name": "controlnet",
          "type": {
            "type": "enum",
            "values": [
              "lllyasviel/control_v11p_sd15_inpaint"
            ],
            "type_name": "nodetool.nodes.huggingface.image_to_image.StableDiffusionControlNetModel"
          },
          "default": "lllyasviel/control_v11p_sd15_inpaint",
          "title": "Controlnet",
          "description": "The ControlNet model to use for guidance."
        },
        {
          "name": "init_image",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Init Image",
          "description": "The initial image to be inpainted."
        },
        {
          "name": "mask_image",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Mask Image",
          "description": "The mask image indicating areas to be inpainted."
        },
        {
          "name": "control_image",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Control Image",
          "description": "The control image to guide the inpainting process."
        },
        {
          "name": "controlnet_conditioning_scale",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Controlnet Conditioning Scale",
          "description": "The scale for ControlNet conditioning.",
          "min": 0.0,
          "max": 2.0
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "image"
        },
        {
          "type": {
            "type": "torch_tensor"
          },
          "name": "latent"
        }
      ],
      "recommended_models": [
        {
          "id": "h94/IP-Adapter:models/ip-adapter_sd15.bin",
          "type": "hf.ip_adapter",
          "name": "h94/IP-Adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "models/ip-adapter_sd15.bin",
          "size_on_disk": 44642825,
          "pipeline_tag": "text-to-image",
          "tags": [
            "diffusers",
            "safetensors",
            "text-to-image",
            "stable-diffusion",
            "en",
            "arxiv:2308.06721",
            "license:apache-2.0",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 1262
        },
        {
          "id": "h94/IP-Adapter:models/ip-adapter_sd15_light.bin",
          "type": "hf.ip_adapter",
          "name": "h94/IP-Adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "models/ip-adapter_sd15_light.bin",
          "size_on_disk": 44642819,
          "pipeline_tag": "text-to-image",
          "tags": [
            "diffusers",
            "safetensors",
            "text-to-image",
            "stable-diffusion",
            "en",
            "arxiv:2308.06721",
            "license:apache-2.0",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 1262
        },
        {
          "id": "h94/IP-Adapter:models/ip-adapter_sd15_vit-G.bin",
          "type": "hf.ip_adapter",
          "name": "h94/IP-Adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "models/ip-adapter_sd15_vit-G.bin",
          "size_on_disk": 46215689,
          "pipeline_tag": "text-to-image",
          "tags": [
            "diffusers",
            "safetensors",
            "text-to-image",
            "stable-diffusion",
            "en",
            "arxiv:2308.06721",
            "license:apache-2.0",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 1262
        },
        {
          "id": "Lykon/DreamShaper:DreamShaper_6.2_BakedVae_pruned.safetensors",
          "type": "hf.stable_diffusion",
          "name": "Lykon/DreamShaper",
          "repo_id": "Lykon/DreamShaper",
          "path": "DreamShaper_6.2_BakedVae_pruned.safetensors",
          "size_on_disk": 2132625894,
          "pipeline_tag": "text-to-image",
          "tags": [
            "diffusers",
            "stable-diffusion",
            "stable-diffusion-diffusers",
            "text-to-image",
            "art",
            "artistic",
            "anime",
            "en",
            "doi:10.57967/hf/0453",
            "license:other",
            "diffusers:StableDiffusionPipeline",
            "region:us"
          ],
          "has_model_index": true,
          "downloads": 86342,
          "likes": 996
        }
      ],
      "basic_fields": [
        "model",
        "prompt",
        "init_image",
        "mask_image",
        "control_image",
        "controlnet_conditioning_scale"
      ]
    },
    {
      "title": "Stable Diffusion (Img2Img)",
      "description": "Transforms existing images based on text prompts using Stable Diffusion.\n    image, generation, image-to-image, SD, img2img, style-transfer\n\n    Use cases:\n    - Modifying existing images to fit a specific style or theme\n    - Enhancing or altering photographs\n    - Creating variations of existing artwork\n    - Applying text-guided edits to images",
      "namespace": "huggingface.image_to_image",
      "node_type": "huggingface.image_to_image.StableDiffusionImg2Img",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.stable_diffusion"
          },
          "default": {
            "type": "hf.stable_diffusion",
            "repo_id": "",
            "path": null,
            "variant": null,
            "allow_patterns": null,
            "ignore_patterns": null
          },
          "title": "Model",
          "description": "The model to use for image generation."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt for image generation."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to guide what should not appear in the generated image."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for the random number generator. Use -1 for a random seed.",
          "min": -1.0,
          "max": 4294967295.0
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 25,
          "title": "Num Inference Steps",
          "description": "Number of denoising steps.",
          "min": 1.0,
          "max": 100.0
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 7.5,
          "title": "Guidance Scale",
          "description": "Guidance scale for generation.",
          "min": 1.0,
          "max": 20.0
        },
        {
          "name": "scheduler",
          "type": {
            "type": "enum",
            "values": [
              "DPMSolverSDEScheduler",
              "EulerDiscreteScheduler",
              "LMSDiscreteScheduler",
              "DDIMScheduler",
              "DDPMScheduler",
              "HeunDiscreteScheduler",
              "DPMSolverMultistepScheduler",
              "DEISMultistepScheduler",
              "PNDMScheduler",
              "EulerAncestralDiscreteScheduler",
              "UniPCMultistepScheduler",
              "KDPM2DiscreteScheduler",
              "DPMSolverSinglestepScheduler",
              "KDPM2AncestralDiscreteScheduler"
            ],
            "type_name": "nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionBaseNode.StableDiffusionScheduler"
          },
          "default": "EulerDiscreteScheduler",
          "title": "Scheduler",
          "description": "The scheduler to use for the diffusion process."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "hf.lora_sd_config"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRA models to use for image processing"
        },
        {
          "name": "ip_adapter_model",
          "type": {
            "type": "hf.ip_adapter"
          },
          "default": {
            "type": "hf.ip_adapter",
            "repo_id": "",
            "path": null,
            "variant": null,
            "allow_patterns": null,
            "ignore_patterns": null
          },
          "title": "Ip Adapter Model",
          "description": "The IP adapter model to use for image processing"
        },
        {
          "name": "ip_adapter_image",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Ip Adapter Image",
          "description": "When provided the image will be fed into the IP adapter"
        },
        {
          "name": "ip_adapter_scale",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Ip Adapter Scale",
          "description": "The strength of the IP adapter",
          "min": 0.0,
          "max": 1.0
        },
        {
          "name": "latents",
          "type": {
            "type": "torch_tensor"
          },
          "default": {
            "type": "torch_tensor",
            "value": null,
            "dtype": "<i8",
            "shape": [
              1
            ]
          },
          "title": "Latents",
          "description": "Optional initial latents to start generation from."
        },
        {
          "name": "enable_attention_slicing",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Attention Slicing",
          "description": "Enable attention slicing for the pipeline. This can reduce VRAM usage."
        },
        {
          "name": "enable_tiling",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Tiling",
          "description": "Legacy VAE tiling flag (disabled in favor of PyTorch 2 attention optimizations)."
        },
        {
          "name": "enable_cpu_offload",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Cpu Offload",
          "description": "Enable CPU offload for the pipeline. This can reduce VRAM usage."
        },
        {
          "name": "output_type",
          "type": {
            "type": "enum",
            "values": [
              "Image",
              "Latent"
            ],
            "type_name": "nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionBaseNode.StableDiffusionOutputType"
          },
          "default": "Image",
          "title": "Output Type",
          "description": "The type of output to generate."
        },
        {
          "name": "init_image",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Init Image",
          "description": "The initial image for Image-to-Image generation."
        },
        {
          "name": "strength",
          "type": {
            "type": "float"
          },
          "default": 0.8,
          "title": "Strength",
          "description": "Strength for Image-to-Image generation. Higher values allow for more deviation from the original image.",
          "min": 0.0,
          "max": 1.0
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "image"
        },
        {
          "type": {
            "type": "torch_tensor"
          },
          "name": "latent"
        }
      ],
      "recommended_models": [
        {
          "id": "h94/IP-Adapter:models/ip-adapter_sd15.bin",
          "type": "hf.ip_adapter",
          "name": "h94/IP-Adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "models/ip-adapter_sd15.bin",
          "size_on_disk": 44642825,
          "pipeline_tag": "text-to-image",
          "tags": [
            "diffusers",
            "safetensors",
            "text-to-image",
            "stable-diffusion",
            "en",
            "arxiv:2308.06721",
            "license:apache-2.0",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 1262
        },
        {
          "id": "h94/IP-Adapter:models/ip-adapter_sd15_light.bin",
          "type": "hf.ip_adapter",
          "name": "h94/IP-Adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "models/ip-adapter_sd15_light.bin",
          "size_on_disk": 44642819,
          "pipeline_tag": "text-to-image",
          "tags": [
            "diffusers",
            "safetensors",
            "text-to-image",
            "stable-diffusion",
            "en",
            "arxiv:2308.06721",
            "license:apache-2.0",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 1262
        },
        {
          "id": "h94/IP-Adapter:models/ip-adapter_sd15_vit-G.bin",
          "type": "hf.ip_adapter",
          "name": "h94/IP-Adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "models/ip-adapter_sd15_vit-G.bin",
          "size_on_disk": 46215689,
          "pipeline_tag": "text-to-image",
          "tags": [
            "diffusers",
            "safetensors",
            "text-to-image",
            "stable-diffusion",
            "en",
            "arxiv:2308.06721",
            "license:apache-2.0",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 1262
        },
        {
          "id": "Lykon/DreamShaper:DreamShaper_6.2_BakedVae_pruned.safetensors",
          "type": "hf.stable_diffusion",
          "name": "Lykon/DreamShaper",
          "repo_id": "Lykon/DreamShaper",
          "path": "DreamShaper_6.2_BakedVae_pruned.safetensors",
          "size_on_disk": 2132625894,
          "pipeline_tag": "text-to-image",
          "tags": [
            "diffusers",
            "stable-diffusion",
            "stable-diffusion-diffusers",
            "text-to-image",
            "art",
            "artistic",
            "anime",
            "en",
            "doi:10.57967/hf/0453",
            "license:other",
            "diffusers:StableDiffusionPipeline",
            "region:us"
          ],
          "has_model_index": true,
          "downloads": 86342,
          "likes": 996
        }
      ],
      "basic_fields": [
        "model",
        "prompt",
        "init_image",
        "strength"
      ]
    },
    {
      "title": "Stable Diffusion (Inpaint)",
      "description": "Performs inpainting on images using Stable Diffusion.\n    image, inpainting, SD\n\n    Use cases:\n    - Remove unwanted objects from images\n    - Fill in missing parts of images\n    - Modify specific areas of images while preserving the rest",
      "namespace": "huggingface.image_to_image",
      "node_type": "huggingface.image_to_image.StableDiffusionInpaint",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.stable_diffusion"
          },
          "default": {
            "type": "hf.stable_diffusion",
            "repo_id": "",
            "path": null,
            "variant": null,
            "allow_patterns": null,
            "ignore_patterns": null
          },
          "title": "Model",
          "description": "The model to use for image generation."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt for image generation."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to guide what should not appear in the generated image."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for the random number generator. Use -1 for a random seed.",
          "min": -1.0,
          "max": 4294967295.0
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 25,
          "title": "Num Inference Steps",
          "description": "Number of denoising steps.",
          "min": 1.0,
          "max": 100.0
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 7.5,
          "title": "Guidance Scale",
          "description": "Guidance scale for generation.",
          "min": 1.0,
          "max": 20.0
        },
        {
          "name": "scheduler",
          "type": {
            "type": "enum",
            "values": [
              "DPMSolverSDEScheduler",
              "EulerDiscreteScheduler",
              "LMSDiscreteScheduler",
              "DDIMScheduler",
              "DDPMScheduler",
              "HeunDiscreteScheduler",
              "DPMSolverMultistepScheduler",
              "DEISMultistepScheduler",
              "PNDMScheduler",
              "EulerAncestralDiscreteScheduler",
              "UniPCMultistepScheduler",
              "KDPM2DiscreteScheduler",
              "DPMSolverSinglestepScheduler",
              "KDPM2AncestralDiscreteScheduler"
            ],
            "type_name": "nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionBaseNode.StableDiffusionScheduler"
          },
          "default": "EulerDiscreteScheduler",
          "title": "Scheduler",
          "description": "The scheduler to use for the diffusion process."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "hf.lora_sd_config"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRA models to use for image processing"
        },
        {
          "name": "ip_adapter_model",
          "type": {
            "type": "hf.ip_adapter"
          },
          "default": {
            "type": "hf.ip_adapter",
            "repo_id": "",
            "path": null,
            "variant": null,
            "allow_patterns": null,
            "ignore_patterns": null
          },
          "title": "Ip Adapter Model",
          "description": "The IP adapter model to use for image processing"
        },
        {
          "name": "ip_adapter_image",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Ip Adapter Image",
          "description": "When provided the image will be fed into the IP adapter"
        },
        {
          "name": "ip_adapter_scale",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Ip Adapter Scale",
          "description": "The strength of the IP adapter",
          "min": 0.0,
          "max": 1.0
        },
        {
          "name": "latents",
          "type": {
            "type": "torch_tensor"
          },
          "default": {
            "type": "torch_tensor",
            "value": null,
            "dtype": "<i8",
            "shape": [
              1
            ]
          },
          "title": "Latents",
          "description": "Optional initial latents to start generation from."
        },
        {
          "name": "enable_attention_slicing",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Attention Slicing",
          "description": "Enable attention slicing for the pipeline. This can reduce VRAM usage."
        },
        {
          "name": "enable_tiling",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Tiling",
          "description": "Legacy VAE tiling flag (disabled in favor of PyTorch 2 attention optimizations)."
        },
        {
          "name": "enable_cpu_offload",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Cpu Offload",
          "description": "Enable CPU offload for the pipeline. This can reduce VRAM usage."
        },
        {
          "name": "output_type",
          "type": {
            "type": "enum",
            "values": [
              "Image",
              "Latent"
            ],
            "type_name": "nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionBaseNode.StableDiffusionOutputType"
          },
          "default": "Image",
          "title": "Output Type",
          "description": "The type of output to generate."
        },
        {
          "name": "init_image",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Init Image",
          "description": "The initial image to be inpainted."
        },
        {
          "name": "mask_image",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Mask Image",
          "description": "The mask image indicating areas to be inpainted."
        },
        {
          "name": "strength",
          "type": {
            "type": "float"
          },
          "default": 0.8,
          "title": "Strength",
          "description": "Strength for inpainting. Higher values allow for more deviation from the original image.",
          "min": 0.0,
          "max": 1.0
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "image"
        },
        {
          "type": {
            "type": "torch_tensor"
          },
          "name": "latent"
        }
      ],
      "recommended_models": [
        {
          "id": "h94/IP-Adapter:models/ip-adapter_sd15.bin",
          "type": "hf.ip_adapter",
          "name": "h94/IP-Adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "models/ip-adapter_sd15.bin",
          "size_on_disk": 44642825,
          "pipeline_tag": "text-to-image",
          "tags": [
            "diffusers",
            "safetensors",
            "text-to-image",
            "stable-diffusion",
            "en",
            "arxiv:2308.06721",
            "license:apache-2.0",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 1262
        },
        {
          "id": "h94/IP-Adapter:models/ip-adapter_sd15_light.bin",
          "type": "hf.ip_adapter",
          "name": "h94/IP-Adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "models/ip-adapter_sd15_light.bin",
          "size_on_disk": 44642819,
          "pipeline_tag": "text-to-image",
          "tags": [
            "diffusers",
            "safetensors",
            "text-to-image",
            "stable-diffusion",
            "en",
            "arxiv:2308.06721",
            "license:apache-2.0",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 1262
        },
        {
          "id": "h94/IP-Adapter:models/ip-adapter_sd15_vit-G.bin",
          "type": "hf.ip_adapter",
          "name": "h94/IP-Adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "models/ip-adapter_sd15_vit-G.bin",
          "size_on_disk": 46215689,
          "pipeline_tag": "text-to-image",
          "tags": [
            "diffusers",
            "safetensors",
            "text-to-image",
            "stable-diffusion",
            "en",
            "arxiv:2308.06721",
            "license:apache-2.0",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 1262
        },
        {
          "id": "Lykon/DreamShaper:DreamShaper_6.2_BakedVae_pruned.safetensors",
          "type": "hf.stable_diffusion",
          "name": "Lykon/DreamShaper",
          "repo_id": "Lykon/DreamShaper",
          "path": "DreamShaper_6.2_BakedVae_pruned.safetensors",
          "size_on_disk": 2132625894,
          "pipeline_tag": "text-to-image",
          "tags": [
            "diffusers",
            "stable-diffusion",
            "stable-diffusion-diffusers",
            "text-to-image",
            "art",
            "artistic",
            "anime",
            "en",
            "doi:10.57967/hf/0453",
            "license:other",
            "diffusers:StableDiffusionPipeline",
            "region:us"
          ],
          "has_model_index": true,
          "downloads": 86342,
          "likes": 996
        }
      ],
      "basic_fields": [
        "model",
        "prompt",
        "init_image",
        "mask_image",
        "strength"
      ]
    },
    {
      "title": "Stable Diffusion Latent Upscaler",
      "description": "Upscales Stable Diffusion latents (x2) using the SD Latent Upscaler pipeline.\n    tensor, upscaling, stable-diffusion, latent, SD\n\n    Input and output are tensors for chaining with latent-based workflows.",
      "namespace": "huggingface.image_to_image",
      "node_type": "huggingface.image_to_image.StableDiffusionLatentUpscaler",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt for upscaling guidance."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to guide what should not appear in the result."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 10,
          "title": "Num Inference Steps",
          "description": "Number of upscaling denoising steps.",
          "min": 1.0,
          "max": 100.0
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 0.0,
          "title": "Guidance Scale",
          "description": "Guidance scale for upscaling. 0 preserves content strongly.",
          "min": 0.0,
          "max": 20.0
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for the random number generator. Use -1 for a random seed.",
          "min": -1.0
        },
        {
          "name": "latents",
          "type": {
            "type": "torch_tensor"
          },
          "default": {
            "type": "torch_tensor",
            "value": null,
            "dtype": "<i8",
            "shape": [
              1
            ]
          },
          "title": "Latents",
          "description": "Low-resolution latents tensor to upscale."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "torch_tensor"
          },
          "name": "output"
        }
      ],
      "recommended_models": [
        {
          "id": "stabilityai/sd-x2-latent-upscaler",
          "type": "hf.image_to_image",
          "name": "stabilityai/sd-x2-latent-upscaler",
          "repo_id": "stabilityai/sd-x2-latent-upscaler",
          "size_on_disk": 4638341919,
          "tags": [
            "diffusers",
            "safetensors",
            "stable-diffusion",
            "license:openrail++",
            "diffusers:StableDiffusionLatentUpscalePipeline",
            "region:us"
          ],
          "has_model_index": true,
          "downloads": 12038,
          "likes": 188
        }
      ],
      "basic_fields": [
        "latents",
        "prompt",
        "negative_prompt",
        "num_inference_steps",
        "guidance_scale"
      ]
    },
    {
      "title": "Stable Diffusion 4x Upscale",
      "description": "Upscales an image using Stable Diffusion 4x upscaler.\n    image, upscaling, stable-diffusion, SD\n\n    Use cases:\n    - Enhance low-resolution images\n    - Improve image quality for printing or display\n    - Create high-resolution versions of small images",
      "namespace": "huggingface.image_to_image",
      "node_type": "huggingface.image_to_image.StableDiffusionUpscale",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt for image generation."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to guide what should not appear in the generated image."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 25,
          "title": "Num Inference Steps",
          "description": "Number of upscaling steps.",
          "min": 1.0,
          "max": 100.0
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 7.5,
          "title": "Guidance Scale",
          "description": "Guidance scale for generation.",
          "min": 1.0,
          "max": 20.0
        },
        {
          "name": "image",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image",
          "description": "The initial image for Image-to-Image generation."
        },
        {
          "name": "scheduler",
          "type": {
            "type": "enum",
            "values": [
              "DPMSolverSDEScheduler",
              "EulerDiscreteScheduler",
              "LMSDiscreteScheduler",
              "DDIMScheduler",
              "DDPMScheduler",
              "HeunDiscreteScheduler",
              "DPMSolverMultistepScheduler",
              "DEISMultistepScheduler",
              "PNDMScheduler",
              "EulerAncestralDiscreteScheduler",
              "UniPCMultistepScheduler",
              "KDPM2DiscreteScheduler",
              "DPMSolverSinglestepScheduler",
              "KDPM2AncestralDiscreteScheduler"
            ],
            "type_name": "nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionBaseNode.StableDiffusionScheduler"
          },
          "default": "HeunDiscreteScheduler",
          "title": "Scheduler",
          "description": "The scheduler to use for the diffusion process."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for the random number generator. Use -1 for a random seed.",
          "min": -1.0,
          "max": 4294967295.0
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "recommended_models": [
        {
          "id": "stabilityai/stable-diffusion-x4-upscaler",
          "type": "hf.image_to_image",
          "name": "stabilityai/stable-diffusion-x4-upscaler",
          "repo_id": "stabilityai/stable-diffusion-x4-upscaler",
          "allow_patterns": [
            "README.md",
            "**/*.fp16.safetensors",
            "**/*.json",
            "**/*.txt",
            "*.json"
          ],
          "size_on_disk": 1739977012,
          "tags": [
            "diffusers",
            "safetensors",
            "stable-diffusion",
            "arxiv:2112.10752",
            "arxiv:2202.00512",
            "arxiv:1910.09700",
            "license:openrail++",
            "diffusers:StableDiffusionUpscalePipeline",
            "region:us"
          ],
          "has_model_index": true,
          "downloads": 53192,
          "likes": 716
        }
      ],
      "basic_fields": [
        "prompt",
        "negative_prompt",
        "image"
      ]
    },
    {
      "title": "Stable Diffusion XL ControlNet",
      "description": "Generates images using Stable Diffusion XL with ControlNet guidance.\n    image, generation, text-to-image, controlnet, SDXL\n\n    Use cases:\n    - Generate images with precise control over composition and structure\n    - Create variations of existing images while maintaining specific features\n    - Artistic image generation with guided outputs",
      "namespace": "huggingface.image_to_image",
      "node_type": "huggingface.image_to_image.StableDiffusionXLControlNet",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.stable_diffusion_xl"
          },
          "default": {
            "type": "hf.stable_diffusion_xl",
            "repo_id": "",
            "path": null,
            "variant": null,
            "allow_patterns": null,
            "ignore_patterns": null
          },
          "title": "Model",
          "description": "The Stable Diffusion XL model to use for generation."
        },
        {
          "name": "quantization",
          "type": {
            "type": "enum",
            "values": [
              "fp16",
              "fp4",
              "int4"
            ],
            "type_name": "nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionXLQuantization"
          },
          "default": "fp16",
          "title": "Quantization",
          "description": "Quantization level for Stable Diffusion XL (enable INT4/FP4 to use a Nunchaku UNet)."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt for image generation."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to guide what should not appear in the generated image."
        },
        {
          "name": "width",
          "type": {
            "type": "int"
          },
          "default": 1024,
          "title": "Width",
          "description": "Width of the generated image.",
          "min": 64.0,
          "max": 2048.0
        },
        {
          "name": "height",
          "type": {
            "type": "int"
          },
          "default": 1024,
          "title": "Height",
          "description": "Height of the generated image",
          "min": 64.0,
          "max": 2048.0
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for the random number generator.",
          "min": -1.0,
          "max": 1000000.0
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 25,
          "title": "Num Inference Steps",
          "description": "Number of inference steps.",
          "min": 1.0,
          "max": 100.0
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 7.0,
          "title": "Guidance Scale",
          "description": "Guidance scale for generation.",
          "min": 0.0,
          "max": 20.0
        },
        {
          "name": "scheduler",
          "type": {
            "type": "enum",
            "values": [
              "DPMSolverSDEScheduler",
              "EulerDiscreteScheduler",
              "LMSDiscreteScheduler",
              "DDIMScheduler",
              "DDPMScheduler",
              "HeunDiscreteScheduler",
              "DPMSolverMultistepScheduler",
              "DEISMultistepScheduler",
              "PNDMScheduler",
              "EulerAncestralDiscreteScheduler",
              "UniPCMultistepScheduler",
              "KDPM2DiscreteScheduler",
              "DPMSolverSinglestepScheduler",
              "KDPM2AncestralDiscreteScheduler"
            ],
            "type_name": "nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionXLBase.StableDiffusionScheduler"
          },
          "default": "EulerDiscreteScheduler",
          "title": "Scheduler",
          "description": "The scheduler to use for the diffusion process."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "hf.lora_sdxl_config"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRA models to use for image processing"
        },
        {
          "name": "lora_scale",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Lora Scale",
          "description": "Strength of the LoRAs",
          "min": 0.0,
          "max": 1.0
        },
        {
          "name": "ip_adapter_model",
          "type": {
            "type": "hf.ip_adapter"
          },
          "default": {
            "type": "hf.ip_adapter",
            "repo_id": "",
            "path": null,
            "variant": null,
            "allow_patterns": null,
            "ignore_patterns": null
          },
          "title": "Ip Adapter Model",
          "description": "The IP adapter model to use for image processing"
        },
        {
          "name": "ip_adapter_image",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Ip Adapter Image",
          "description": "When provided the image will be fed into the IP adapter"
        },
        {
          "name": "ip_adapter_scale",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Ip Adapter Scale",
          "description": "Strength of the IP adapter image",
          "min": 0.0,
          "max": 1.0
        },
        {
          "name": "enable_attention_slicing",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Attention Slicing",
          "description": "Enable attention slicing for the pipeline. This can reduce VRAM usage but may slow down generation."
        },
        {
          "name": "enable_tiling",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Tiling",
          "description": "Legacy VAE tiling flag (disabled in favor of PyTorch 2 attention optimizations)."
        },
        {
          "name": "enable_cpu_offload",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Cpu Offload",
          "description": "Enable CPU offload for the pipeline. This can reduce VRAM usage."
        },
        {
          "name": "output_type",
          "type": {
            "type": "enum",
            "values": [
              "Image",
              "Latent"
            ],
            "type_name": "nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionXLBase.StableDiffusionOutputType"
          },
          "default": "Image",
          "title": "Output Type",
          "description": "The type of output to generate."
        },
        {
          "name": "controlnet",
          "type": {
            "type": "hf.controlnet"
          },
          "default": {
            "type": "hf.controlnet",
            "repo_id": "",
            "path": null,
            "variant": null,
            "allow_patterns": null,
            "ignore_patterns": null
          },
          "title": "Controlnet",
          "description": "The ControlNet model to use for guidance."
        },
        {
          "name": "control_image",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Control Image",
          "description": "The control image to guide the generation process."
        },
        {
          "name": "controlnet_conditioning_scale",
          "type": {
            "type": "float"
          },
          "default": 1.0,
          "title": "Controlnet Conditioning Scale",
          "description": "The scale for ControlNet conditioning.",
          "min": 0.0,
          "max": 2.0
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "image"
        },
        {
          "type": {
            "type": "torch_tensor"
          },
          "name": "latent"
        }
      ],
      "recommended_models": [
        {
          "id": "diffusers/controlnet-canny-sdxl-1.0:diffusion_pytorch_model.fp16.safetensors",
          "type": "hf.controlnet",
          "name": "diffusers/controlnet-canny-sdxl-1.0",
          "repo_id": "diffusers/controlnet-canny-sdxl-1.0",
          "path": "diffusion_pytorch_model.fp16.safetensors",
          "size_on_disk": 2502139136,
          "pipeline_tag": "text-to-image",
          "tags": [
            "diffusers",
            "safetensors",
            "stable-diffusion-xl",
            "stable-diffusion-xl-diffusers",
            "text-to-image",
            "base_model:runwayml/stable-diffusion-v1-5",
            "base_model:finetune:runwayml/stable-diffusion-v1-5",
            "license:openrail++",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 12070,
          "likes": 518
        },
        {
          "id": "diffusers/controlnet-depth-sdxl-1.0:diffusion_pytorch_model.fp16.safetensors",
          "type": "hf.controlnet",
          "name": "diffusers/controlnet-depth-sdxl-1.0",
          "repo_id": "diffusers/controlnet-depth-sdxl-1.0",
          "path": "diffusion_pytorch_model.fp16.safetensors",
          "size_on_disk": 2502139134,
          "pipeline_tag": "text-to-image",
          "tags": [
            "diffusers",
            "safetensors",
            "stable-diffusion-xl",
            "stable-diffusion-xl-diffusers",
            "text-to-image",
            "controlnet",
            "base_model:stabilityai/stable-diffusion-xl-base-1.0",
            "base_model:adapter:stabilityai/stable-diffusion-xl-base-1.0",
            "license:openrail++",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 15541,
          "likes": 190
        },
        {
          "id": "diffusers/controlnet-zoe-depth-sdxl-1.0:diffusion_pytorch_model.safetensors",
          "type": "hf.controlnet",
          "name": "diffusers/controlnet-zoe-depth-sdxl-1.0",
          "repo_id": "diffusers/controlnet-zoe-depth-sdxl-1.0",
          "path": "diffusion_pytorch_model.safetensors",
          "size_on_disk": 2502139134,
          "pipeline_tag": "text-to-image",
          "tags": [
            "diffusers",
            "safetensors",
            "stable-diffusion-xl",
            "stable-diffusion-xl-diffusers",
            "text-to-image",
            "controlnet",
            "base_model:stabilityai/stable-diffusion-xl-base-1.0",
            "base_model:adapter:stabilityai/stable-diffusion-xl-base-1.0",
            "license:openrail++",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 411,
          "likes": 39
        },
        {
          "id": "lllyasviel/sd_control_collection:diffusers_xl_canny_full.safetensors",
          "type": "hf.controlnet",
          "name": "lllyasviel/sd_control_collection",
          "repo_id": "lllyasviel/sd_control_collection",
          "path": "diffusers_xl_canny_full.safetensors",
          "size_on_disk": 2502139104,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 2074
        },
        {
          "id": "lllyasviel/sd_control_collection:diffusers_xl_canny_mid.safetensors",
          "type": "hf.controlnet",
          "name": "lllyasviel/sd_control_collection",
          "repo_id": "lllyasviel/sd_control_collection",
          "path": "diffusers_xl_canny_mid.safetensors",
          "size_on_disk": 545197704,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 2074
        },
        {
          "id": "lllyasviel/sd_control_collection:diffusers_xl_canny_small.safetensors",
          "type": "hf.controlnet",
          "name": "lllyasviel/sd_control_collection",
          "repo_id": "lllyasviel/sd_control_collection",
          "path": "diffusers_xl_canny_small.safetensors",
          "size_on_disk": 320237152,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 2074
        },
        {
          "id": "lllyasviel/sd_control_collection:diffusers_xl_depth_full.safetensors",
          "type": "hf.controlnet",
          "name": "lllyasviel/sd_control_collection",
          "repo_id": "lllyasviel/sd_control_collection",
          "path": "diffusers_xl_depth_full.safetensors",
          "size_on_disk": 2502139104,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 2074
        },
        {
          "id": "lllyasviel/sd_control_collection:diffusers_xl_depth_mid.safetensors",
          "type": "hf.controlnet",
          "name": "lllyasviel/sd_control_collection",
          "repo_id": "lllyasviel/sd_control_collection",
          "path": "diffusers_xl_depth_mid.safetensors",
          "size_on_disk": 545197704,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 2074
        },
        {
          "id": "lllyasviel/sd_control_collection:diffusers_xl_depth_small.safetensors",
          "type": "hf.controlnet",
          "name": "lllyasviel/sd_control_collection",
          "repo_id": "lllyasviel/sd_control_collection",
          "path": "diffusers_xl_depth_small.safetensors",
          "size_on_disk": 320237152,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 2074
        },
        {
          "id": "lllyasviel/sd_control_collection:t2i-adapter_diffusers_xl_canny.safetensors",
          "type": "hf.controlnet",
          "name": "lllyasviel/sd_control_collection",
          "repo_id": "lllyasviel/sd_control_collection",
          "path": "t2i-adapter_diffusers_xl_canny.safetensors",
          "size_on_disk": 158060416,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 2074
        },
        {
          "id": "lllyasviel/sd_control_collection:t2i-adapter_diffusers_xl_depth_midas.safetensors",
          "type": "hf.controlnet",
          "name": "lllyasviel/sd_control_collection",
          "repo_id": "lllyasviel/sd_control_collection",
          "path": "t2i-adapter_diffusers_xl_depth_midas.safetensors",
          "size_on_disk": 158060416,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 2074
        },
        {
          "id": "lllyasviel/sd_control_collection:t2i-adapter_diffusers_xl_depth_zoe.safetensors",
          "type": "hf.controlnet",
          "name": "lllyasviel/sd_control_collection",
          "repo_id": "lllyasviel/sd_control_collection",
          "path": "t2i-adapter_diffusers_xl_depth_zoe.safetensors",
          "size_on_disk": 158060416,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 2074
        },
        {
          "id": "lllyasviel/sd_control_collection:t2i-adapter_diffusers_xl_lineart.safetensors",
          "type": "hf.controlnet",
          "name": "lllyasviel/sd_control_collection",
          "repo_id": "lllyasviel/sd_control_collection",
          "path": "t2i-adapter_diffusers_xl_lineart.safetensors",
          "size_on_disk": 158060416,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 2074
        },
        {
          "id": "lllyasviel/sd_control_collection:t2i-adapter_diffusers_xl_openpose.safetensors",
          "type": "hf.controlnet",
          "name": "lllyasviel/sd_control_collection",
          "repo_id": "lllyasviel/sd_control_collection",
          "path": "t2i-adapter_diffusers_xl_openpose.safetensors",
          "size_on_disk": 158060416,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 2074
        },
        {
          "id": "lllyasviel/sd_control_collection:t2i-adapter_diffusers_xl_sketch.safetensors",
          "type": "hf.controlnet",
          "name": "lllyasviel/sd_control_collection",
          "repo_id": "lllyasviel/sd_control_collection",
          "path": "t2i-adapter_diffusers_xl_sketch.safetensors",
          "size_on_disk": 158060416,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 2074
        },
        {
          "id": "lllyasviel/sd_control_collection:t2i-adapter_xl_canny.safetensors",
          "type": "hf.controlnet",
          "name": "lllyasviel/sd_control_collection",
          "repo_id": "lllyasviel/sd_control_collection",
          "path": "t2i-adapter_xl_canny.safetensors",
          "size_on_disk": 155110672,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 2074
        },
        {
          "id": "lllyasviel/sd_control_collection:t2i-adapter_xl_openpose.safetensors",
          "type": "hf.controlnet",
          "name": "lllyasviel/sd_control_collection",
          "repo_id": "lllyasviel/sd_control_collection",
          "path": "t2i-adapter_xl_openpose.safetensors",
          "size_on_disk": 158059792,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 2074
        },
        {
          "id": "lllyasviel/sd_control_collection:t2i-adapter_xl_sketch.safetensors",
          "type": "hf.controlnet",
          "name": "lllyasviel/sd_control_collection",
          "repo_id": "lllyasviel/sd_control_collection",
          "path": "t2i-adapter_xl_sketch.safetensors",
          "size_on_disk": 155110672,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 2074
        },
        {
          "id": "lllyasviel/sd_control_collection:thibaud_xl_openpose.safetensors",
          "type": "hf.controlnet",
          "name": "lllyasviel/sd_control_collection",
          "repo_id": "lllyasviel/sd_control_collection",
          "path": "thibaud_xl_openpose.safetensors",
          "size_on_disk": 2502139104,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 2074
        },
        {
          "id": "lllyasviel/sd_control_collection:thibaud_xl_openpose_256lora.safetensors",
          "type": "hf.controlnet",
          "name": "lllyasviel/sd_control_collection",
          "repo_id": "lllyasviel/sd_control_collection",
          "path": "thibaud_xl_openpose_256lora.safetensors",
          "size_on_disk": 774423040,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 2074
        },
        {
          "id": "lllyasviel/sd_control_collection:sargezt_xl_depth_faid_vidit.safetensors",
          "type": "hf.controlnet",
          "name": "lllyasviel/sd_control_collection",
          "repo_id": "lllyasviel/sd_control_collection",
          "path": "sargezt_xl_depth_faid_vidit.safetensors",
          "size_on_disk": 2502139104,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 2074
        },
        {
          "id": "lllyasviel/sd_control_collection:sargezt_xl_depth_zeed.safetensors",
          "type": "hf.controlnet",
          "name": "lllyasviel/sd_control_collection",
          "repo_id": "lllyasviel/sd_control_collection",
          "path": "sargezt_xl_depth_zeed.safetensors",
          "size_on_disk": 2502139104,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 2074
        },
        {
          "id": "lllyasviel/sd_control_collection:sargezt_xl_depth.safetensors",
          "type": "hf.controlnet",
          "name": "lllyasviel/sd_control_collection",
          "repo_id": "lllyasviel/sd_control_collection",
          "path": "sargezt_xl_depth.safetensors",
          "size_on_disk": 2502139104,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 2074
        },
        {
          "id": "lllyasviel/sd_control_collection:sargezt_xl_softedge.safetensors",
          "type": "hf.controlnet",
          "name": "lllyasviel/sd_control_collection",
          "repo_id": "lllyasviel/sd_control_collection",
          "path": "sargezt_xl_softedge.safetensors",
          "size_on_disk": 2502139104,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 2074
        },
        {
          "id": "lllyasviel/sd_control_collection:ip-adapter_xl.pth",
          "type": "hf.controlnet",
          "name": "lllyasviel/sd_control_collection",
          "repo_id": "lllyasviel/sd_control_collection",
          "path": "ip-adapter_xl.pth",
          "size_on_disk": 702585021,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 2074
        },
        {
          "id": "lllyasviel/sd_control_collection:kohya_controllllite_xl_depth_anime.safetensors",
          "type": "hf.controlnet",
          "name": "lllyasviel/sd_control_collection",
          "repo_id": "lllyasviel/sd_control_collection",
          "path": "kohya_controllllite_xl_depth_anime.safetensors",
          "size_on_disk": 11321048,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 2074
        },
        {
          "id": "lllyasviel/sd_control_collection:kohya_controllllite_xl_canny_anime.safetensors",
          "type": "hf.controlnet",
          "name": "lllyasviel/sd_control_collection",
          "repo_id": "lllyasviel/sd_control_collection",
          "path": "kohya_controllllite_xl_canny_anime.safetensors",
          "size_on_disk": 46172168,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 2074
        },
        {
          "id": "lllyasviel/sd_control_collection:kohya_controllllite_xl_scribble_anime.safetensors",
          "type": "hf.controlnet",
          "name": "lllyasviel/sd_control_collection",
          "repo_id": "lllyasviel/sd_control_collection",
          "path": "kohya_controllllite_xl_scribble_anime.safetensors",
          "size_on_disk": 46172168,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 2074
        },
        {
          "id": "lllyasviel/sd_control_collection:kohya_controllllite_xl_openpose_anime.safetensors",
          "type": "hf.controlnet",
          "name": "lllyasviel/sd_control_collection",
          "repo_id": "lllyasviel/sd_control_collection",
          "path": "kohya_controllllite_xl_openpose_anime.safetensors",
          "size_on_disk": 46172168,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 2074
        },
        {
          "id": "lllyasviel/sd_control_collection:kohya_controllllite_xl_openpose_anime_v2.safetensors",
          "type": "hf.controlnet",
          "name": "lllyasviel/sd_control_collection",
          "repo_id": "lllyasviel/sd_control_collection",
          "path": "kohya_controllllite_xl_openpose_anime_v2.safetensors",
          "size_on_disk": 46172168,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 2074
        },
        {
          "id": "lllyasviel/sd_control_collection:kohya_controllllite_xl_blur_anime_beta.safetensors",
          "type": "hf.controlnet",
          "name": "lllyasviel/sd_control_collection",
          "repo_id": "lllyasviel/sd_control_collection",
          "path": "kohya_controllllite_xl_blur_anime_beta.safetensors",
          "size_on_disk": 22512816,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 2074
        },
        {
          "id": "lllyasviel/sd_control_collection:kohya_controllllite_xl_blur.safetensors",
          "type": "hf.controlnet",
          "name": "lllyasviel/sd_control_collection",
          "repo_id": "lllyasviel/sd_control_collection",
          "path": "kohya_controllllite_xl_blur.safetensors",
          "size_on_disk": 46172168,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 2074
        },
        {
          "id": "lllyasviel/sd_control_collection:kohya_controllllite_xl_blur_anime.safetensors",
          "type": "hf.controlnet",
          "name": "lllyasviel/sd_control_collection",
          "repo_id": "lllyasviel/sd_control_collection",
          "path": "kohya_controllllite_xl_blur_anime.safetensors",
          "size_on_disk": 46172168,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 2074
        },
        {
          "id": "lllyasviel/sd_control_collection:kohya_controllllite_xl_canny.safetensors",
          "type": "hf.controlnet",
          "name": "lllyasviel/sd_control_collection",
          "repo_id": "lllyasviel/sd_control_collection",
          "path": "kohya_controllllite_xl_canny.safetensors",
          "size_on_disk": 46172168,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 2074
        },
        {
          "id": "lllyasviel/sd_control_collection:kohya_controllllite_xl_depth.safetensors",
          "type": "hf.controlnet",
          "name": "lllyasviel/sd_control_collection",
          "repo_id": "lllyasviel/sd_control_collection",
          "path": "kohya_controllllite_xl_depth.safetensors",
          "size_on_disk": 46172168,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 2074
        },
        {
          "id": "bdsqlsz/qinglong_controlnet-lllite:bdsqlsz_controlllite_xl_canny.safetensors",
          "type": "hf.controlnet",
          "name": "bdsqlsz/qinglong_controlnet-lllite",
          "repo_id": "bdsqlsz/qinglong_controlnet-lllite",
          "path": "bdsqlsz_controlllite_xl_canny.safetensors",
          "size_on_disk": 223906688,
          "tags": [
            "diffusers",
            "onnx",
            "license:cc-by-nc-sa-4.0",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 11923,
          "likes": 340
        },
        {
          "id": "bdsqlsz/qinglong_controlnet-lllite:bdsqlsz_controlllite_xl_depth.safetensors",
          "type": "hf.controlnet",
          "name": "bdsqlsz/qinglong_controlnet-lllite",
          "repo_id": "bdsqlsz/qinglong_controlnet-lllite",
          "path": "bdsqlsz_controlllite_xl_depth.safetensors",
          "size_on_disk": 223906688,
          "tags": [
            "diffusers",
            "onnx",
            "license:cc-by-nc-sa-4.0",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 11923,
          "likes": 340
        },
        {
          "id": "bdsqlsz/qinglong_controlnet-lllite:bdsqlsz_controlllite_xl_depth_V2.safetensors",
          "type": "hf.controlnet",
          "name": "bdsqlsz/qinglong_controlnet-lllite",
          "repo_id": "bdsqlsz/qinglong_controlnet-lllite",
          "path": "bdsqlsz_controlllite_xl_depth_V2.safetensors",
          "size_on_disk": 223905112,
          "tags": [
            "diffusers",
            "onnx",
            "license:cc-by-nc-sa-4.0",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 11923,
          "likes": 340
        },
        {
          "id": "bdsqlsz/qinglong_controlnet-lllite:bdsqlsz_controlllite_xl_dw_openpose.safetensors",
          "type": "hf.controlnet",
          "name": "bdsqlsz/qinglong_controlnet-lllite",
          "repo_id": "bdsqlsz/qinglong_controlnet-lllite",
          "path": "bdsqlsz_controlllite_xl_dw_openpose.safetensors",
          "size_on_disk": 223906688,
          "tags": [
            "diffusers",
            "onnx",
            "license:cc-by-nc-sa-4.0",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 11923,
          "likes": 340
        },
        {
          "id": "bdsqlsz/qinglong_controlnet-lllite:bdsqlsz_controlllite_xl_lineart_anime_denoise.safetensors",
          "type": "hf.controlnet",
          "name": "bdsqlsz/qinglong_controlnet-lllite",
          "repo_id": "bdsqlsz/qinglong_controlnet-lllite",
          "path": "bdsqlsz_controlllite_xl_lineart_anime_denoise.safetensors",
          "size_on_disk": 223906688,
          "tags": [
            "diffusers",
            "onnx",
            "license:cc-by-nc-sa-4.0",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 11923,
          "likes": 340
        },
        {
          "id": "bdsqlsz/qinglong_controlnet-lllite:bdsqlsz_controlllite_xl_mlsd_V2.safetensors",
          "type": "hf.controlnet",
          "name": "bdsqlsz/qinglong_controlnet-lllite",
          "repo_id": "bdsqlsz/qinglong_controlnet-lllite",
          "path": "bdsqlsz_controlllite_xl_mlsd_V2.safetensors",
          "size_on_disk": 223906688,
          "tags": [
            "diffusers",
            "onnx",
            "license:cc-by-nc-sa-4.0",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 11923,
          "likes": 340
        },
        {
          "id": "bdsqlsz/qinglong_controlnet-lllite:bdsqlsz_controlllite_xl_normal.safetensors",
          "type": "hf.controlnet",
          "name": "bdsqlsz/qinglong_controlnet-lllite",
          "repo_id": "bdsqlsz/qinglong_controlnet-lllite",
          "path": "bdsqlsz_controlllite_xl_normal.safetensors",
          "size_on_disk": 223906688,
          "tags": [
            "diffusers",
            "onnx",
            "license:cc-by-nc-sa-4.0",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 11923,
          "likes": 340
        },
        {
          "id": "bdsqlsz/qinglong_controlnet-lllite:bdsqlsz_controlllite_xl_normal_dsine.safetensors",
          "type": "hf.controlnet",
          "name": "bdsqlsz/qinglong_controlnet-lllite",
          "repo_id": "bdsqlsz/qinglong_controlnet-lllite",
          "path": "bdsqlsz_controlllite_xl_normal_dsine.safetensors",
          "size_on_disk": 223905104,
          "tags": [
            "diffusers",
            "onnx",
            "license:cc-by-nc-sa-4.0",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 11923,
          "likes": 340
        },
        {
          "id": "bdsqlsz/qinglong_controlnet-lllite:bdsqlsz_controlllite_xl_recolor_luminance.safetensors",
          "type": "hf.controlnet",
          "name": "bdsqlsz/qinglong_controlnet-lllite",
          "repo_id": "bdsqlsz/qinglong_controlnet-lllite",
          "path": "bdsqlsz_controlllite_xl_recolor_luminance.safetensors",
          "size_on_disk": 223906688,
          "tags": [
            "diffusers",
            "onnx",
            "license:cc-by-nc-sa-4.0",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 11923,
          "likes": 340
        },
        {
          "id": "bdsqlsz/qinglong_controlnet-lllite:bdsqlsz_controlllite_xl_segment_animeface.safetensors",
          "type": "hf.controlnet",
          "name": "bdsqlsz/qinglong_controlnet-lllite",
          "repo_id": "bdsqlsz/qinglong_controlnet-lllite",
          "path": "bdsqlsz_controlllite_xl_segment_animeface.safetensors",
          "size_on_disk": 223906696,
          "tags": [
            "diffusers",
            "onnx",
            "license:cc-by-nc-sa-4.0",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 11923,
          "likes": 340
        },
        {
          "id": "bdsqlsz/qinglong_controlnet-lllite:bdsqlsz_controlllite_xl_segment_animeface_V2.safetensors",
          "type": "hf.controlnet",
          "name": "bdsqlsz/qinglong_controlnet-lllite",
          "repo_id": "bdsqlsz/qinglong_controlnet-lllite",
          "path": "bdsqlsz_controlllite_xl_segment_animeface_V2.safetensors",
          "size_on_disk": 223906688,
          "tags": [
            "diffusers",
            "onnx",
            "license:cc-by-nc-sa-4.0",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 11923,
          "likes": 340
        },
        {
          "id": "bdsqlsz/qinglong_controlnet-lllite:bdsqlsz_controlllite_xl_sketch.safetensors",
          "type": "hf.controlnet",
          "name": "bdsqlsz/qinglong_controlnet-lllite",
          "repo_id": "bdsqlsz/qinglong_controlnet-lllite",
          "path": "bdsqlsz_controlllite_xl_sketch.safetensors",
          "size_on_disk": 223906688,
          "tags": [
            "diffusers",
            "onnx",
            "license:cc-by-nc-sa-4.0",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 11923,
          "likes": 340
        },
        {
          "id": "bdsqlsz/qinglong_controlnet-lllite:bdsqlsz_controlllite_xl_softedge.safetensors",
          "type": "hf.controlnet",
          "name": "bdsqlsz/qinglong_controlnet-lllite",
          "repo_id": "bdsqlsz/qinglong_controlnet-lllite",
          "path": "bdsqlsz_controlllite_xl_softedge.safetensors",
          "size_on_disk": 223906688,
          "tags": [
            "diffusers",
            "onnx",
            "license:cc-by-nc-sa-4.0",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 11923,
          "likes": 340
        },
        {
          "id": "bdsqlsz/qinglong_controlnet-lllite:bdsqlsz_controlllite_xl_t2i-adapter_color_shuffle.safetensors",
          "type": "hf.controlnet",
          "name": "bdsqlsz/qinglong_controlnet-lllite",
          "repo_id": "bdsqlsz/qinglong_controlnet-lllite",
          "path": "bdsqlsz_controlllite_xl_t2i-adapter_color_shuffle.safetensors",
          "size_on_disk": 223906688,
          "tags": [
            "diffusers",
            "onnx",
            "license:cc-by-nc-sa-4.0",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 11923,
          "likes": 340
        },
        {
          "id": "bdsqlsz/qinglong_controlnet-lllite:bdsqlsz_controlllite_xl_tile_anime_alpha.safetensors",
          "type": "hf.controlnet",
          "name": "bdsqlsz/qinglong_controlnet-lllite",
          "repo_id": "bdsqlsz/qinglong_controlnet-lllite",
          "path": "bdsqlsz_controlllite_xl_tile_anime_alpha.safetensors",
          "size_on_disk": 223906688,
          "tags": [
            "diffusers",
            "onnx",
            "license:cc-by-nc-sa-4.0",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 11923,
          "likes": 340
        },
        {
          "id": "bdsqlsz/qinglong_controlnet-lllite:bdsqlsz_controlllite_xl_tile_anime_beta.safetensors",
          "type": "hf.controlnet",
          "name": "bdsqlsz/qinglong_controlnet-lllite",
          "repo_id": "bdsqlsz/qinglong_controlnet-lllite",
          "path": "bdsqlsz_controlllite_xl_tile_anime_beta.safetensors",
          "size_on_disk": 223906688,
          "tags": [
            "diffusers",
            "onnx",
            "license:cc-by-nc-sa-4.0",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 11923,
          "likes": 340
        },
        {
          "id": "bdsqlsz/qinglong_controlnet-lllite:bdsqlsz_controlllite_xl_tile_realistic.safetensors",
          "type": "hf.controlnet",
          "name": "bdsqlsz/qinglong_controlnet-lllite",
          "repo_id": "bdsqlsz/qinglong_controlnet-lllite",
          "path": "bdsqlsz_controlllite_xl_tile_realistic.safetensors",
          "size_on_disk": 317085856,
          "tags": [
            "diffusers",
            "onnx",
            "license:cc-by-nc-sa-4.0",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 11923,
          "likes": 340
        }
      ],
      "basic_fields": [
        "model",
        "quantization",
        "prompt",
        "width",
        "height",
        "controlnet",
        "control_image",
        "controlnet_conditioning_scale"
      ]
    },
    {
      "title": "Stable Diffusion XL ControlNet (Img2Img)",
      "description": "Transforms existing images using Stable Diffusion XL with ControlNet guidance.\n    image, generation, image-to-image, controlnet, SDXL\n\n    Use cases:\n    - Modify existing images with precise control over composition and structure\n    - Apply specific styles or concepts to photographs or artwork with guided transformations\n    - Create variations of existing visual content while maintaining certain features\n    - Enhance image editing capabilities with AI-guided transformations",
      "namespace": "huggingface.image_to_image",
      "node_type": "huggingface.image_to_image.StableDiffusionXLControlNetImg2Img",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.stable_diffusion_xl"
          },
          "default": {
            "type": "hf.stable_diffusion_xl",
            "repo_id": "",
            "path": null,
            "variant": null,
            "allow_patterns": null,
            "ignore_patterns": null
          },
          "title": "Model",
          "description": "The Stable Diffusion XL model to use for generation."
        },
        {
          "name": "quantization",
          "type": {
            "type": "enum",
            "values": [
              "fp16",
              "fp4",
              "int4"
            ],
            "type_name": "nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionXLQuantization"
          },
          "default": "fp16",
          "title": "Quantization",
          "description": "Quantization level for Stable Diffusion XL (enable INT4/FP4 to use a Nunchaku UNet)."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt for image generation."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to guide what should not appear in the generated image."
        },
        {
          "name": "width",
          "type": {
            "type": "int"
          },
          "default": 1024,
          "title": "Width",
          "description": "Width of the generated image.",
          "min": 64.0,
          "max": 2048.0
        },
        {
          "name": "height",
          "type": {
            "type": "int"
          },
          "default": 1024,
          "title": "Height",
          "description": "Height of the generated image",
          "min": 64.0,
          "max": 2048.0
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for the random number generator.",
          "min": -1.0,
          "max": 1000000.0
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 25,
          "title": "Num Inference Steps",
          "description": "Number of inference steps.",
          "min": 1.0,
          "max": 100.0
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 7.0,
          "title": "Guidance Scale",
          "description": "Guidance scale for generation.",
          "min": 0.0,
          "max": 20.0
        },
        {
          "name": "scheduler",
          "type": {
            "type": "enum",
            "values": [
              "DPMSolverSDEScheduler",
              "EulerDiscreteScheduler",
              "LMSDiscreteScheduler",
              "DDIMScheduler",
              "DDPMScheduler",
              "HeunDiscreteScheduler",
              "DPMSolverMultistepScheduler",
              "DEISMultistepScheduler",
              "PNDMScheduler",
              "EulerAncestralDiscreteScheduler",
              "UniPCMultistepScheduler",
              "KDPM2DiscreteScheduler",
              "DPMSolverSinglestepScheduler",
              "KDPM2AncestralDiscreteScheduler"
            ],
            "type_name": "nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionXLBase.StableDiffusionScheduler"
          },
          "default": "EulerDiscreteScheduler",
          "title": "Scheduler",
          "description": "The scheduler to use for the diffusion process."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "hf.lora_sdxl_config"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRA models to use for image processing"
        },
        {
          "name": "lora_scale",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Lora Scale",
          "description": "Strength of the LoRAs",
          "min": 0.0,
          "max": 1.0
        },
        {
          "name": "ip_adapter_model",
          "type": {
            "type": "hf.ip_adapter"
          },
          "default": {
            "type": "hf.ip_adapter",
            "repo_id": "",
            "path": null,
            "variant": null,
            "allow_patterns": null,
            "ignore_patterns": null
          },
          "title": "Ip Adapter Model",
          "description": "The IP adapter model to use for image processing"
        },
        {
          "name": "ip_adapter_image",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Ip Adapter Image",
          "description": "When provided the image will be fed into the IP adapter"
        },
        {
          "name": "ip_adapter_scale",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Ip Adapter Scale",
          "description": "Strength of the IP adapter image",
          "min": 0.0,
          "max": 1.0
        },
        {
          "name": "enable_attention_slicing",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Attention Slicing",
          "description": "Enable attention slicing for the pipeline. This can reduce VRAM usage but may slow down generation."
        },
        {
          "name": "enable_tiling",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Tiling",
          "description": "Legacy VAE tiling flag (disabled in favor of PyTorch 2 attention optimizations)."
        },
        {
          "name": "enable_cpu_offload",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Cpu Offload",
          "description": "Enable CPU offload for the pipeline. This can reduce VRAM usage."
        },
        {
          "name": "output_type",
          "type": {
            "type": "enum",
            "values": [
              "Image",
              "Latent"
            ],
            "type_name": "nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionXLBase.StableDiffusionOutputType"
          },
          "default": "Image",
          "title": "Output Type",
          "description": "The type of output to generate."
        },
        {
          "name": "init_image",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Init Image",
          "description": "The initial image for Image-to-Image generation."
        },
        {
          "name": "strength",
          "type": {
            "type": "float"
          },
          "default": 0.8,
          "title": "Strength",
          "description": "Strength for Image-to-Image generation. Higher values allow for more deviation from the original image.",
          "min": 0.0,
          "max": 1.0
        },
        {
          "name": "controlnet",
          "type": {
            "type": "hf.controlnet"
          },
          "default": {
            "type": "hf.controlnet",
            "repo_id": "",
            "path": null,
            "variant": null,
            "allow_patterns": null,
            "ignore_patterns": null
          },
          "title": "Controlnet",
          "description": "The ControlNet model to use for guidance."
        },
        {
          "name": "control_image",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Control Image",
          "description": "The control image to guide the transformation."
        },
        {
          "name": "controlnet_conditioning_scale",
          "type": {
            "type": "float"
          },
          "default": 1.0,
          "title": "Controlnet Conditioning Scale",
          "description": "The scale for ControlNet conditioning.",
          "min": 0.0,
          "max": 2.0
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "image"
        },
        {
          "type": {
            "type": "torch_tensor"
          },
          "name": "latent"
        }
      ],
      "recommended_models": [
        {
          "id": "diffusers/controlnet-canny-sdxl-1.0:diffusion_pytorch_model.fp16.safetensors",
          "type": "hf.controlnet",
          "name": "diffusers/controlnet-canny-sdxl-1.0",
          "repo_id": "diffusers/controlnet-canny-sdxl-1.0",
          "path": "diffusion_pytorch_model.fp16.safetensors",
          "size_on_disk": 2502139136,
          "pipeline_tag": "text-to-image",
          "tags": [
            "diffusers",
            "safetensors",
            "stable-diffusion-xl",
            "stable-diffusion-xl-diffusers",
            "text-to-image",
            "base_model:runwayml/stable-diffusion-v1-5",
            "base_model:finetune:runwayml/stable-diffusion-v1-5",
            "license:openrail++",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 12070,
          "likes": 518
        },
        {
          "id": "diffusers/controlnet-depth-sdxl-1.0:diffusion_pytorch_model.fp16.safetensors",
          "type": "hf.controlnet",
          "name": "diffusers/controlnet-depth-sdxl-1.0",
          "repo_id": "diffusers/controlnet-depth-sdxl-1.0",
          "path": "diffusion_pytorch_model.fp16.safetensors",
          "size_on_disk": 2502139134,
          "pipeline_tag": "text-to-image",
          "tags": [
            "diffusers",
            "safetensors",
            "stable-diffusion-xl",
            "stable-diffusion-xl-diffusers",
            "text-to-image",
            "controlnet",
            "base_model:stabilityai/stable-diffusion-xl-base-1.0",
            "base_model:adapter:stabilityai/stable-diffusion-xl-base-1.0",
            "license:openrail++",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 15541,
          "likes": 190
        },
        {
          "id": "diffusers/controlnet-zoe-depth-sdxl-1.0:diffusion_pytorch_model.safetensors",
          "type": "hf.controlnet",
          "name": "diffusers/controlnet-zoe-depth-sdxl-1.0",
          "repo_id": "diffusers/controlnet-zoe-depth-sdxl-1.0",
          "path": "diffusion_pytorch_model.safetensors",
          "size_on_disk": 2502139134,
          "pipeline_tag": "text-to-image",
          "tags": [
            "diffusers",
            "safetensors",
            "stable-diffusion-xl",
            "stable-diffusion-xl-diffusers",
            "text-to-image",
            "controlnet",
            "base_model:stabilityai/stable-diffusion-xl-base-1.0",
            "base_model:adapter:stabilityai/stable-diffusion-xl-base-1.0",
            "license:openrail++",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 411,
          "likes": 39
        },
        {
          "id": "lllyasviel/sd_control_collection:diffusers_xl_canny_full.safetensors",
          "type": "hf.controlnet",
          "name": "lllyasviel/sd_control_collection",
          "repo_id": "lllyasviel/sd_control_collection",
          "path": "diffusers_xl_canny_full.safetensors",
          "size_on_disk": 2502139104,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 2074
        },
        {
          "id": "lllyasviel/sd_control_collection:diffusers_xl_canny_mid.safetensors",
          "type": "hf.controlnet",
          "name": "lllyasviel/sd_control_collection",
          "repo_id": "lllyasviel/sd_control_collection",
          "path": "diffusers_xl_canny_mid.safetensors",
          "size_on_disk": 545197704,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 2074
        },
        {
          "id": "lllyasviel/sd_control_collection:diffusers_xl_canny_small.safetensors",
          "type": "hf.controlnet",
          "name": "lllyasviel/sd_control_collection",
          "repo_id": "lllyasviel/sd_control_collection",
          "path": "diffusers_xl_canny_small.safetensors",
          "size_on_disk": 320237152,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 2074
        },
        {
          "id": "lllyasviel/sd_control_collection:diffusers_xl_depth_full.safetensors",
          "type": "hf.controlnet",
          "name": "lllyasviel/sd_control_collection",
          "repo_id": "lllyasviel/sd_control_collection",
          "path": "diffusers_xl_depth_full.safetensors",
          "size_on_disk": 2502139104,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 2074
        },
        {
          "id": "lllyasviel/sd_control_collection:diffusers_xl_depth_mid.safetensors",
          "type": "hf.controlnet",
          "name": "lllyasviel/sd_control_collection",
          "repo_id": "lllyasviel/sd_control_collection",
          "path": "diffusers_xl_depth_mid.safetensors",
          "size_on_disk": 545197704,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 2074
        },
        {
          "id": "lllyasviel/sd_control_collection:diffusers_xl_depth_small.safetensors",
          "type": "hf.controlnet",
          "name": "lllyasviel/sd_control_collection",
          "repo_id": "lllyasviel/sd_control_collection",
          "path": "diffusers_xl_depth_small.safetensors",
          "size_on_disk": 320237152,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 2074
        },
        {
          "id": "lllyasviel/sd_control_collection:t2i-adapter_diffusers_xl_canny.safetensors",
          "type": "hf.controlnet",
          "name": "lllyasviel/sd_control_collection",
          "repo_id": "lllyasviel/sd_control_collection",
          "path": "t2i-adapter_diffusers_xl_canny.safetensors",
          "size_on_disk": 158060416,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 2074
        },
        {
          "id": "lllyasviel/sd_control_collection:t2i-adapter_diffusers_xl_depth_midas.safetensors",
          "type": "hf.controlnet",
          "name": "lllyasviel/sd_control_collection",
          "repo_id": "lllyasviel/sd_control_collection",
          "path": "t2i-adapter_diffusers_xl_depth_midas.safetensors",
          "size_on_disk": 158060416,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 2074
        },
        {
          "id": "lllyasviel/sd_control_collection:t2i-adapter_diffusers_xl_depth_zoe.safetensors",
          "type": "hf.controlnet",
          "name": "lllyasviel/sd_control_collection",
          "repo_id": "lllyasviel/sd_control_collection",
          "path": "t2i-adapter_diffusers_xl_depth_zoe.safetensors",
          "size_on_disk": 158060416,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 2074
        },
        {
          "id": "lllyasviel/sd_control_collection:t2i-adapter_diffusers_xl_lineart.safetensors",
          "type": "hf.controlnet",
          "name": "lllyasviel/sd_control_collection",
          "repo_id": "lllyasviel/sd_control_collection",
          "path": "t2i-adapter_diffusers_xl_lineart.safetensors",
          "size_on_disk": 158060416,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 2074
        },
        {
          "id": "lllyasviel/sd_control_collection:t2i-adapter_diffusers_xl_openpose.safetensors",
          "type": "hf.controlnet",
          "name": "lllyasviel/sd_control_collection",
          "repo_id": "lllyasviel/sd_control_collection",
          "path": "t2i-adapter_diffusers_xl_openpose.safetensors",
          "size_on_disk": 158060416,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 2074
        },
        {
          "id": "lllyasviel/sd_control_collection:t2i-adapter_diffusers_xl_sketch.safetensors",
          "type": "hf.controlnet",
          "name": "lllyasviel/sd_control_collection",
          "repo_id": "lllyasviel/sd_control_collection",
          "path": "t2i-adapter_diffusers_xl_sketch.safetensors",
          "size_on_disk": 158060416,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 2074
        },
        {
          "id": "lllyasviel/sd_control_collection:t2i-adapter_xl_canny.safetensors",
          "type": "hf.controlnet",
          "name": "lllyasviel/sd_control_collection",
          "repo_id": "lllyasviel/sd_control_collection",
          "path": "t2i-adapter_xl_canny.safetensors",
          "size_on_disk": 155110672,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 2074
        },
        {
          "id": "lllyasviel/sd_control_collection:t2i-adapter_xl_openpose.safetensors",
          "type": "hf.controlnet",
          "name": "lllyasviel/sd_control_collection",
          "repo_id": "lllyasviel/sd_control_collection",
          "path": "t2i-adapter_xl_openpose.safetensors",
          "size_on_disk": 158059792,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 2074
        },
        {
          "id": "lllyasviel/sd_control_collection:t2i-adapter_xl_sketch.safetensors",
          "type": "hf.controlnet",
          "name": "lllyasviel/sd_control_collection",
          "repo_id": "lllyasviel/sd_control_collection",
          "path": "t2i-adapter_xl_sketch.safetensors",
          "size_on_disk": 155110672,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 2074
        },
        {
          "id": "lllyasviel/sd_control_collection:thibaud_xl_openpose.safetensors",
          "type": "hf.controlnet",
          "name": "lllyasviel/sd_control_collection",
          "repo_id": "lllyasviel/sd_control_collection",
          "path": "thibaud_xl_openpose.safetensors",
          "size_on_disk": 2502139104,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 2074
        },
        {
          "id": "lllyasviel/sd_control_collection:thibaud_xl_openpose_256lora.safetensors",
          "type": "hf.controlnet",
          "name": "lllyasviel/sd_control_collection",
          "repo_id": "lllyasviel/sd_control_collection",
          "path": "thibaud_xl_openpose_256lora.safetensors",
          "size_on_disk": 774423040,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 2074
        },
        {
          "id": "lllyasviel/sd_control_collection:sargezt_xl_depth_faid_vidit.safetensors",
          "type": "hf.controlnet",
          "name": "lllyasviel/sd_control_collection",
          "repo_id": "lllyasviel/sd_control_collection",
          "path": "sargezt_xl_depth_faid_vidit.safetensors",
          "size_on_disk": 2502139104,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 2074
        },
        {
          "id": "lllyasviel/sd_control_collection:sargezt_xl_depth_zeed.safetensors",
          "type": "hf.controlnet",
          "name": "lllyasviel/sd_control_collection",
          "repo_id": "lllyasviel/sd_control_collection",
          "path": "sargezt_xl_depth_zeed.safetensors",
          "size_on_disk": 2502139104,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 2074
        },
        {
          "id": "lllyasviel/sd_control_collection:sargezt_xl_depth.safetensors",
          "type": "hf.controlnet",
          "name": "lllyasviel/sd_control_collection",
          "repo_id": "lllyasviel/sd_control_collection",
          "path": "sargezt_xl_depth.safetensors",
          "size_on_disk": 2502139104,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 2074
        },
        {
          "id": "lllyasviel/sd_control_collection:sargezt_xl_softedge.safetensors",
          "type": "hf.controlnet",
          "name": "lllyasviel/sd_control_collection",
          "repo_id": "lllyasviel/sd_control_collection",
          "path": "sargezt_xl_softedge.safetensors",
          "size_on_disk": 2502139104,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 2074
        },
        {
          "id": "lllyasviel/sd_control_collection:ip-adapter_xl.pth",
          "type": "hf.controlnet",
          "name": "lllyasviel/sd_control_collection",
          "repo_id": "lllyasviel/sd_control_collection",
          "path": "ip-adapter_xl.pth",
          "size_on_disk": 702585021,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 2074
        },
        {
          "id": "lllyasviel/sd_control_collection:kohya_controllllite_xl_depth_anime.safetensors",
          "type": "hf.controlnet",
          "name": "lllyasviel/sd_control_collection",
          "repo_id": "lllyasviel/sd_control_collection",
          "path": "kohya_controllllite_xl_depth_anime.safetensors",
          "size_on_disk": 11321048,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 2074
        },
        {
          "id": "lllyasviel/sd_control_collection:kohya_controllllite_xl_canny_anime.safetensors",
          "type": "hf.controlnet",
          "name": "lllyasviel/sd_control_collection",
          "repo_id": "lllyasviel/sd_control_collection",
          "path": "kohya_controllllite_xl_canny_anime.safetensors",
          "size_on_disk": 46172168,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 2074
        },
        {
          "id": "lllyasviel/sd_control_collection:kohya_controllllite_xl_scribble_anime.safetensors",
          "type": "hf.controlnet",
          "name": "lllyasviel/sd_control_collection",
          "repo_id": "lllyasviel/sd_control_collection",
          "path": "kohya_controllllite_xl_scribble_anime.safetensors",
          "size_on_disk": 46172168,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 2074
        },
        {
          "id": "lllyasviel/sd_control_collection:kohya_controllllite_xl_openpose_anime.safetensors",
          "type": "hf.controlnet",
          "name": "lllyasviel/sd_control_collection",
          "repo_id": "lllyasviel/sd_control_collection",
          "path": "kohya_controllllite_xl_openpose_anime.safetensors",
          "size_on_disk": 46172168,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 2074
        },
        {
          "id": "lllyasviel/sd_control_collection:kohya_controllllite_xl_openpose_anime_v2.safetensors",
          "type": "hf.controlnet",
          "name": "lllyasviel/sd_control_collection",
          "repo_id": "lllyasviel/sd_control_collection",
          "path": "kohya_controllllite_xl_openpose_anime_v2.safetensors",
          "size_on_disk": 46172168,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 2074
        },
        {
          "id": "lllyasviel/sd_control_collection:kohya_controllllite_xl_blur_anime_beta.safetensors",
          "type": "hf.controlnet",
          "name": "lllyasviel/sd_control_collection",
          "repo_id": "lllyasviel/sd_control_collection",
          "path": "kohya_controllllite_xl_blur_anime_beta.safetensors",
          "size_on_disk": 22512816,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 2074
        },
        {
          "id": "lllyasviel/sd_control_collection:kohya_controllllite_xl_blur.safetensors",
          "type": "hf.controlnet",
          "name": "lllyasviel/sd_control_collection",
          "repo_id": "lllyasviel/sd_control_collection",
          "path": "kohya_controllllite_xl_blur.safetensors",
          "size_on_disk": 46172168,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 2074
        },
        {
          "id": "lllyasviel/sd_control_collection:kohya_controllllite_xl_blur_anime.safetensors",
          "type": "hf.controlnet",
          "name": "lllyasviel/sd_control_collection",
          "repo_id": "lllyasviel/sd_control_collection",
          "path": "kohya_controllllite_xl_blur_anime.safetensors",
          "size_on_disk": 46172168,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 2074
        },
        {
          "id": "lllyasviel/sd_control_collection:kohya_controllllite_xl_canny.safetensors",
          "type": "hf.controlnet",
          "name": "lllyasviel/sd_control_collection",
          "repo_id": "lllyasviel/sd_control_collection",
          "path": "kohya_controllllite_xl_canny.safetensors",
          "size_on_disk": 46172168,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 2074
        },
        {
          "id": "lllyasviel/sd_control_collection:kohya_controllllite_xl_depth.safetensors",
          "type": "hf.controlnet",
          "name": "lllyasviel/sd_control_collection",
          "repo_id": "lllyasviel/sd_control_collection",
          "path": "kohya_controllllite_xl_depth.safetensors",
          "size_on_disk": 46172168,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 2074
        },
        {
          "id": "bdsqlsz/qinglong_controlnet-lllite:bdsqlsz_controlllite_xl_canny.safetensors",
          "type": "hf.controlnet",
          "name": "bdsqlsz/qinglong_controlnet-lllite",
          "repo_id": "bdsqlsz/qinglong_controlnet-lllite",
          "path": "bdsqlsz_controlllite_xl_canny.safetensors",
          "size_on_disk": 223906688,
          "tags": [
            "diffusers",
            "onnx",
            "license:cc-by-nc-sa-4.0",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 11923,
          "likes": 340
        },
        {
          "id": "bdsqlsz/qinglong_controlnet-lllite:bdsqlsz_controlllite_xl_depth.safetensors",
          "type": "hf.controlnet",
          "name": "bdsqlsz/qinglong_controlnet-lllite",
          "repo_id": "bdsqlsz/qinglong_controlnet-lllite",
          "path": "bdsqlsz_controlllite_xl_depth.safetensors",
          "size_on_disk": 223906688,
          "tags": [
            "diffusers",
            "onnx",
            "license:cc-by-nc-sa-4.0",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 11923,
          "likes": 340
        },
        {
          "id": "bdsqlsz/qinglong_controlnet-lllite:bdsqlsz_controlllite_xl_depth_V2.safetensors",
          "type": "hf.controlnet",
          "name": "bdsqlsz/qinglong_controlnet-lllite",
          "repo_id": "bdsqlsz/qinglong_controlnet-lllite",
          "path": "bdsqlsz_controlllite_xl_depth_V2.safetensors",
          "size_on_disk": 223905112,
          "tags": [
            "diffusers",
            "onnx",
            "license:cc-by-nc-sa-4.0",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 11923,
          "likes": 340
        },
        {
          "id": "bdsqlsz/qinglong_controlnet-lllite:bdsqlsz_controlllite_xl_dw_openpose.safetensors",
          "type": "hf.controlnet",
          "name": "bdsqlsz/qinglong_controlnet-lllite",
          "repo_id": "bdsqlsz/qinglong_controlnet-lllite",
          "path": "bdsqlsz_controlllite_xl_dw_openpose.safetensors",
          "size_on_disk": 223906688,
          "tags": [
            "diffusers",
            "onnx",
            "license:cc-by-nc-sa-4.0",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 11923,
          "likes": 340
        },
        {
          "id": "bdsqlsz/qinglong_controlnet-lllite:bdsqlsz_controlllite_xl_lineart_anime_denoise.safetensors",
          "type": "hf.controlnet",
          "name": "bdsqlsz/qinglong_controlnet-lllite",
          "repo_id": "bdsqlsz/qinglong_controlnet-lllite",
          "path": "bdsqlsz_controlllite_xl_lineart_anime_denoise.safetensors",
          "size_on_disk": 223906688,
          "tags": [
            "diffusers",
            "onnx",
            "license:cc-by-nc-sa-4.0",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 11923,
          "likes": 340
        },
        {
          "id": "bdsqlsz/qinglong_controlnet-lllite:bdsqlsz_controlllite_xl_mlsd_V2.safetensors",
          "type": "hf.controlnet",
          "name": "bdsqlsz/qinglong_controlnet-lllite",
          "repo_id": "bdsqlsz/qinglong_controlnet-lllite",
          "path": "bdsqlsz_controlllite_xl_mlsd_V2.safetensors",
          "size_on_disk": 223906688,
          "tags": [
            "diffusers",
            "onnx",
            "license:cc-by-nc-sa-4.0",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 11923,
          "likes": 340
        },
        {
          "id": "bdsqlsz/qinglong_controlnet-lllite:bdsqlsz_controlllite_xl_normal.safetensors",
          "type": "hf.controlnet",
          "name": "bdsqlsz/qinglong_controlnet-lllite",
          "repo_id": "bdsqlsz/qinglong_controlnet-lllite",
          "path": "bdsqlsz_controlllite_xl_normal.safetensors",
          "size_on_disk": 223906688,
          "tags": [
            "diffusers",
            "onnx",
            "license:cc-by-nc-sa-4.0",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 11923,
          "likes": 340
        },
        {
          "id": "bdsqlsz/qinglong_controlnet-lllite:bdsqlsz_controlllite_xl_normal_dsine.safetensors",
          "type": "hf.controlnet",
          "name": "bdsqlsz/qinglong_controlnet-lllite",
          "repo_id": "bdsqlsz/qinglong_controlnet-lllite",
          "path": "bdsqlsz_controlllite_xl_normal_dsine.safetensors",
          "size_on_disk": 223905104,
          "tags": [
            "diffusers",
            "onnx",
            "license:cc-by-nc-sa-4.0",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 11923,
          "likes": 340
        },
        {
          "id": "bdsqlsz/qinglong_controlnet-lllite:bdsqlsz_controlllite_xl_recolor_luminance.safetensors",
          "type": "hf.controlnet",
          "name": "bdsqlsz/qinglong_controlnet-lllite",
          "repo_id": "bdsqlsz/qinglong_controlnet-lllite",
          "path": "bdsqlsz_controlllite_xl_recolor_luminance.safetensors",
          "size_on_disk": 223906688,
          "tags": [
            "diffusers",
            "onnx",
            "license:cc-by-nc-sa-4.0",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 11923,
          "likes": 340
        },
        {
          "id": "bdsqlsz/qinglong_controlnet-lllite:bdsqlsz_controlllite_xl_segment_animeface.safetensors",
          "type": "hf.controlnet",
          "name": "bdsqlsz/qinglong_controlnet-lllite",
          "repo_id": "bdsqlsz/qinglong_controlnet-lllite",
          "path": "bdsqlsz_controlllite_xl_segment_animeface.safetensors",
          "size_on_disk": 223906696,
          "tags": [
            "diffusers",
            "onnx",
            "license:cc-by-nc-sa-4.0",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 11923,
          "likes": 340
        },
        {
          "id": "bdsqlsz/qinglong_controlnet-lllite:bdsqlsz_controlllite_xl_segment_animeface_V2.safetensors",
          "type": "hf.controlnet",
          "name": "bdsqlsz/qinglong_controlnet-lllite",
          "repo_id": "bdsqlsz/qinglong_controlnet-lllite",
          "path": "bdsqlsz_controlllite_xl_segment_animeface_V2.safetensors",
          "size_on_disk": 223906688,
          "tags": [
            "diffusers",
            "onnx",
            "license:cc-by-nc-sa-4.0",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 11923,
          "likes": 340
        },
        {
          "id": "bdsqlsz/qinglong_controlnet-lllite:bdsqlsz_controlllite_xl_sketch.safetensors",
          "type": "hf.controlnet",
          "name": "bdsqlsz/qinglong_controlnet-lllite",
          "repo_id": "bdsqlsz/qinglong_controlnet-lllite",
          "path": "bdsqlsz_controlllite_xl_sketch.safetensors",
          "size_on_disk": 223906688,
          "tags": [
            "diffusers",
            "onnx",
            "license:cc-by-nc-sa-4.0",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 11923,
          "likes": 340
        },
        {
          "id": "bdsqlsz/qinglong_controlnet-lllite:bdsqlsz_controlllite_xl_softedge.safetensors",
          "type": "hf.controlnet",
          "name": "bdsqlsz/qinglong_controlnet-lllite",
          "repo_id": "bdsqlsz/qinglong_controlnet-lllite",
          "path": "bdsqlsz_controlllite_xl_softedge.safetensors",
          "size_on_disk": 223906688,
          "tags": [
            "diffusers",
            "onnx",
            "license:cc-by-nc-sa-4.0",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 11923,
          "likes": 340
        },
        {
          "id": "bdsqlsz/qinglong_controlnet-lllite:bdsqlsz_controlllite_xl_t2i-adapter_color_shuffle.safetensors",
          "type": "hf.controlnet",
          "name": "bdsqlsz/qinglong_controlnet-lllite",
          "repo_id": "bdsqlsz/qinglong_controlnet-lllite",
          "path": "bdsqlsz_controlllite_xl_t2i-adapter_color_shuffle.safetensors",
          "size_on_disk": 223906688,
          "tags": [
            "diffusers",
            "onnx",
            "license:cc-by-nc-sa-4.0",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 11923,
          "likes": 340
        },
        {
          "id": "bdsqlsz/qinglong_controlnet-lllite:bdsqlsz_controlllite_xl_tile_anime_alpha.safetensors",
          "type": "hf.controlnet",
          "name": "bdsqlsz/qinglong_controlnet-lllite",
          "repo_id": "bdsqlsz/qinglong_controlnet-lllite",
          "path": "bdsqlsz_controlllite_xl_tile_anime_alpha.safetensors",
          "size_on_disk": 223906688,
          "tags": [
            "diffusers",
            "onnx",
            "license:cc-by-nc-sa-4.0",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 11923,
          "likes": 340
        },
        {
          "id": "bdsqlsz/qinglong_controlnet-lllite:bdsqlsz_controlllite_xl_tile_anime_beta.safetensors",
          "type": "hf.controlnet",
          "name": "bdsqlsz/qinglong_controlnet-lllite",
          "repo_id": "bdsqlsz/qinglong_controlnet-lllite",
          "path": "bdsqlsz_controlllite_xl_tile_anime_beta.safetensors",
          "size_on_disk": 223906688,
          "tags": [
            "diffusers",
            "onnx",
            "license:cc-by-nc-sa-4.0",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 11923,
          "likes": 340
        },
        {
          "id": "bdsqlsz/qinglong_controlnet-lllite:bdsqlsz_controlllite_xl_tile_realistic.safetensors",
          "type": "hf.controlnet",
          "name": "bdsqlsz/qinglong_controlnet-lllite",
          "repo_id": "bdsqlsz/qinglong_controlnet-lllite",
          "path": "bdsqlsz_controlllite_xl_tile_realistic.safetensors",
          "size_on_disk": 317085856,
          "tags": [
            "diffusers",
            "onnx",
            "license:cc-by-nc-sa-4.0",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 11923,
          "likes": 340
        }
      ],
      "basic_fields": [
        "model",
        "quantization",
        "prompt",
        "width",
        "height",
        "init_image",
        "strength",
        "controlnet",
        "control_image",
        "controlnet_conditioning_scale"
      ]
    },
    {
      "title": "Stable Diffusion XL (Img2Img)",
      "description": "Transforms existing images based on text prompts using Stable Diffusion XL.\n    image, generation, image-to-image, SDXL, style-transfer\n\n    Use cases:\n    - Modifying existing images to fit a specific style or theme\n    - Enhancing or altering photographs\n    - Creating variations of existing artwork\n    - Applying text-guided edits to images",
      "namespace": "huggingface.image_to_image",
      "node_type": "huggingface.image_to_image.StableDiffusionXLImg2Img",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.stable_diffusion_xl"
          },
          "default": {
            "type": "hf.stable_diffusion_xl",
            "repo_id": "",
            "path": null,
            "variant": null,
            "allow_patterns": null,
            "ignore_patterns": null
          },
          "title": "Model",
          "description": "The Stable Diffusion XL model to use for generation."
        },
        {
          "name": "quantization",
          "type": {
            "type": "enum",
            "values": [
              "fp16",
              "fp4",
              "int4"
            ],
            "type_name": "nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionXLQuantization"
          },
          "default": "fp16",
          "title": "Quantization",
          "description": "Quantization level for Stable Diffusion XL (enable INT4/FP4 to use a Nunchaku UNet)."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt for image generation."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to guide what should not appear in the generated image."
        },
        {
          "name": "width",
          "type": {
            "type": "int"
          },
          "default": 1024,
          "title": "Width",
          "description": "Width of the generated image.",
          "min": 64.0,
          "max": 2048.0
        },
        {
          "name": "height",
          "type": {
            "type": "int"
          },
          "default": 1024,
          "title": "Height",
          "description": "Height of the generated image",
          "min": 64.0,
          "max": 2048.0
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for the random number generator.",
          "min": -1.0,
          "max": 1000000.0
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 25,
          "title": "Num Inference Steps",
          "description": "Number of inference steps.",
          "min": 1.0,
          "max": 100.0
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 7.0,
          "title": "Guidance Scale",
          "description": "Guidance scale for generation.",
          "min": 0.0,
          "max": 20.0
        },
        {
          "name": "scheduler",
          "type": {
            "type": "enum",
            "values": [
              "DPMSolverSDEScheduler",
              "EulerDiscreteScheduler",
              "LMSDiscreteScheduler",
              "DDIMScheduler",
              "DDPMScheduler",
              "HeunDiscreteScheduler",
              "DPMSolverMultistepScheduler",
              "DEISMultistepScheduler",
              "PNDMScheduler",
              "EulerAncestralDiscreteScheduler",
              "UniPCMultistepScheduler",
              "KDPM2DiscreteScheduler",
              "DPMSolverSinglestepScheduler",
              "KDPM2AncestralDiscreteScheduler"
            ],
            "type_name": "nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionXLBase.StableDiffusionScheduler"
          },
          "default": "EulerDiscreteScheduler",
          "title": "Scheduler",
          "description": "The scheduler to use for the diffusion process."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "hf.lora_sdxl_config"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRA models to use for image processing"
        },
        {
          "name": "lora_scale",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Lora Scale",
          "description": "Strength of the LoRAs",
          "min": 0.0,
          "max": 1.0
        },
        {
          "name": "ip_adapter_model",
          "type": {
            "type": "hf.ip_adapter"
          },
          "default": {
            "type": "hf.ip_adapter",
            "repo_id": "",
            "path": null,
            "variant": null,
            "allow_patterns": null,
            "ignore_patterns": null
          },
          "title": "Ip Adapter Model",
          "description": "The IP adapter model to use for image processing"
        },
        {
          "name": "ip_adapter_image",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Ip Adapter Image",
          "description": "When provided the image will be fed into the IP adapter"
        },
        {
          "name": "ip_adapter_scale",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Ip Adapter Scale",
          "description": "Strength of the IP adapter image",
          "min": 0.0,
          "max": 1.0
        },
        {
          "name": "enable_attention_slicing",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Attention Slicing",
          "description": "Enable attention slicing for the pipeline. This can reduce VRAM usage."
        },
        {
          "name": "enable_tiling",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Tiling",
          "description": "Legacy VAE tiling flag (disabled in favor of PyTorch 2 attention optimizations)."
        },
        {
          "name": "enable_cpu_offload",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Cpu Offload",
          "description": "Enable CPU offload for the pipeline. This can reduce VRAM usage."
        },
        {
          "name": "output_type",
          "type": {
            "type": "enum",
            "values": [
              "Image",
              "Latent"
            ],
            "type_name": "nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionXLBase.StableDiffusionOutputType"
          },
          "default": "Image",
          "title": "Output Type",
          "description": "The type of output to generate."
        },
        {
          "name": "init_image",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Init Image",
          "description": "The initial image for Image-to-Image generation."
        },
        {
          "name": "strength",
          "type": {
            "type": "float"
          },
          "default": 0.8,
          "title": "Strength",
          "description": "Strength for Image-to-Image generation. Higher values allow for more deviation from the original image.",
          "min": 0.0,
          "max": 1.0
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "image"
        },
        {
          "type": {
            "type": "torch_tensor"
          },
          "name": "latent"
        }
      ],
      "recommended_models": [
        {
          "id": "h94/IP-Adapter:sdxl_models/ip-adapter_sdxl.bin",
          "type": "hf.ip_adapter",
          "name": "h94/IP-Adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "sdxl_models/ip-adapter_sdxl.bin",
          "size_on_disk": 702585097,
          "pipeline_tag": "text-to-image",
          "tags": [
            "diffusers",
            "safetensors",
            "text-to-image",
            "stable-diffusion",
            "en",
            "arxiv:2308.06721",
            "license:apache-2.0",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 1262
        },
        {
          "id": "h94/IP-Adapter:sdxl_models/ip-adapter_sdxl_vit-h.bin",
          "type": "hf.ip_adapter",
          "name": "h94/IP-Adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "sdxl_models/ip-adapter_sdxl_vit-h.bin",
          "size_on_disk": 698390793,
          "pipeline_tag": "text-to-image",
          "tags": [
            "diffusers",
            "safetensors",
            "text-to-image",
            "stable-diffusion",
            "en",
            "arxiv:2308.06721",
            "license:apache-2.0",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 1262
        },
        {
          "id": "h94/IP-Adapter:sdxl_models/ip-adapter-plus_sdxl_vit-h.bin",
          "type": "hf.ip_adapter",
          "name": "h94/IP-Adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "sdxl_models/ip-adapter-plus_sdxl_vit-h.bin",
          "size_on_disk": 1013454427,
          "pipeline_tag": "text-to-image",
          "tags": [
            "diffusers",
            "safetensors",
            "text-to-image",
            "stable-diffusion",
            "en",
            "arxiv:2308.06721",
            "license:apache-2.0",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 1262
        },
        {
          "id": "stabilityai/stable-diffusion-xl-base-1.0:sd_xl_base_1.0.safetensors",
          "type": "hf.stable_diffusion_xl",
          "name": "stabilityai/stable-diffusion-xl-base-1.0",
          "repo_id": "stabilityai/stable-diffusion-xl-base-1.0",
          "path": "sd_xl_base_1.0.safetensors",
          "size_on_disk": 6938078334,
          "pipeline_tag": "text-to-image",
          "tags": [
            "diffusers",
            "onnx",
            "safetensors",
            "text-to-image",
            "stable-diffusion",
            "arxiv:2307.01952",
            "arxiv:2211.01324",
            "arxiv:2108.01073",
            "arxiv:2112.10752",
            "license:openrail++",
            "endpoints_compatible",
            "diffusers:StableDiffusionXLPipeline",
            "deploy:azure",
            "region:us"
          ],
          "has_model_index": true,
          "downloads": 2122049,
          "likes": 7232
        },
        {
          "id": "Lykon/dreamshaper-xl-v2-turbo:DreamShaperXL_Turbo_v2_1.safetensors",
          "type": "hf.stable_diffusion_xl",
          "name": "Lykon/dreamshaper-xl-v2-turbo",
          "repo_id": "Lykon/dreamshaper-xl-v2-turbo",
          "path": "DreamShaperXL_Turbo_v2_1.safetensors",
          "size_on_disk": 6939220250,
          "pipeline_tag": "text-to-image",
          "tags": [
            "diffusers",
            "safetensors",
            "stable-diffusion",
            "stable-diffusion-diffusers",
            "stable-diffusion-xl",
            "stable-diffusion-xl-turbo",
            "text-to-image",
            "art",
            "artistic",
            "anime",
            "dreamshaper",
            "turbo",
            "lcm",
            "en",
            "license:openrail++",
            "endpoints_compatible",
            "diffusers:StableDiffusionXLPipeline",
            "region:us"
          ],
          "has_model_index": true,
          "downloads": 62403,
          "likes": 70
        },
        {
          "id": "RunDiffusion/Juggernaut-XL-v9:Juggernaut-XL_v9_RunDiffusionPhoto_v2.safetensors",
          "type": "hf.stable_diffusion_xl",
          "name": "RunDiffusion/Juggernaut-XL-v9",
          "repo_id": "RunDiffusion/Juggernaut-XL-v9",
          "path": "Juggernaut-XL_v9_RunDiffusionPhoto_v2.safetensors",
          "size_on_disk": 7105348188,
          "pipeline_tag": "text-to-image",
          "tags": [
            "diffusers",
            "art",
            "people",
            "diffusion",
            "Cinematic",
            "Photography",
            "Landscape",
            "Interior",
            "Food",
            "Car",
            "Wildlife",
            "Architecture",
            "text-to-image",
            "en",
            "base_model:stabilityai/stable-diffusion-xl-base-1.0",
            "base_model:finetune:stabilityai/stable-diffusion-xl-base-1.0",
            "license:creativeml-openrail-m",
            "endpoints_compatible",
            "diffusers:StableDiffusionXLPipeline",
            "region:us"
          ],
          "has_model_index": true,
          "downloads": 201043,
          "likes": 277
        },
        {
          "id": "dataautogpt3/ProteusV0.3:ProteusV0.3.safetensors",
          "type": "hf.stable_diffusion_xl",
          "name": "dataautogpt3/ProteusV0.3",
          "repo_id": "dataautogpt3/ProteusV0.3",
          "path": "ProteusV0.3.safetensors",
          "size_on_disk": 6938040736,
          "pipeline_tag": "text-to-image",
          "tags": [
            "diffusers",
            "text-to-image",
            "license:gpl-3.0",
            "endpoints_compatible",
            "diffusers:StableDiffusionXLPipeline",
            "region:us"
          ],
          "has_model_index": true,
          "downloads": 159694,
          "likes": 94
        },
        {
          "id": "John6666/prefect-illustrious-xl-v3-sdxl",
          "type": "hf.stable_diffusion_xl",
          "name": "John6666/prefect-illustrious-xl-v3-sdxl",
          "repo_id": "John6666/prefect-illustrious-xl-v3-sdxl",
          "size_on_disk": 6941387072,
          "pipeline_tag": "text-to-image",
          "tags": [
            "diffusers",
            "safetensors",
            "text-to-image",
            "stable-diffusion",
            "stable-diffusion-xl",
            "anime",
            "girls",
            "styles",
            "lighting",
            "texture",
            "clean",
            "color balance",
            "prompt understanding",
            "merge",
            "noobai",
            "Illustrious XL v1.0",
            "illustrious",
            "en",
            "base_model:Laxhar/noobai-XL-1.1",
            "base_model:merge:Laxhar/noobai-XL-1.1",
            "base_model:OnomaAIResearch/Illustrious-XL-v1.0",
            "base_model:merge:OnomaAIResearch/Illustrious-XL-v1.0",
            "license:other",
            "endpoints_compatible",
            "diffusers:StableDiffusionXLPipeline",
            "region:us"
          ],
          "has_model_index": true,
          "downloads": 278268,
          "likes": 0
        },
        {
          "id": "cagliostrolab/animagine-xl-4.0:animagine-xl-4.0-opt.safetensors",
          "type": "hf.stable_diffusion_xl",
          "name": "cagliostrolab/animagine-xl-4.0",
          "repo_id": "cagliostrolab/animagine-xl-4.0",
          "path": "animagine-xl-4.0-opt.safetensors",
          "size_on_disk": 6938350040,
          "pipeline_tag": "text-to-image",
          "tags": [
            "diffusers",
            "safetensors",
            "text-to-image",
            "stable-diffusion",
            "stable-diffusion-xl",
            "en",
            "base_model:stabilityai/stable-diffusion-xl-base-1.0",
            "base_model:finetune:stabilityai/stable-diffusion-xl-base-1.0",
            "license:openrail++",
            "endpoints_compatible",
            "diffusers:StableDiffusionXLPipeline",
            "region:us"
          ],
          "has_model_index": true,
          "downloads": 249066,
          "likes": 363
        },
        {
          "id": "SG161222/RealVisXL_V5.0:RealVisXL_V5.0_fp16.safetensors",
          "type": "hf.stable_diffusion_xl",
          "name": "SG161222/RealVisXL_V5.0",
          "repo_id": "SG161222/RealVisXL_V5.0",
          "path": "RealVisXL_V5.0_fp16.safetensors",
          "size_on_disk": 6938065488,
          "pipeline_tag": "text-to-image",
          "tags": [
            "diffusers",
            "safetensors",
            "license:openrail++",
            "endpoints_compatible",
            "diffusers:StableDiffusionXLPipeline",
            "region:us"
          ],
          "has_model_index": true,
          "downloads": 55543,
          "likes": 121
        },
        {
          "id": "nunchaku-tech/nunchaku-sdxl:svdq-int4_r32-sdxl.safetensors",
          "type": "hf.stable_diffusion_xl",
          "name": "nunchaku-tech/nunchaku-sdxl",
          "repo_id": "nunchaku-tech/nunchaku-sdxl",
          "path": "svdq-int4_r32-sdxl.safetensors",
          "size_on_disk": 2559021560,
          "pipeline_tag": "text-to-image",
          "tags": [
            "diffusers",
            "text-to-image",
            "SVDQuant",
            "SDXL",
            "Diffusion",
            "Quantization",
            "stable-diffusion",
            "en",
            "dataset:mit-han-lab/svdquant-datasets",
            "arxiv:2411.05007",
            "base_model:stabilityai/stable-diffusion-xl-base-1.0",
            "base_model:quantized:stabilityai/stable-diffusion-xl-base-1.0",
            "license:openrail++",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 819,
          "likes": 26
        },
        {
          "id": "nunchaku-tech/nunchaku-sdxl-turbo:svdq-int4_r32-sdxl-turbo.safetensors",
          "type": "hf.stable_diffusion_xl",
          "name": "nunchaku-tech/nunchaku-sdxl-turbo",
          "repo_id": "nunchaku-tech/nunchaku-sdxl-turbo",
          "path": "svdq-int4_r32-sdxl-turbo.safetensors",
          "size_on_disk": 2559021776,
          "pipeline_tag": "text-to-image",
          "tags": [
            "diffusers",
            "text-to-image",
            "SVDQuant",
            "SDXL-Turbo",
            "Diffusion",
            "Quantization",
            "stable-diffusion",
            "en",
            "dataset:mit-han-lab/svdquant-datasets",
            "arxiv:2411.05007",
            "base_model:stabilityai/stable-diffusion-xl-base-1.0",
            "base_model:quantized:stabilityai/stable-diffusion-xl-base-1.0",
            "license:other",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 746,
          "likes": 10
        }
      ],
      "basic_fields": [
        "model",
        "quantization",
        "prompt",
        "width",
        "height",
        "init_image",
        "strength"
      ]
    },
    {
      "title": "Stable Diffusion XL (Inpaint)",
      "description": "Performs inpainting on images using Stable Diffusion XL.\n    image, inpainting, SDXL\n\n    Use cases:\n    - Remove unwanted objects from images\n    - Fill in missing parts of images\n    - Modify specific areas of images while preserving the rest",
      "namespace": "huggingface.image_to_image",
      "node_type": "huggingface.image_to_image.StableDiffusionXLInpainting",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.stable_diffusion_xl"
          },
          "default": {
            "type": "hf.stable_diffusion_xl",
            "repo_id": "",
            "path": null,
            "variant": null,
            "allow_patterns": null,
            "ignore_patterns": null
          },
          "title": "Model",
          "description": "The Stable Diffusion XL model to use for generation."
        },
        {
          "name": "quantization",
          "type": {
            "type": "enum",
            "values": [
              "fp16",
              "fp4",
              "int4"
            ],
            "type_name": "nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionXLQuantization"
          },
          "default": "fp16",
          "title": "Quantization",
          "description": "Quantization level for Stable Diffusion XL (enable INT4/FP4 to use a Nunchaku UNet)."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt for image generation."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to guide what should not appear in the generated image."
        },
        {
          "name": "width",
          "type": {
            "type": "int"
          },
          "default": 1024,
          "title": "Width",
          "description": "Width of the generated image.",
          "min": 64.0,
          "max": 2048.0
        },
        {
          "name": "height",
          "type": {
            "type": "int"
          },
          "default": 1024,
          "title": "Height",
          "description": "Height of the generated image",
          "min": 64.0,
          "max": 2048.0
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for the random number generator.",
          "min": -1.0,
          "max": 1000000.0
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 25,
          "title": "Num Inference Steps",
          "description": "Number of inference steps.",
          "min": 1.0,
          "max": 100.0
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 7.0,
          "title": "Guidance Scale",
          "description": "Guidance scale for generation.",
          "min": 0.0,
          "max": 20.0
        },
        {
          "name": "scheduler",
          "type": {
            "type": "enum",
            "values": [
              "DPMSolverSDEScheduler",
              "EulerDiscreteScheduler",
              "LMSDiscreteScheduler",
              "DDIMScheduler",
              "DDPMScheduler",
              "HeunDiscreteScheduler",
              "DPMSolverMultistepScheduler",
              "DEISMultistepScheduler",
              "PNDMScheduler",
              "EulerAncestralDiscreteScheduler",
              "UniPCMultistepScheduler",
              "KDPM2DiscreteScheduler",
              "DPMSolverSinglestepScheduler",
              "KDPM2AncestralDiscreteScheduler"
            ],
            "type_name": "nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionXLBase.StableDiffusionScheduler"
          },
          "default": "EulerDiscreteScheduler",
          "title": "Scheduler",
          "description": "The scheduler to use for the diffusion process."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "hf.lora_sdxl_config"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRA models to use for image processing"
        },
        {
          "name": "lora_scale",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Lora Scale",
          "description": "Strength of the LoRAs",
          "min": 0.0,
          "max": 1.0
        },
        {
          "name": "ip_adapter_model",
          "type": {
            "type": "hf.ip_adapter"
          },
          "default": {
            "type": "hf.ip_adapter",
            "repo_id": "",
            "path": null,
            "variant": null,
            "allow_patterns": null,
            "ignore_patterns": null
          },
          "title": "Ip Adapter Model",
          "description": "The IP adapter model to use for image processing"
        },
        {
          "name": "ip_adapter_image",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Ip Adapter Image",
          "description": "When provided the image will be fed into the IP adapter"
        },
        {
          "name": "ip_adapter_scale",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Ip Adapter Scale",
          "description": "Strength of the IP adapter image",
          "min": 0.0,
          "max": 1.0
        },
        {
          "name": "enable_attention_slicing",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Attention Slicing",
          "description": "Enable attention slicing for the pipeline. This can reduce VRAM usage."
        },
        {
          "name": "enable_tiling",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Tiling",
          "description": "Legacy VAE tiling flag (disabled in favor of PyTorch 2 attention optimizations)."
        },
        {
          "name": "enable_cpu_offload",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Cpu Offload",
          "description": "Enable CPU offload for the pipeline. This can reduce VRAM usage."
        },
        {
          "name": "output_type",
          "type": {
            "type": "enum",
            "values": [
              "Image",
              "Latent"
            ],
            "type_name": "nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionXLBase.StableDiffusionOutputType"
          },
          "default": "Image",
          "title": "Output Type",
          "description": "The type of output to generate."
        },
        {
          "name": "image",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image",
          "description": "The initial image to be inpainted."
        },
        {
          "name": "mask_image",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Mask Image",
          "description": "The mask image indicating areas to be inpainted."
        },
        {
          "name": "strength",
          "type": {
            "type": "float"
          },
          "default": 0.8,
          "title": "Strength",
          "description": "Strength for inpainting. Higher values allow for more deviation from the original image.",
          "min": 0.0,
          "max": 1.0
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "image"
        },
        {
          "type": {
            "type": "torch_tensor"
          },
          "name": "latent"
        }
      ],
      "recommended_models": [
        {
          "id": "h94/IP-Adapter:sdxl_models/ip-adapter_sdxl.bin",
          "type": "hf.ip_adapter",
          "name": "h94/IP-Adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "sdxl_models/ip-adapter_sdxl.bin",
          "size_on_disk": 702585097,
          "pipeline_tag": "text-to-image",
          "tags": [
            "diffusers",
            "safetensors",
            "text-to-image",
            "stable-diffusion",
            "en",
            "arxiv:2308.06721",
            "license:apache-2.0",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 1262
        },
        {
          "id": "h94/IP-Adapter:sdxl_models/ip-adapter_sdxl_vit-h.bin",
          "type": "hf.ip_adapter",
          "name": "h94/IP-Adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "sdxl_models/ip-adapter_sdxl_vit-h.bin",
          "size_on_disk": 698390793,
          "pipeline_tag": "text-to-image",
          "tags": [
            "diffusers",
            "safetensors",
            "text-to-image",
            "stable-diffusion",
            "en",
            "arxiv:2308.06721",
            "license:apache-2.0",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 1262
        },
        {
          "id": "h94/IP-Adapter:sdxl_models/ip-adapter-plus_sdxl_vit-h.bin",
          "type": "hf.ip_adapter",
          "name": "h94/IP-Adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "sdxl_models/ip-adapter-plus_sdxl_vit-h.bin",
          "size_on_disk": 1013454427,
          "pipeline_tag": "text-to-image",
          "tags": [
            "diffusers",
            "safetensors",
            "text-to-image",
            "stable-diffusion",
            "en",
            "arxiv:2308.06721",
            "license:apache-2.0",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 1262
        },
        {
          "id": "stabilityai/stable-diffusion-xl-base-1.0:sd_xl_base_1.0.safetensors",
          "type": "hf.stable_diffusion_xl",
          "name": "stabilityai/stable-diffusion-xl-base-1.0",
          "repo_id": "stabilityai/stable-diffusion-xl-base-1.0",
          "path": "sd_xl_base_1.0.safetensors",
          "size_on_disk": 6938078334,
          "pipeline_tag": "text-to-image",
          "tags": [
            "diffusers",
            "onnx",
            "safetensors",
            "text-to-image",
            "stable-diffusion",
            "arxiv:2307.01952",
            "arxiv:2211.01324",
            "arxiv:2108.01073",
            "arxiv:2112.10752",
            "license:openrail++",
            "endpoints_compatible",
            "diffusers:StableDiffusionXLPipeline",
            "deploy:azure",
            "region:us"
          ],
          "has_model_index": true,
          "downloads": 2122049,
          "likes": 7232
        },
        {
          "id": "Lykon/dreamshaper-xl-v2-turbo:DreamShaperXL_Turbo_v2_1.safetensors",
          "type": "hf.stable_diffusion_xl",
          "name": "Lykon/dreamshaper-xl-v2-turbo",
          "repo_id": "Lykon/dreamshaper-xl-v2-turbo",
          "path": "DreamShaperXL_Turbo_v2_1.safetensors",
          "size_on_disk": 6939220250,
          "pipeline_tag": "text-to-image",
          "tags": [
            "diffusers",
            "safetensors",
            "stable-diffusion",
            "stable-diffusion-diffusers",
            "stable-diffusion-xl",
            "stable-diffusion-xl-turbo",
            "text-to-image",
            "art",
            "artistic",
            "anime",
            "dreamshaper",
            "turbo",
            "lcm",
            "en",
            "license:openrail++",
            "endpoints_compatible",
            "diffusers:StableDiffusionXLPipeline",
            "region:us"
          ],
          "has_model_index": true,
          "downloads": 62403,
          "likes": 70
        },
        {
          "id": "RunDiffusion/Juggernaut-XL-v9:Juggernaut-XL_v9_RunDiffusionPhoto_v2.safetensors",
          "type": "hf.stable_diffusion_xl",
          "name": "RunDiffusion/Juggernaut-XL-v9",
          "repo_id": "RunDiffusion/Juggernaut-XL-v9",
          "path": "Juggernaut-XL_v9_RunDiffusionPhoto_v2.safetensors",
          "size_on_disk": 7105348188,
          "pipeline_tag": "text-to-image",
          "tags": [
            "diffusers",
            "art",
            "people",
            "diffusion",
            "Cinematic",
            "Photography",
            "Landscape",
            "Interior",
            "Food",
            "Car",
            "Wildlife",
            "Architecture",
            "text-to-image",
            "en",
            "base_model:stabilityai/stable-diffusion-xl-base-1.0",
            "base_model:finetune:stabilityai/stable-diffusion-xl-base-1.0",
            "license:creativeml-openrail-m",
            "endpoints_compatible",
            "diffusers:StableDiffusionXLPipeline",
            "region:us"
          ],
          "has_model_index": true,
          "downloads": 201043,
          "likes": 277
        },
        {
          "id": "dataautogpt3/ProteusV0.3:ProteusV0.3.safetensors",
          "type": "hf.stable_diffusion_xl",
          "name": "dataautogpt3/ProteusV0.3",
          "repo_id": "dataautogpt3/ProteusV0.3",
          "path": "ProteusV0.3.safetensors",
          "size_on_disk": 6938040736,
          "pipeline_tag": "text-to-image",
          "tags": [
            "diffusers",
            "text-to-image",
            "license:gpl-3.0",
            "endpoints_compatible",
            "diffusers:StableDiffusionXLPipeline",
            "region:us"
          ],
          "has_model_index": true,
          "downloads": 159694,
          "likes": 94
        },
        {
          "id": "John6666/prefect-illustrious-xl-v3-sdxl",
          "type": "hf.stable_diffusion_xl",
          "name": "John6666/prefect-illustrious-xl-v3-sdxl",
          "repo_id": "John6666/prefect-illustrious-xl-v3-sdxl",
          "size_on_disk": 6941387072,
          "pipeline_tag": "text-to-image",
          "tags": [
            "diffusers",
            "safetensors",
            "text-to-image",
            "stable-diffusion",
            "stable-diffusion-xl",
            "anime",
            "girls",
            "styles",
            "lighting",
            "texture",
            "clean",
            "color balance",
            "prompt understanding",
            "merge",
            "noobai",
            "Illustrious XL v1.0",
            "illustrious",
            "en",
            "base_model:Laxhar/noobai-XL-1.1",
            "base_model:merge:Laxhar/noobai-XL-1.1",
            "base_model:OnomaAIResearch/Illustrious-XL-v1.0",
            "base_model:merge:OnomaAIResearch/Illustrious-XL-v1.0",
            "license:other",
            "endpoints_compatible",
            "diffusers:StableDiffusionXLPipeline",
            "region:us"
          ],
          "has_model_index": true,
          "downloads": 278268,
          "likes": 0
        },
        {
          "id": "cagliostrolab/animagine-xl-4.0:animagine-xl-4.0-opt.safetensors",
          "type": "hf.stable_diffusion_xl",
          "name": "cagliostrolab/animagine-xl-4.0",
          "repo_id": "cagliostrolab/animagine-xl-4.0",
          "path": "animagine-xl-4.0-opt.safetensors",
          "size_on_disk": 6938350040,
          "pipeline_tag": "text-to-image",
          "tags": [
            "diffusers",
            "safetensors",
            "text-to-image",
            "stable-diffusion",
            "stable-diffusion-xl",
            "en",
            "base_model:stabilityai/stable-diffusion-xl-base-1.0",
            "base_model:finetune:stabilityai/stable-diffusion-xl-base-1.0",
            "license:openrail++",
            "endpoints_compatible",
            "diffusers:StableDiffusionXLPipeline",
            "region:us"
          ],
          "has_model_index": true,
          "downloads": 249066,
          "likes": 363
        },
        {
          "id": "SG161222/RealVisXL_V5.0:RealVisXL_V5.0_fp16.safetensors",
          "type": "hf.stable_diffusion_xl",
          "name": "SG161222/RealVisXL_V5.0",
          "repo_id": "SG161222/RealVisXL_V5.0",
          "path": "RealVisXL_V5.0_fp16.safetensors",
          "size_on_disk": 6938065488,
          "pipeline_tag": "text-to-image",
          "tags": [
            "diffusers",
            "safetensors",
            "license:openrail++",
            "endpoints_compatible",
            "diffusers:StableDiffusionXLPipeline",
            "region:us"
          ],
          "has_model_index": true,
          "downloads": 55543,
          "likes": 121
        },
        {
          "id": "nunchaku-tech/nunchaku-sdxl:svdq-int4_r32-sdxl.safetensors",
          "type": "hf.stable_diffusion_xl",
          "name": "nunchaku-tech/nunchaku-sdxl",
          "repo_id": "nunchaku-tech/nunchaku-sdxl",
          "path": "svdq-int4_r32-sdxl.safetensors",
          "size_on_disk": 2559021560,
          "pipeline_tag": "text-to-image",
          "tags": [
            "diffusers",
            "text-to-image",
            "SVDQuant",
            "SDXL",
            "Diffusion",
            "Quantization",
            "stable-diffusion",
            "en",
            "dataset:mit-han-lab/svdquant-datasets",
            "arxiv:2411.05007",
            "base_model:stabilityai/stable-diffusion-xl-base-1.0",
            "base_model:quantized:stabilityai/stable-diffusion-xl-base-1.0",
            "license:openrail++",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 819,
          "likes": 26
        },
        {
          "id": "nunchaku-tech/nunchaku-sdxl-turbo:svdq-int4_r32-sdxl-turbo.safetensors",
          "type": "hf.stable_diffusion_xl",
          "name": "nunchaku-tech/nunchaku-sdxl-turbo",
          "repo_id": "nunchaku-tech/nunchaku-sdxl-turbo",
          "path": "svdq-int4_r32-sdxl-turbo.safetensors",
          "size_on_disk": 2559021776,
          "pipeline_tag": "text-to-image",
          "tags": [
            "diffusers",
            "text-to-image",
            "SVDQuant",
            "SDXL-Turbo",
            "Diffusion",
            "Quantization",
            "stable-diffusion",
            "en",
            "dataset:mit-han-lab/svdquant-datasets",
            "arxiv:2411.05007",
            "base_model:stabilityai/stable-diffusion-xl-base-1.0",
            "base_model:quantized:stabilityai/stable-diffusion-xl-base-1.0",
            "license:other",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 746,
          "likes": 10
        }
      ],
      "basic_fields": [
        "model",
        "quantization",
        "prompt",
        "width",
        "height",
        "image",
        "mask_image",
        "strength"
      ]
    },
    {
      "title": "Swin2SR",
      "description": "Performs image super-resolution using the Swin2SR model.\n    image, super-resolution, enhancement, huggingface\n\n    Use cases:\n    - Enhance low-resolution images\n    - Improve image quality for printing or display\n    - Upscale images for better detail",
      "namespace": "huggingface.image_to_image",
      "node_type": "huggingface.image_to_image.Swin2SR",
      "properties": [
        {
          "name": "image",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Input Image",
          "description": "The input image to transform"
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt to guide the image transformation (if applicable)"
        },
        {
          "name": "model",
          "type": {
            "type": "hf.image_to_image"
          },
          "default": {
            "type": "hf.image_to_image",
            "repo_id": "",
            "path": null,
            "variant": null,
            "allow_patterns": null,
            "ignore_patterns": null
          },
          "title": "Model ID on Huggingface",
          "description": "The model ID to use for image super-resolution"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "recommended_models": [
        {
          "id": "caidas/swin2SR-classical-sr-x2-64",
          "type": "hf.image_to_image",
          "name": "caidas/swin2SR-classical-sr-x2-64",
          "repo_id": "caidas/swin2SR-classical-sr-x2-64",
          "allow_patterns": [
            "README.md",
            "*.safetensors",
            "*.json",
            "**/*.json"
          ],
          "size_on_disk": 48462226,
          "pipeline_tag": "image-to-image",
          "tags": [
            "transformers",
            "pytorch",
            "safetensors",
            "swin2sr",
            "image-to-image",
            "vision",
            "arxiv:2209.11345",
            "license:apache-2.0",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 65409,
          "likes": 37
        },
        {
          "id": "caidas/swin2SR-classical-sr-x4-64",
          "type": "hf.image_to_image",
          "name": "caidas/swin2SR-classical-sr-x4-64",
          "repo_id": "caidas/swin2SR-classical-sr-x4-64",
          "allow_patterns": [
            "README.md",
            "*.safetensors",
            "*.json",
            "**/*.json"
          ],
          "size_on_disk": 49053290,
          "pipeline_tag": "image-to-image",
          "tags": [
            "transformers",
            "pytorch",
            "safetensors",
            "swin2sr",
            "image-to-image",
            "vision",
            "arxiv:2209.11345",
            "license:apache-2.0",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 8304,
          "likes": 3
        },
        {
          "id": "caidas/swin2SR-lightweight-x2-64",
          "type": "hf.image_to_image",
          "name": "caidas/swin2SR-lightweight-x2-64",
          "repo_id": "caidas/swin2SR-lightweight-x2-64",
          "allow_patterns": [
            "README.md",
            "*.safetensors",
            "*.json",
            "**/*.json"
          ],
          "size_on_disk": 4084507,
          "pipeline_tag": "image-to-image",
          "tags": [
            "transformers",
            "pytorch",
            "safetensors",
            "swin2sr",
            "image-to-image",
            "vision",
            "arxiv:2209.11345",
            "license:apache-2.0",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 2852,
          "likes": 13
        },
        {
          "id": "caidas/swin2SR-compressed-sr-x4-48",
          "type": "hf.image_to_image",
          "name": "caidas/swin2SR-compressed-sr-x4-48",
          "repo_id": "caidas/swin2SR-compressed-sr-x4-48",
          "allow_patterns": [
            "README.md",
            "*.safetensors",
            "*.json",
            "**/*.json"
          ],
          "size_on_disk": 49075149,
          "pipeline_tag": "image-to-image",
          "tags": [
            "transformers",
            "pytorch",
            "safetensors",
            "swin2sr",
            "image-to-image",
            "vision",
            "arxiv:2209.11345",
            "license:apache-2.0",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 196,
          "likes": 3
        },
        {
          "id": "caidas/swin2SR-realworld-sr-x4-64-bsrgan-psnr",
          "type": "hf.image_to_image",
          "name": "caidas/swin2SR-realworld-sr-x4-64-bsrgan-psnr",
          "repo_id": "caidas/swin2SR-realworld-sr-x4-64-bsrgan-psnr",
          "allow_patterns": [
            "README.md",
            "*.bin",
            "*.json",
            "**/*.json"
          ],
          "size_on_disk": 48461718,
          "pipeline_tag": "image-to-image",
          "tags": [
            "transformers",
            "pytorch",
            "swin2sr",
            "image-to-image",
            "vision",
            "arxiv:2209.11345",
            "license:apache-2.0",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 5685,
          "likes": 16
        }
      ],
      "basic_fields": [
        "image",
        "prompt",
        "model"
      ]
    },
    {
      "title": "VAE Decode",
      "description": "Decodes latents into an image using a VAE.\n    tensor (TorchTensor) -> image",
      "namespace": "huggingface.image_to_image",
      "node_type": "huggingface.image_to_image.VAEDecode",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.vae"
          },
          "default": {
            "type": "hf.vae",
            "repo_id": "",
            "path": null,
            "variant": null,
            "allow_patterns": null,
            "ignore_patterns": null
          },
          "title": "Model",
          "description": "The VAE model to use."
        },
        {
          "name": "latents",
          "type": {
            "type": "torch_tensor"
          },
          "default": {
            "type": "torch_tensor",
            "value": null,
            "dtype": "<i8",
            "shape": [
              1
            ]
          },
          "title": "Latents",
          "description": "Latent tensor to decode."
        },
        {
          "name": "scale_factor",
          "type": {
            "type": "float"
          },
          "default": 0.18215,
          "title": "Scale Factor",
          "description": "Scaling factor used for encoding (inverse is applied before decode)",
          "min": 0.0,
          "max": 10.0
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "recommended_models": [
        {
          "id": "stabilityai/sd-vae-ft-mse",
          "type": "hf.vae",
          "name": "stabilityai/sd-vae-ft-mse",
          "repo_id": "stabilityai/sd-vae-ft-mse",
          "size_on_disk": 669359340,
          "tags": [
            "diffusers",
            "safetensors",
            "stable-diffusion",
            "stable-diffusion-diffusers",
            "license:mit",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 171246,
          "likes": 398
        },
        {
          "id": "stabilityai/sd-vae-ft-ema",
          "type": "hf.vae",
          "name": "stabilityai/sd-vae-ft-ema",
          "repo_id": "stabilityai/sd-vae-ft-ema",
          "size_on_disk": 669359341,
          "tags": [
            "diffusers",
            "safetensors",
            "stable-diffusion",
            "stable-diffusion-diffusers",
            "license:mit",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 19698,
          "likes": 132
        },
        {
          "id": "stabilityai/sdxl-vae",
          "type": "hf.vae",
          "name": "stabilityai/sdxl-vae",
          "repo_id": "stabilityai/sdxl-vae",
          "size_on_disk": 1004001843,
          "tags": [
            "diffusers",
            "safetensors",
            "stable-diffusion",
            "stable-diffusion-diffusers",
            "arxiv:2112.10752",
            "license:mit",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 355337,
          "likes": 716
        },
        {
          "id": "madebyollin/sdxl-vae-fp16-fix",
          "type": "hf.vae",
          "name": "madebyollin/sdxl-vae-fp16-fix",
          "repo_id": "madebyollin/sdxl-vae-fp16-fix",
          "size_on_disk": 1343646154,
          "tags": [
            "diffusers",
            "safetensors",
            "stable-diffusion",
            "stable-diffusion-diffusers",
            "license:mit",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 298332,
          "likes": 598
        }
      ],
      "basic_fields": [
        "model",
        "latents",
        "scale_factor"
      ]
    },
    {
      "title": "VAE Encode",
      "description": "Encodes an image into latents using a VAE.\n    image -> tensor (TorchTensor)",
      "namespace": "huggingface.image_to_image",
      "node_type": "huggingface.image_to_image.VAEEncode",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.vae"
          },
          "default": {
            "type": "hf.vae",
            "repo_id": "",
            "path": null,
            "variant": null,
            "allow_patterns": null,
            "ignore_patterns": null
          },
          "title": "Model",
          "description": "The VAE model to use."
        },
        {
          "name": "image",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image",
          "description": "Input image to encode."
        },
        {
          "name": "scale_factor",
          "type": {
            "type": "float"
          },
          "default": 0.18215,
          "title": "Scale Factor",
          "description": "Scaling factor applied to latents (e.g., 0.18215 for SD15)",
          "min": 0.0,
          "max": 10.0
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "torch_tensor"
          },
          "name": "output"
        }
      ],
      "recommended_models": [
        {
          "id": "stabilityai/sd-vae-ft-mse",
          "type": "hf.vae",
          "name": "stabilityai/sd-vae-ft-mse",
          "repo_id": "stabilityai/sd-vae-ft-mse",
          "size_on_disk": 669359340,
          "tags": [
            "diffusers",
            "safetensors",
            "stable-diffusion",
            "stable-diffusion-diffusers",
            "license:mit",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 171246,
          "likes": 398
        },
        {
          "id": "stabilityai/sd-vae-ft-ema",
          "type": "hf.vae",
          "name": "stabilityai/sd-vae-ft-ema",
          "repo_id": "stabilityai/sd-vae-ft-ema",
          "size_on_disk": 669359341,
          "tags": [
            "diffusers",
            "safetensors",
            "stable-diffusion",
            "stable-diffusion-diffusers",
            "license:mit",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 19698,
          "likes": 132
        },
        {
          "id": "stabilityai/sdxl-vae",
          "type": "hf.vae",
          "name": "stabilityai/sdxl-vae",
          "repo_id": "stabilityai/sdxl-vae",
          "size_on_disk": 1004001843,
          "tags": [
            "diffusers",
            "safetensors",
            "stable-diffusion",
            "stable-diffusion-diffusers",
            "arxiv:2112.10752",
            "license:mit",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 355337,
          "likes": 716
        },
        {
          "id": "madebyollin/sdxl-vae-fp16-fix",
          "type": "hf.vae",
          "name": "madebyollin/sdxl-vae-fp16-fix",
          "repo_id": "madebyollin/sdxl-vae-fp16-fix",
          "size_on_disk": 1343646154,
          "tags": [
            "diffusers",
            "safetensors",
            "stable-diffusion",
            "stable-diffusion-diffusers",
            "license:mit",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 298332,
          "likes": 598
        }
      ],
      "basic_fields": [
        "model",
        "image",
        "scale_factor"
      ]
    },
    {
      "title": "Sentence Similarity",
      "description": "Generates dense vector embeddings from text for semantic similarity comparisons.\n    text, sentence-similarity, embeddings, NLP, semantic-search\n\n    Use cases:\n    - Compute semantic similarity between sentences or documents\n    - Build semantic search and recommendation engines\n    - Detect duplicate or near-duplicate content\n    - Cluster documents by meaning rather than keywords\n    - Create text embeddings for downstream ML tasks",
      "namespace": "huggingface.sentence_similarity",
      "node_type": "huggingface.sentence_similarity.SentenceSimilarity",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.sentence_similarity"
          },
          "default": {
            "type": "hf.sentence_similarity",
            "repo_id": "",
            "path": null,
            "variant": null,
            "allow_patterns": null,
            "ignore_patterns": null
          },
          "title": "Model",
          "description": "The sentence embedding model. all-mpnet-base-v2 offers high quality; MiniLM variants are faster; BGE-m3 is multilingual."
        },
        {
          "name": "inputs",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Input Text",
          "description": "The text to generate embeddings for. Can be a sentence, paragraph, or short document."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "np_array"
          },
          "name": "output"
        }
      ],
      "recommended_models": [
        {
          "id": "sentence-transformers/all-mpnet-base-v2",
          "type": "hf.sentence_similarity",
          "name": "sentence-transformers/all-mpnet-base-v2",
          "repo_id": "sentence-transformers/all-mpnet-base-v2",
          "allow_patterns": [
            "*.safetensors",
            "*.txt",
            "*,json"
          ],
          "size_on_disk": 438203408,
          "pipeline_tag": "sentence-similarity",
          "tags": [
            "sentence-transformers",
            "pytorch",
            "onnx",
            "safetensors",
            "openvino",
            "mpnet",
            "fill-mask",
            "feature-extraction",
            "sentence-similarity",
            "transformers",
            "text-embeddings-inference",
            "en",
            "dataset:s2orc",
            "dataset:flax-sentence-embeddings/stackexchange_xml",
            "dataset:ms_marco",
            "dataset:gooaq",
            "dataset:yahoo_answers_topics",
            "dataset:code_search_net",
            "dataset:search_qa",
            "dataset:eli5",
            "dataset:snli",
            "dataset:multi_nli",
            "dataset:wikihow",
            "dataset:natural_questions",
            "dataset:trivia_qa",
            "dataset:embedding-data/sentence-compression",
            "dataset:embedding-data/flickr30k-captions",
            "dataset:embedding-data/altlex",
            "dataset:embedding-data/simple-wiki",
            "dataset:embedding-data/QQP",
            "dataset:embedding-data/SPECTER",
            "dataset:embedding-data/PAQ_pairs",
            "dataset:embedding-data/WikiAnswers",
            "arxiv:1904.06472",
            "arxiv:2102.07033",
            "arxiv:2104.08727",
            "arxiv:1704.05179",
            "arxiv:1810.09305",
            "license:apache-2.0",
            "endpoints_compatible",
            "deploy:azure",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 25475439,
          "likes": 1207
        },
        {
          "id": "sentence-transformers/all-MiniLM-L6-v2",
          "type": "hf.sentence_similarity",
          "name": "sentence-transformers/all-MiniLM-L6-v2",
          "repo_id": "sentence-transformers/all-MiniLM-L6-v2",
          "allow_patterns": [
            "*.safetensors",
            "*.txt",
            "*,json"
          ],
          "size_on_disk": 91099884,
          "pipeline_tag": "sentence-similarity",
          "tags": [
            "sentence-transformers",
            "pytorch",
            "tf",
            "rust",
            "onnx",
            "safetensors",
            "openvino",
            "bert",
            "feature-extraction",
            "sentence-similarity",
            "transformers",
            "en",
            "dataset:s2orc",
            "dataset:flax-sentence-embeddings/stackexchange_xml",
            "dataset:ms_marco",
            "dataset:gooaq",
            "dataset:yahoo_answers_topics",
            "dataset:code_search_net",
            "dataset:search_qa",
            "dataset:eli5",
            "dataset:snli",
            "dataset:multi_nli",
            "dataset:wikihow",
            "dataset:natural_questions",
            "dataset:trivia_qa",
            "dataset:embedding-data/sentence-compression",
            "dataset:embedding-data/flickr30k-captions",
            "dataset:embedding-data/altlex",
            "dataset:embedding-data/simple-wiki",
            "dataset:embedding-data/QQP",
            "dataset:embedding-data/SPECTER",
            "dataset:embedding-data/PAQ_pairs",
            "dataset:embedding-data/WikiAnswers",
            "arxiv:1904.06472",
            "arxiv:2102.07033",
            "arxiv:2104.08727",
            "arxiv:1704.05179",
            "arxiv:1810.09305",
            "license:apache-2.0",
            "text-embeddings-inference",
            "endpoints_compatible",
            "deploy:azure",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 149696739,
          "likes": 4240
        },
        {
          "id": "BAAI/bge-m3",
          "type": "hf.sentence_similarity",
          "name": "BAAI/bge-m3",
          "repo_id": "BAAI/bge-m3",
          "allow_patterns": [
            "*.safetensors",
            "*.txt",
            "*,json"
          ],
          "size_on_disk": 0,
          "pipeline_tag": "sentence-similarity",
          "tags": [
            "sentence-transformers",
            "pytorch",
            "onnx",
            "xlm-roberta",
            "feature-extraction",
            "sentence-similarity",
            "arxiv:2402.03216",
            "arxiv:2004.04906",
            "arxiv:2106.14807",
            "arxiv:2107.05720",
            "arxiv:2004.12832",
            "license:mit",
            "text-embeddings-inference",
            "endpoints_compatible",
            "deploy:azure",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 8209888,
          "likes": 2575
        },
        {
          "id": "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
          "type": "hf.sentence_similarity",
          "name": "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
          "repo_id": "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
          "allow_patterns": [
            "*.safetensors",
            "*.txt",
            "*,json"
          ],
          "size_on_disk": 470641600,
          "pipeline_tag": "sentence-similarity",
          "tags": [
            "sentence-transformers",
            "pytorch",
            "tf",
            "onnx",
            "safetensors",
            "openvino",
            "bert",
            "feature-extraction",
            "sentence-similarity",
            "transformers",
            "multilingual",
            "ar",
            "bg",
            "ca",
            "cs",
            "da",
            "de",
            "el",
            "en",
            "es",
            "et",
            "fa",
            "fi",
            "fr",
            "gl",
            "gu",
            "he",
            "hi",
            "hr",
            "hu",
            "hy",
            "id",
            "it",
            "ja",
            "ka",
            "ko",
            "ku",
            "lt",
            "lv",
            "mk",
            "mn",
            "mr",
            "ms",
            "my",
            "nb",
            "nl",
            "pl",
            "pt",
            "ro",
            "ru",
            "sk",
            "sl",
            "sq",
            "sr",
            "sv",
            "th",
            "tr",
            "uk",
            "ur",
            "vi",
            "arxiv:1908.10084",
            "license:apache-2.0",
            "text-embeddings-inference",
            "endpoints_compatible",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 16724510,
          "likes": 1075
        }
      ],
      "basic_fields": [
        "model",
        "inputs"
      ]
    },
    {
      "title": "Feature Extraction",
      "description": "Extracts dense vector embeddings from text using transformer models for downstream ML tasks.\n    text, feature-extraction, embeddings, NLP, semantic-search\n\n    Use cases:\n    - Compute text embeddings for semantic similarity comparisons\n    - Cluster documents by meaning rather than keywords\n    - Generate input features for machine learning classifiers\n    - Build semantic search engines and recommendation systems\n    - Create vector databases for retrieval-augmented generation (RAG)",
      "namespace": "huggingface.feature_extraction",
      "node_type": "huggingface.feature_extraction.FeatureExtraction",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.feature_extraction"
          },
          "default": {
            "type": "hf.feature_extraction",
            "repo_id": "",
            "path": null,
            "variant": null,
            "allow_patterns": null,
            "ignore_patterns": null
          },
          "title": "Model",
          "description": "The embedding model to use. mxbai-embed-large-v1 and BGE models offer excellent quality; smaller models trade accuracy for speed."
        },
        {
          "name": "inputs",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Input Text",
          "description": "The text to extract embeddings from. Can be a sentence, paragraph, or document."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "np_array"
          },
          "name": "output"
        }
      ],
      "recommended_models": [
        {
          "id": "mixedbread-ai/mxbai-embed-large-v1",
          "type": "hf.feature_extraction",
          "name": "mixedbread-ai/mxbai-embed-large-v1",
          "repo_id": "mixedbread-ai/mxbai-embed-large-v1",
          "allow_patterns": [
            "*.safetensors",
            "*.txt",
            "*,json"
          ],
          "size_on_disk": 670559900,
          "pipeline_tag": "feature-extraction",
          "tags": [
            "sentence-transformers",
            "onnx",
            "safetensors",
            "openvino",
            "gguf",
            "bert",
            "feature-extraction",
            "mteb",
            "transformers.js",
            "transformers",
            "en",
            "arxiv:2309.12871",
            "license:apache-2.0",
            "model-index",
            "text-embeddings-inference",
            "endpoints_compatible",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 2231823,
          "likes": 746
        },
        {
          "id": "BAAI/bge-base-en-v1.5",
          "type": "hf.feature_extraction",
          "name": "BAAI/bge-base-en-v1.5",
          "repo_id": "BAAI/bge-base-en-v1.5",
          "allow_patterns": [
            "*.safetensors",
            "*.txt",
            "*,json"
          ],
          "size_on_disk": 438187020,
          "pipeline_tag": "feature-extraction",
          "tags": [
            "sentence-transformers",
            "pytorch",
            "onnx",
            "safetensors",
            "bert",
            "feature-extraction",
            "sentence-similarity",
            "transformers",
            "mteb",
            "en",
            "arxiv:2401.03462",
            "arxiv:2312.15503",
            "arxiv:2311.13534",
            "arxiv:2310.07554",
            "arxiv:2309.07597",
            "license:mit",
            "model-index",
            "text-embeddings-inference",
            "endpoints_compatible",
            "deploy:azure",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 3408250,
          "likes": 385
        },
        {
          "id": "BAAI/bge-large-en-v1.5",
          "type": "hf.feature_extraction",
          "name": "BAAI/bge-large-en-v1.5",
          "repo_id": "BAAI/bge-large-en-v1.5",
          "allow_patterns": [
            "*.safetensors",
            "*.txt",
            "*,json"
          ],
          "size_on_disk": 1340848124,
          "pipeline_tag": "feature-extraction",
          "tags": [
            "sentence-transformers",
            "pytorch",
            "onnx",
            "safetensors",
            "bert",
            "feature-extraction",
            "sentence-similarity",
            "transformers",
            "mteb",
            "en",
            "arxiv:2401.03462",
            "arxiv:2312.15503",
            "arxiv:2311.13534",
            "arxiv:2310.07554",
            "arxiv:2309.07597",
            "license:mit",
            "model-index",
            "text-embeddings-inference",
            "endpoints_compatible",
            "deploy:azure",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 3624816,
          "likes": 607
        }
      ],
      "basic_fields": [
        "model",
        "inputs"
      ]
    },
    {
      "title": "Depth Estimation",
      "description": "Generates depth maps from single RGB images using monocular depth estimation models.\n    image, depth-estimation, 3D, huggingface, computer-vision\n\n    Use cases:\n    - Create depth maps for 3D modeling and scene reconstruction\n    - Enable augmented reality applications with depth awareness\n    - Improve robotic navigation and obstacle detection\n    - Enhance scene understanding in autonomous vehicles\n    - Generate depth-based visual effects for images and video",
      "namespace": "huggingface.depth_estimation",
      "node_type": "huggingface.depth_estimation.DepthEstimation",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.depth_estimation"
          },
          "default": {
            "type": "hf.depth_estimation",
            "repo_id": "LiheYoung/depth-anything-base-hf",
            "path": null,
            "variant": null,
            "allow_patterns": null,
            "ignore_patterns": null
          },
          "title": "Model",
          "description": "The depth estimation model to use. Depth-Anything V2 models offer state-of-the-art accuracy; DPT-large is a reliable alternative."
        },
        {
          "name": "image",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image",
          "description": "The input RGB image to estimate depth from. Any standard image format is supported."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "recommended_models": [
        {
          "id": "depth-anything/Depth-Anything-V2-Small-hf",
          "type": "hf.depth_estimation",
          "name": "depth-anything/Depth-Anything-V2-Small-hf",
          "repo_id": "depth-anything/Depth-Anything-V2-Small-hf",
          "size_on_disk": 99181304,
          "pipeline_tag": "depth-estimation",
          "tags": [
            "transformers",
            "safetensors",
            "depth_anything",
            "depth-estimation",
            "depth",
            "relative depth",
            "arxiv:2406.09414",
            "arxiv:2401.10891",
            "license:apache-2.0",
            "endpoints_compatible",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 654825,
          "likes": 25
        },
        {
          "id": "depth-anything/Depth-Anything-V2-Base-hf",
          "type": "hf.depth_estimation",
          "name": "depth-anything/Depth-Anything-V2-Base-hf",
          "repo_id": "depth-anything/Depth-Anything-V2-Base-hf",
          "size_on_disk": 389924651,
          "pipeline_tag": "depth-estimation",
          "tags": [
            "transformers",
            "safetensors",
            "depth_anything",
            "depth-estimation",
            "depth",
            "relative depth",
            "arxiv:2406.09414",
            "arxiv:2401.10891",
            "license:cc-by-nc-4.0",
            "endpoints_compatible",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 159633,
          "likes": 2
        },
        {
          "id": "depth-anything/Depth-Anything-V2-Large-hf",
          "type": "hf.depth_estimation",
          "name": "depth-anything/Depth-Anything-V2-Large-hf",
          "repo_id": "depth-anything/Depth-Anything-V2-Large-hf",
          "size_on_disk": 1341331020,
          "pipeline_tag": "depth-estimation",
          "tags": [
            "transformers",
            "safetensors",
            "depth_anything",
            "depth-estimation",
            "depth",
            "relative depth",
            "arxiv:2406.09414",
            "arxiv:2401.10891",
            "license:cc-by-nc-4.0",
            "endpoints_compatible",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 751508,
          "likes": 27
        },
        {
          "id": "Intel/dpt-large",
          "type": "hf.depth_estimation",
          "name": "Intel/dpt-large",
          "repo_id": "Intel/dpt-large",
          "allow_patterns": [
            "README.md",
            "*.safetensors",
            "*.json",
            "**/*.json",
            "txt"
          ],
          "size_on_disk": 1367465032,
          "pipeline_tag": "depth-estimation",
          "tags": [
            "transformers",
            "pytorch",
            "safetensors",
            "dpt",
            "depth-estimation",
            "vision",
            "arxiv:2103.13413",
            "license:apache-2.0",
            "model-index",
            "endpoints_compatible",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 73017,
          "likes": 197
        }
      ],
      "basic_fields": [
        "model",
        "image"
      ]
    },
    {
      "title": "Bark",
      "description": "Generates realistic multilingual speech and non-verbal audio from text using Suno's Bark model.\n    tts, audio, speech, huggingface, multilingual, voice-synthesis\n\n    Use cases:\n    - Create natural-sounding voice content for apps and videos\n    - Generate multilingual speech for global applications\n    - Produce expressive speech with emotions (laughing, sighing, crying)\n    - Add realistic voice to chatbots and virtual assistants\n    - Create audio content with background music and sound effects",
      "namespace": "huggingface.text_to_speech",
      "node_type": "huggingface.text_to_speech.Bark",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.text_to_speech"
          },
          "default": {
            "type": "hf.text_to_speech",
            "repo_id": "",
            "path": null,
            "variant": null,
            "allow_patterns": null,
            "ignore_patterns": null
          },
          "title": "Model",
          "description": "The Bark model variant. bark-small is faster; bark offers higher quality. Both support multilingual output."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Text Prompt",
          "description": "The text to convert to speech. Can include non-verbal markers like [laughs] or [sighs]."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "recommended_models": [
        {
          "id": "suno/bark",
          "type": "hf.text_to_speech",
          "name": "suno/bark",
          "repo_id": "suno/bark",
          "allow_patterns": [
            "*.bin",
            "*.json",
            "*.txt"
          ],
          "size_on_disk": 4490634036,
          "pipeline_tag": "text-to-speech",
          "tags": [
            "transformers",
            "pytorch",
            "bark",
            "text-to-audio",
            "audio",
            "text-to-speech",
            "en",
            "de",
            "es",
            "fr",
            "hi",
            "it",
            "ja",
            "ko",
            "pl",
            "pt",
            "ru",
            "tr",
            "zh",
            "license:mit",
            "endpoints_compatible",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 18837,
          "likes": 1485
        },
        {
          "id": "suno/bark-small",
          "type": "hf.text_to_speech",
          "name": "suno/bark-small",
          "repo_id": "suno/bark-small",
          "allow_patterns": [
            "*.bin",
            "*.json",
            "*.txt"
          ],
          "size_on_disk": 1680654085,
          "pipeline_tag": "text-to-speech",
          "tags": [
            "transformers",
            "pytorch",
            "bark",
            "text-to-audio",
            "audio",
            "text-to-speech",
            "en",
            "de",
            "es",
            "fr",
            "hi",
            "it",
            "ja",
            "ko",
            "pl",
            "pt",
            "ru",
            "tr",
            "zh",
            "license:mit",
            "endpoints_compatible",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 19448,
          "likes": 247
        }
      ],
      "basic_fields": [
        "model",
        "prompt"
      ]
    },
    {
      "title": "Kokoro TTS",
      "description": "Generates high-quality speech from text using the lightweight Kokoro TTS model.\n    tts, audio, speech, huggingface, kokoro, multilingual, voice\n\n    Use cases:\n    - Generate natural-sounding speech for applications and assistants\n    - Create voice content with multiple voice options and styles\n    - Build low-latency TTS systems for real-time applications\n    - Produce multilingual speech in various languages\n    - Generate narration for videos, presentations, and e-learning\n\n    **Note:** Kokoro is a fast, lightweight model (~82M params) with Apache-2.0 license.\n    See https://huggingface.co/hexgrad/Kokoro-82M for voice samples.",
      "namespace": "huggingface.text_to_speech",
      "node_type": "huggingface.text_to_speech.KokoroTTS",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.text_to_speech"
          },
          "default": {
            "type": "hf.text_to_speech",
            "repo_id": "hexgrad/Kokoro-82M",
            "path": null,
            "variant": null,
            "allow_patterns": null,
            "ignore_patterns": null
          },
          "title": "Model",
          "description": "The Kokoro model repository."
        },
        {
          "name": "text",
          "type": {
            "type": "str"
          },
          "default": "Hello from Kokoro.",
          "title": "Text",
          "description": "The text to convert to speech."
        },
        {
          "name": "lang_code",
          "type": {
            "type": "enum",
            "values": [
              "a",
              "b",
              "e",
              "f",
              "h",
              "i",
              "p",
              "j",
              "z",
              "k",
              "r",
              "t",
              "v",
              "a",
              "g",
              "p",
              "r",
              "u"
            ],
            "type_name": "nodetool.nodes.huggingface.text_to_speech.KokoroTTS.LanguageCode"
          },
          "default": "a",
          "title": "Language",
          "description": "Language for pronunciation. Choose based on your text's language."
        },
        {
          "name": "voice",
          "type": {
            "type": "enum",
            "values": [
              "af_alloy",
              "af_aoede",
              "af_bella",
              "af_heart",
              "af_jessica",
              "af_kore",
              "af_nicole",
              "af_nova",
              "af_river",
              "af_sarah",
              "af_sky",
              "am_adam",
              "am_echo",
              "am_eric",
              "am_fenrir",
              "am_liam",
              "am_michael",
              "am_onyx",
              "am_puck",
              "am_santa",
              "bf_alice",
              "bf_emma",
              "bf_isabella",
              "bf_lily",
              "bm_daniel",
              "bm_fable",
              "bm_george",
              "bm_lewis",
              "ef_dora",
              "em_alex",
              "em_santa",
              "ff_siwis",
              "hf_alpha",
              "hf_beta",
              "hm_omega",
              "hm_psi",
              "if_sara",
              "im_nicola",
              "jf_alpha",
              "jf_gongitsune",
              "jf_nezumi",
              "jf_tebukuro",
              "jm_kumo",
              "pf_dora",
              "pm_alex",
              "pm_santa",
              "zf_xiaobei",
              "zf_xiaoni",
              "zf_xiaoxiao",
              "zf_xiaoyi"
            ],
            "type_name": "nodetool.nodes.huggingface.text_to_speech.KokoroTTS.Voice"
          },
          "default": "af_heart",
          "title": "Voice",
          "description": "The voice to use. af_* = American female, am_* = American male, bf_* = British female, etc."
        },
        {
          "name": "speed",
          "type": {
            "type": "float"
          },
          "default": 1.0,
          "title": "Speed",
          "description": "Speech speed multiplier: 0.5 = half speed, 1.0 = normal, 2.0 = double speed.",
          "min": 0.5,
          "max": 2.0
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "audio"
        },
        {
          "type": {
            "type": "chunk"
          },
          "name": "chunk"
        }
      ],
      "recommended_models": [
        {
          "id": "hexgrad/Kokoro-82M",
          "type": "hf.text_to_speech",
          "name": "hexgrad/Kokoro-82M",
          "repo_id": "hexgrad/Kokoro-82M",
          "allow_patterns": [
            "*.json",
            "*.pth",
            "voices/*.pt"
          ],
          "size_on_disk": 355479286,
          "pipeline_tag": "text-to-speech",
          "tags": [
            "text-to-speech",
            "en",
            "arxiv:2306.07691",
            "arxiv:2203.02395",
            "base_model:yl4579/StyleTTS2-LJSpeech",
            "base_model:finetune:yl4579/StyleTTS2-LJSpeech",
            "doi:10.57967/hf/4329",
            "license:apache-2.0",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 3637205,
          "likes": 5414
        }
      ],
      "basic_fields": [
        "model",
        "text",
        "lang_code",
        "voice",
        "speed"
      ],
      "is_streaming_output": true
    },
    {
      "title": "Text To Speech",
      "description": "Converts text to speech using various Hugging Face TTS models for multiple languages.\n    tts, audio, speech, huggingface, voice, speak\n\n    Use cases:\n    - Generate speech from text for applications and websites\n    - Create voice content for virtual assistants and chatbots\n    - Produce audio narrations for videos and presentations\n    - Build accessibility features with text-to-speech output\n    - Generate multilingual speech using MMS-TTS models",
      "namespace": "huggingface.text_to_speech",
      "node_type": "huggingface.text_to_speech.TextToSpeech",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.text_to_speech"
          },
          "default": {
            "type": "hf.text_to_speech",
            "repo_id": "",
            "path": null,
            "variant": null,
            "allow_patterns": null,
            "ignore_patterns": null
          },
          "title": "Model",
          "description": "The TTS model to use. facebook/mms-tts-* models support many languages (eng=English, fra=French, deu=German, kor=Korean, etc.)."
        },
        {
          "name": "text",
          "type": {
            "type": "str"
          },
          "default": "Hello, this is a test of the text-to-speech system.",
          "title": "Input Text",
          "description": "The text to convert to speech."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "recommended_models": [
        {
          "id": "facebook/mms-tts-eng",
          "type": "hf.text_to_speech",
          "name": "facebook/mms-tts-eng",
          "repo_id": "facebook/mms-tts-eng",
          "allow_patterns": [
            "*.bin",
            "*.json",
            "*.txt"
          ],
          "size_on_disk": 145391110,
          "pipeline_tag": "text-to-speech",
          "tags": [
            "transformers",
            "pytorch",
            "safetensors",
            "vits",
            "text-to-audio",
            "mms",
            "text-to-speech",
            "arxiv:2305.13516",
            "license:cc-by-nc-4.0",
            "endpoints_compatible",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 20060,
          "likes": 160
        },
        {
          "id": "facebook/mms-tts-kor",
          "type": "hf.text_to_speech",
          "name": "facebook/mms-tts-kor",
          "repo_id": "facebook/mms-tts-kor",
          "allow_patterns": [
            "*.bin",
            "*.json",
            "*.txt"
          ],
          "size_on_disk": 145380980,
          "pipeline_tag": "text-to-speech",
          "tags": [
            "transformers",
            "pytorch",
            "safetensors",
            "vits",
            "text-to-audio",
            "mms",
            "text-to-speech",
            "arxiv:2305.13516",
            "license:cc-by-nc-4.0",
            "endpoints_compatible",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 2614,
          "likes": 14
        },
        {
          "id": "facebook/mms-tts-fra",
          "type": "hf.text_to_speech",
          "name": "facebook/mms-tts-fra",
          "repo_id": "facebook/mms-tts-fra",
          "allow_patterns": [
            "*.bin",
            "*.json",
            "*.txt"
          ],
          "size_on_disk": 145395800,
          "pipeline_tag": "text-to-speech",
          "tags": [
            "transformers",
            "pytorch",
            "safetensors",
            "vits",
            "text-to-audio",
            "mms",
            "text-to-speech",
            "arxiv:2305.13516",
            "license:cc-by-nc-4.0",
            "endpoints_compatible",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 2662,
          "likes": 14
        },
        {
          "id": "facebook/mms-tts-deu",
          "type": "hf.text_to_speech",
          "name": "facebook/mms-tts-deu",
          "repo_id": "facebook/mms-tts-deu",
          "allow_patterns": [
            "*.bin",
            "*.json",
            "*.txt"
          ],
          "size_on_disk": 145396569,
          "pipeline_tag": "text-to-speech",
          "tags": [
            "transformers",
            "pytorch",
            "safetensors",
            "vits",
            "text-to-audio",
            "mms",
            "text-to-speech",
            "arxiv:2305.13516",
            "license:cc-by-nc-4.0",
            "endpoints_compatible",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 1225,
          "likes": 14
        }
      ],
      "basic_fields": [
        "model",
        "text"
      ]
    },
    {
      "title": "Base Qwen VL",
      "description": "Base class for Qwen vision-language models supporting image and video understanding.\n    image, video, multimodal, VLM, Qwen",
      "namespace": "huggingface.image_text_to_text",
      "node_type": "huggingface.image_text_to_text.BaseQwenVL",
      "properties": [
        {
          "name": "image",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Input Image",
          "description": "An image to analyze. Leave empty if providing video input."
        },
        {
          "name": "video",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Input Video",
          "description": "A video to analyze. Leave empty if providing image input."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "Describe this image.",
          "title": "Prompt",
          "description": "Your question or instruction about the visual content."
        },
        {
          "name": "min_pixels",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Min Pixels",
          "description": "Minimum pixel count for image resizing. Use 0 for automatic sizing."
        },
        {
          "name": "max_pixels",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Max Pixels",
          "description": "Maximum pixel count for image resizing. Use 0 for automatic sizing."
        },
        {
          "name": "max_new_tokens",
          "type": {
            "type": "int"
          },
          "default": 128,
          "title": "Max New Tokens",
          "description": "Maximum length of the generated response in tokens.",
          "min": 1.0
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "str"
          },
          "name": "text"
        },
        {
          "type": {
            "type": "chunk"
          },
          "name": "chunk"
        }
      ],
      "basic_fields": [
        "model",
        "quantization",
        "image",
        "prompt"
      ],
      "is_streaming_output": true
    },
    {
      "title": "Image Text To Text",
      "description": "Generates text responses based on an image and text prompt using vision-language models.\n    image, text, visual-question-answering, multimodal, VLM, captioning\n\n    Use cases:\n    - Answer questions about image content with detailed explanations\n    - Generate comprehensive image descriptions and captions\n    - Extract structured information (objects, text, layout) from images\n    - Perform OCR-free document understanding via natural language\n    - Build multi-turn visual conversations and assistants",
      "namespace": "huggingface.image_text_to_text",
      "node_type": "huggingface.image_text_to_text.ImageTextToText",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.image_text_to_text"
          },
          "default": {
            "type": "hf.image_text_to_text",
            "repo_id": "HuggingFaceTB/SmolVLM-Instruct",
            "path": null,
            "variant": null,
            "allow_patterns": null,
            "ignore_patterns": null
          },
          "title": "Model",
          "description": "The vision-language model to use. SmolVLM is lightweight; LLaVA variants offer different capability levels."
        },
        {
          "name": "image",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Input Image",
          "description": "The image to analyze and discuss."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "Describe this image.",
          "title": "Prompt",
          "description": "Your question or instruction about the image. Be specific for better results."
        },
        {
          "name": "max_new_tokens",
          "type": {
            "type": "int"
          },
          "default": 256,
          "title": "Max New Tokens",
          "description": "Maximum length of the generated response in tokens.",
          "min": 1.0
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "str"
          },
          "name": "text"
        },
        {
          "type": {
            "type": "chunk"
          },
          "name": "chunk"
        }
      ],
      "recommended_models": [
        {
          "id": "HuggingFaceTB/SmolVLM-Instruct",
          "type": "hf.image_text_to_text",
          "name": "HuggingFaceTB/SmolVLM-Instruct",
          "repo_id": "HuggingFaceTB/SmolVLM-Instruct",
          "size_on_disk": 29534636310,
          "pipeline_tag": "image-text-to-text",
          "tags": [
            "transformers",
            "onnx",
            "safetensors",
            "idefics3",
            "image-to-text",
            "image-text-to-text",
            "conversational",
            "en",
            "dataset:HuggingFaceM4/the_cauldron",
            "dataset:HuggingFaceM4/Docmatix",
            "arxiv:2504.05299",
            "base_model:HuggingFaceTB/SmolLM2-1.7B-Instruct",
            "base_model:quantized:HuggingFaceTB/SmolLM2-1.7B-Instruct",
            "license:apache-2.0",
            "endpoints_compatible",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 51437,
          "likes": 564
        },
        {
          "id": "llava-hf/llava-v1.5-13b",
          "type": "hf.image_text_to_text",
          "name": "llava-hf/llava-v1.5-13b",
          "repo_id": "llava-hf/llava-v1.5-13b"
        },
        {
          "id": "llava-hf/llava-v1.5-7b",
          "type": "hf.image_text_to_text",
          "name": "llava-hf/llava-v1.5-7b",
          "repo_id": "llava-hf/llava-v1.5-7b"
        },
        {
          "id": "llava-hf/bakLlava-v1-hf",
          "type": "hf.image_text_to_text",
          "name": "llava-hf/bakLlava-v1-hf",
          "repo_id": "llava-hf/bakLlava-v1-hf",
          "size_on_disk": 15744675400,
          "pipeline_tag": "image-text-to-text",
          "tags": [
            "transformers",
            "safetensors",
            "llava",
            "image-to-text",
            "vision",
            "image-text-to-text",
            "conversational",
            "en",
            "dataset:liuhaotian/LLaVA-Instruct-150K",
            "license:llama2",
            "endpoints_compatible",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 2117,
          "likes": 55
        },
        {
          "id": "llava-hf/llava-v1.6-mistral-7b-hf",
          "type": "hf.image_text_to_text",
          "name": "llava-hf/llava-v1.6-mistral-7b-hf",
          "repo_id": "llava-hf/llava-v1.6-mistral-7b-hf",
          "size_on_disk": 15135960322,
          "pipeline_tag": "image-text-to-text",
          "tags": [
            "transformers",
            "safetensors",
            "llava_next",
            "image-to-text",
            "vision",
            "image-text-to-text",
            "conversational",
            "en",
            "arxiv:2310.03744",
            "license:apache-2.0",
            "text-generation-inference",
            "endpoints_compatible",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 372301,
          "likes": 298
        },
        {
          "id": "llava-hf/llava-v1.6-vicuna-7b-hf",
          "type": "hf.image_text_to_text",
          "name": "llava-hf/llava-v1.6-vicuna-7b-hf",
          "repo_id": "llava-hf/llava-v1.6-vicuna-7b-hf",
          "size_on_disk": 14129379559,
          "pipeline_tag": "image-text-to-text",
          "tags": [
            "transformers",
            "safetensors",
            "llava_next",
            "image-to-text",
            "vision",
            "image-text-to-text",
            "conversational",
            "en",
            "arxiv:2310.03744",
            "license:llama2",
            "text-generation-inference",
            "endpoints_compatible",
            "deploy:azure",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 20725,
          "likes": 30
        },
        {
          "id": "llava-hf/llava-v1.6-vicuna-13b-hf",
          "type": "hf.image_text_to_text",
          "name": "llava-hf/llava-v1.6-vicuna-13b-hf",
          "repo_id": "llava-hf/llava-v1.6-vicuna-13b-hf",
          "size_on_disk": 26705533480,
          "pipeline_tag": "image-text-to-text",
          "tags": [
            "transformers",
            "safetensors",
            "llava_next",
            "image-to-text",
            "vision",
            "image-text-to-text",
            "conversational",
            "en",
            "arxiv:2310.03744",
            "license:llama2",
            "text-generation-inference",
            "endpoints_compatible",
            "deploy:azure",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 11371,
          "likes": 22
        },
        {
          "id": "llava-hf/llava-v1.6-34b-hf",
          "type": "hf.image_text_to_text",
          "name": "llava-hf/llava-v1.6-34b-hf",
          "repo_id": "llava-hf/llava-v1.6-34b-hf",
          "size_on_disk": 69505434534,
          "pipeline_tag": "image-text-to-text",
          "tags": [
            "transformers",
            "safetensors",
            "llava_next",
            "image-to-text",
            "vision",
            "image-text-to-text",
            "conversational",
            "en",
            "arxiv:2310.03744",
            "text-generation-inference",
            "endpoints_compatible",
            "deploy:azure",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 3002,
          "likes": 93
        },
        {
          "id": "llava-hf/llama3-llava-next-8b-hf",
          "type": "hf.image_text_to_text",
          "name": "llava-hf/llama3-llava-next-8b-hf",
          "repo_id": "llava-hf/llama3-llava-next-8b-hf",
          "size_on_disk": 16719864294,
          "pipeline_tag": "image-text-to-text",
          "tags": [
            "transformers",
            "safetensors",
            "llava_next",
            "image-to-text",
            "vision",
            "image-text-to-text",
            "conversational",
            "en",
            "license:llama3",
            "text-generation-inference",
            "endpoints_compatible",
            "deploy:azure",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 7192,
          "likes": 46
        },
        {
          "id": "zai-org/GLM-4.6V-Flash",
          "type": "hf.image_text_to_text",
          "name": "zai-org/GLM-4.6V-Flash",
          "repo_id": "zai-org/GLM-4.6V-Flash",
          "size_on_disk": 20605705573,
          "pipeline_tag": "image-text-to-text",
          "tags": [
            "transformers",
            "safetensors",
            "glm4v",
            "any-to-any",
            "image-text-to-text",
            "conversational",
            "zh",
            "en",
            "arxiv:2507.01006",
            "license:mit",
            "endpoints_compatible",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 54050,
          "likes": 387
        }
      ],
      "basic_fields": [
        "model",
        "quantization",
        "image",
        "prompt"
      ],
      "is_streaming_output": true
    },
    {
      "title": "Load Image Text To Text Model",
      "description": "Loads and validates a Hugging Face image-text-to-text model for use in downstream nodes.\n    model-loader, vision-language, multimodal, VLM\n\n    Use cases:\n    - Pre-load vision-language models for image understanding tasks\n    - Validate model availability before running pipelines\n    - Configure model settings for ImageTextToText processing",
      "namespace": "huggingface.image_text_to_text",
      "node_type": "huggingface.image_text_to_text.LoadImageTextToTextModel",
      "properties": [
        {
          "name": "repo_id",
          "type": {
            "type": "str"
          },
          "default": "HuggingFaceTB/SmolVLM-Instruct",
          "title": "Model ID",
          "description": "The Hugging Face repository ID for the vision-language model (e.g., SmolVLM, LLaVA variants)."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "hf.image_text_to_text"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "repo_id"
      ]
    },
    {
      "title": "Qwen 2 5 VL",
      "description": "Analyzes images and videos using Alibaba's Qwen2.5-VL vision-language model.\n    image, video, multimodal, VLM, Qwen, visual-understanding\n\n    Use cases:\n    - Understand visual content including objects, text, charts, and layouts\n    - Comprehend videos with temporal event tracking and scene changes\n    - Localize objects with bounding boxes or point annotations\n    - Generate structured output (JSON, tables) from visual data\n    - Read and interpret documents, diagrams, and UI screenshots\n\n    **Note:** BNB-4bit variants from Unsloth reduce memory usage significantly.",
      "namespace": "huggingface.image_text_to_text",
      "node_type": "huggingface.image_text_to_text.Qwen2_5_VL",
      "properties": [
        {
          "name": "image",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Input Image",
          "description": "An image to analyze. Leave empty if providing video input."
        },
        {
          "name": "video",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Input Video",
          "description": "A video to analyze. Leave empty if providing image input."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "Describe this image.",
          "title": "Prompt",
          "description": "Your question or instruction about the visual content."
        },
        {
          "name": "min_pixels",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Min Pixels",
          "description": "Minimum pixel count for image resizing. Use 0 for automatic sizing."
        },
        {
          "name": "max_pixels",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Max Pixels",
          "description": "Maximum pixel count for image resizing. Use 0 for automatic sizing."
        },
        {
          "name": "max_new_tokens",
          "type": {
            "type": "int"
          },
          "default": 128,
          "title": "Max New Tokens",
          "description": "Maximum length of the generated response in tokens.",
          "min": 1.0
        },
        {
          "name": "model",
          "type": {
            "type": "hf.qwen2_5_vl"
          },
          "default": {
            "type": "hf.qwen2_5_vl",
            "repo_id": "Qwen/Qwen2.5-VL-7B-Instruct",
            "path": null,
            "variant": null,
            "allow_patterns": null,
            "ignore_patterns": null
          },
          "title": "Model",
          "description": "The Qwen2.5-VL model variant. Larger models (32B, 72B) offer better accuracy; BNB-4bit variants reduce memory usage."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "str"
          },
          "name": "text"
        },
        {
          "type": {
            "type": "chunk"
          },
          "name": "chunk"
        }
      ],
      "recommended_models": [
        {
          "id": "Qwen/Qwen2.5-VL-3B-Instruct",
          "type": "hf.qwen2_5_vl",
          "name": "Qwen/Qwen2.5-VL-3B-Instruct",
          "repo_id": "Qwen/Qwen2.5-VL-3B-Instruct",
          "size_on_disk": 7520919614,
          "pipeline_tag": "image-text-to-text",
          "tags": [
            "transformers",
            "safetensors",
            "qwen2_5_vl",
            "image-to-text",
            "multimodal",
            "image-text-to-text",
            "conversational",
            "en",
            "arxiv:2309.00071",
            "arxiv:2409.12191",
            "arxiv:2308.12966",
            "text-generation-inference",
            "endpoints_compatible",
            "deploy:azure",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 6776295,
          "likes": 575
        },
        {
          "id": "Qwen/Qwen2.5-VL-7B-Instruct",
          "type": "hf.qwen2_5_vl",
          "name": "Qwen/Qwen2.5-VL-7B-Instruct",
          "repo_id": "Qwen/Qwen2.5-VL-7B-Instruct",
          "size_on_disk": 16595981281,
          "pipeline_tag": "image-text-to-text",
          "tags": [
            "transformers",
            "safetensors",
            "qwen2_5_vl",
            "image-to-text",
            "multimodal",
            "image-text-to-text",
            "conversational",
            "en",
            "arxiv:2309.00071",
            "arxiv:2409.12191",
            "arxiv:2308.12966",
            "license:apache-2.0",
            "text-generation-inference",
            "endpoints_compatible",
            "deploy:azure",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 3129340,
          "likes": 1390
        },
        {
          "id": "Qwen/Qwen2.5-VL-32B-Instruct",
          "type": "hf.qwen2_5_vl",
          "name": "Qwen/Qwen2.5-VL-32B-Instruct",
          "repo_id": "Qwen/Qwen2.5-VL-32B-Instruct",
          "size_on_disk": 68299245118,
          "pipeline_tag": "image-text-to-text",
          "tags": [
            "transformers",
            "safetensors",
            "qwen2_5_vl",
            "image-to-text",
            "multimodal",
            "image-text-to-text",
            "conversational",
            "en",
            "arxiv:2309.00071",
            "arxiv:2502.13923",
            "license:apache-2.0",
            "text-generation-inference",
            "endpoints_compatible",
            "deploy:azure",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 201710,
          "likes": 469
        },
        {
          "id": "Qwen/Qwen2.5-VL-72B-Instruct",
          "type": "hf.qwen2_5_vl",
          "name": "Qwen/Qwen2.5-VL-72B-Instruct",
          "repo_id": "Qwen/Qwen2.5-VL-72B-Instruct",
          "size_on_disk": 146833336536,
          "pipeline_tag": "image-text-to-text",
          "tags": [
            "transformers",
            "safetensors",
            "qwen2_5_vl",
            "image-to-text",
            "multimodal",
            "image-text-to-text",
            "conversational",
            "en",
            "arxiv:2309.00071",
            "arxiv:2409.12191",
            "arxiv:2308.12966",
            "license:other",
            "text-generation-inference",
            "endpoints_compatible",
            "deploy:azure",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 111373,
          "likes": 571
        },
        {
          "id": "unsloth/Qwen2.5-VL-3B-Instruct-unsloth-bnb-4bit",
          "type": "hf.qwen2_5_vl",
          "name": "unsloth/Qwen2.5-VL-3B-Instruct-unsloth-bnb-4bit",
          "repo_id": "unsloth/Qwen2.5-VL-3B-Instruct-unsloth-bnb-4bit",
          "size_on_disk": 3809425192,
          "pipeline_tag": "image-text-to-text",
          "tags": [
            "transformers",
            "safetensors",
            "qwen2_5_vl",
            "image-to-text",
            "multimodal",
            "unsloth",
            "image-text-to-text",
            "conversational",
            "en",
            "arxiv:2309.00071",
            "arxiv:2409.12191",
            "arxiv:2308.12966",
            "base_model:Qwen/Qwen2.5-VL-3B-Instruct",
            "base_model:quantized:Qwen/Qwen2.5-VL-3B-Instruct",
            "text-generation-inference",
            "endpoints_compatible",
            "4-bit",
            "bitsandbytes",
            "deploy:azure",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 5971,
          "likes": 14
        },
        {
          "id": "unsloth/Qwen2.5-VL-3B-Instruct-bnb-4bit",
          "type": "hf.qwen2_5_vl",
          "name": "unsloth/Qwen2.5-VL-3B-Instruct-bnb-4bit",
          "repo_id": "unsloth/Qwen2.5-VL-3B-Instruct-bnb-4bit",
          "size_on_disk": 2473284498,
          "pipeline_tag": "image-text-to-text",
          "tags": [
            "transformers",
            "safetensors",
            "qwen2_5_vl",
            "image-to-text",
            "multimodal",
            "unsloth",
            "image-text-to-text",
            "conversational",
            "en",
            "arxiv:2309.00071",
            "arxiv:2409.12191",
            "arxiv:2308.12966",
            "base_model:Qwen/Qwen2.5-VL-3B-Instruct",
            "base_model:quantized:Qwen/Qwen2.5-VL-3B-Instruct",
            "text-generation-inference",
            "endpoints_compatible",
            "4-bit",
            "bitsandbytes",
            "deploy:azure",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 3804,
          "likes": 4
        },
        {
          "id": "unsloth/Qwen2.5-VL-7B-Instruct-unsloth-bnb-4bit",
          "type": "hf.qwen2_5_vl",
          "name": "unsloth/Qwen2.5-VL-7B-Instruct-unsloth-bnb-4bit",
          "repo_id": "unsloth/Qwen2.5-VL-7B-Instruct-unsloth-bnb-4bit",
          "size_on_disk": 6916305279,
          "pipeline_tag": "image-text-to-text",
          "tags": [
            "transformers",
            "safetensors",
            "qwen2_5_vl",
            "image-to-text",
            "multimodal",
            "unsloth",
            "image-text-to-text",
            "conversational",
            "en",
            "arxiv:2309.00071",
            "arxiv:2409.12191",
            "arxiv:2308.12966",
            "base_model:Qwen/Qwen2.5-VL-7B-Instruct",
            "base_model:quantized:Qwen/Qwen2.5-VL-7B-Instruct",
            "license:apache-2.0",
            "text-generation-inference",
            "endpoints_compatible",
            "4-bit",
            "bitsandbytes",
            "deploy:azure",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 17680,
          "likes": 51
        },
        {
          "id": "unsloth/Qwen2.5-VL-7B-Instruct-bnb-4bit",
          "type": "hf.qwen2_5_vl",
          "name": "unsloth/Qwen2.5-VL-7B-Instruct-bnb-4bit",
          "repo_id": "unsloth/Qwen2.5-VL-7B-Instruct-bnb-4bit",
          "size_on_disk": 6916303028,
          "pipeline_tag": "image-text-to-text",
          "tags": [
            "transformers",
            "safetensors",
            "qwen2_5_vl",
            "image-to-text",
            "multimodal",
            "unsloth",
            "image-text-to-text",
            "conversational",
            "en",
            "arxiv:2309.00071",
            "arxiv:2409.12191",
            "arxiv:2308.12966",
            "base_model:Qwen/Qwen2.5-VL-7B-Instruct",
            "base_model:quantized:Qwen/Qwen2.5-VL-7B-Instruct",
            "license:apache-2.0",
            "text-generation-inference",
            "endpoints_compatible",
            "4-bit",
            "bitsandbytes",
            "deploy:azure",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 11717,
          "likes": 13
        },
        {
          "id": "unsloth/Qwen2.5-VL-32B-Instruct-unsloth-bnb-4bit",
          "type": "hf.qwen2_5_vl",
          "name": "unsloth/Qwen2.5-VL-32B-Instruct-unsloth-bnb-4bit",
          "repo_id": "unsloth/Qwen2.5-VL-32B-Instruct-unsloth-bnb-4bit",
          "size_on_disk": 24484336855,
          "pipeline_tag": "image-text-to-text",
          "tags": [
            "transformers",
            "safetensors",
            "qwen2_5_vl",
            "image-to-text",
            "multimodal",
            "unsloth",
            "image-text-to-text",
            "conversational",
            "en",
            "arxiv:2309.00071",
            "arxiv:2502.13923",
            "base_model:Qwen/Qwen2.5-VL-32B-Instruct",
            "base_model:quantized:Qwen/Qwen2.5-VL-32B-Instruct",
            "license:apache-2.0",
            "text-generation-inference",
            "endpoints_compatible",
            "4-bit",
            "bitsandbytes",
            "deploy:azure",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 1736,
          "likes": 14
        },
        {
          "id": "unsloth/Qwen2.5-VL-32B-Instruct-bnb-4bit",
          "type": "hf.qwen2_5_vl",
          "name": "unsloth/Qwen2.5-VL-32B-Instruct-bnb-4bit",
          "repo_id": "unsloth/Qwen2.5-VL-32B-Instruct-bnb-4bit",
          "size_on_disk": 19668243482,
          "pipeline_tag": "image-text-to-text",
          "tags": [
            "transformers",
            "safetensors",
            "qwen2_5_vl",
            "image-to-text",
            "multimodal",
            "unsloth",
            "image-text-to-text",
            "conversational",
            "en",
            "arxiv:2309.00071",
            "arxiv:2502.13923",
            "base_model:Qwen/Qwen2.5-VL-32B-Instruct",
            "base_model:quantized:Qwen/Qwen2.5-VL-32B-Instruct",
            "license:apache-2.0",
            "text-generation-inference",
            "endpoints_compatible",
            "4-bit",
            "bitsandbytes",
            "deploy:azure",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 897,
          "likes": 3
        },
        {
          "id": "unsloth/Qwen2.5-VL-72B-Instruct-unsloth-bnb-4bit",
          "type": "hf.qwen2_5_vl",
          "name": "unsloth/Qwen2.5-VL-72B-Instruct-unsloth-bnb-4bit",
          "repo_id": "unsloth/Qwen2.5-VL-72B-Instruct-unsloth-bnb-4bit",
          "size_on_disk": 45089979545,
          "pipeline_tag": "image-text-to-text",
          "tags": [
            "transformers",
            "safetensors",
            "qwen2_5_vl",
            "image-to-text",
            "multimodal",
            "unsloth",
            "image-text-to-text",
            "conversational",
            "en",
            "arxiv:2309.00071",
            "arxiv:2409.12191",
            "arxiv:2308.12966",
            "base_model:Qwen/Qwen2.5-VL-72B-Instruct",
            "base_model:quantized:Qwen/Qwen2.5-VL-72B-Instruct",
            "license:other",
            "text-generation-inference",
            "endpoints_compatible",
            "4-bit",
            "bitsandbytes",
            "deploy:azure",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 262,
          "likes": 8
        },
        {
          "id": "unsloth/Qwen2.5-VL-72B-Instruct-bnb-4bit",
          "type": "hf.qwen2_5_vl",
          "name": "unsloth/Qwen2.5-VL-72B-Instruct-bnb-4bit",
          "repo_id": "unsloth/Qwen2.5-VL-72B-Instruct-bnb-4bit",
          "size_on_disk": 41693132125,
          "pipeline_tag": "image-text-to-text",
          "tags": [
            "transformers",
            "safetensors",
            "qwen2_5_vl",
            "image-to-text",
            "multimodal",
            "unsloth",
            "image-text-to-text",
            "conversational",
            "en",
            "arxiv:2309.00071",
            "arxiv:2409.12191",
            "arxiv:2308.12966",
            "base_model:Qwen/Qwen2.5-VL-72B-Instruct",
            "base_model:quantized:Qwen/Qwen2.5-VL-72B-Instruct",
            "license:other",
            "text-generation-inference",
            "endpoints_compatible",
            "4-bit",
            "bitsandbytes",
            "deploy:azure",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 1167,
          "likes": 15
        }
      ],
      "basic_fields": [
        "model",
        "quantization",
        "image",
        "prompt"
      ],
      "is_streaming_output": true
    },
    {
      "title": "Qwen 3 VL",
      "description": "Analyzes images and videos using Alibaba's next-generation Qwen3-VL vision-language model.\n    image, video, multimodal, VLM, Qwen, visual-reasoning, thinking\n\n    Use cases:\n    - Advanced visual reasoning across images and video content\n    - Instruction-following with improved spatial-temporal grounding\n    - Complex multi-step visual analysis with chain-of-thought reasoning\n    - Document, chart, and diagram understanding with enhanced accuracy\n    - Long-context visual conversations with memory\n\n    **Note:** Thinking variants include extended reasoning capabilities. BNB-4bit variants reduce memory usage.",
      "namespace": "huggingface.image_text_to_text",
      "node_type": "huggingface.image_text_to_text.Qwen3_VL",
      "properties": [
        {
          "name": "image",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Input Image",
          "description": "An image to analyze. Leave empty if providing video input."
        },
        {
          "name": "video",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Input Video",
          "description": "A video to analyze. Leave empty if providing image input."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "Describe this image.",
          "title": "Prompt",
          "description": "Your question or instruction about the visual content."
        },
        {
          "name": "min_pixels",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Min Pixels",
          "description": "Minimum pixel count for image resizing. Use 0 for automatic sizing."
        },
        {
          "name": "max_pixels",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Max Pixels",
          "description": "Maximum pixel count for image resizing. Use 0 for automatic sizing."
        },
        {
          "name": "max_new_tokens",
          "type": {
            "type": "int"
          },
          "default": 128,
          "title": "Max New Tokens",
          "description": "Maximum length of the generated response in tokens.",
          "min": 1.0
        },
        {
          "name": "model",
          "type": {
            "type": "hf.qwen3_vl"
          },
          "default": {
            "type": "hf.qwen3_vl",
            "repo_id": "Qwen/Qwen3-VL-4B-Instruct",
            "path": null,
            "variant": null,
            "allow_patterns": null,
            "ignore_patterns": null
          },
          "title": "Model",
          "description": "The Qwen3-VL model variant. Thinking variants add chain-of-thought; BNB-4bit reduces memory; larger models improve accuracy."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "str"
          },
          "name": "text"
        },
        {
          "type": {
            "type": "chunk"
          },
          "name": "chunk"
        }
      ],
      "recommended_models": [
        {
          "id": "Qwen/Qwen3-VL-2B-Instruct",
          "type": "hf.qwen3_vl",
          "name": "Qwen/Qwen3-VL-2B-Instruct",
          "repo_id": "Qwen/Qwen3-VL-2B-Instruct",
          "size_on_disk": 4266648961,
          "pipeline_tag": "image-text-to-text",
          "tags": [
            "transformers",
            "safetensors",
            "qwen3_vl",
            "image-to-text",
            "image-text-to-text",
            "conversational",
            "arxiv:2505.09388",
            "arxiv:2502.13923",
            "arxiv:2409.12191",
            "arxiv:2308.12966",
            "license:apache-2.0",
            "endpoints_compatible",
            "deploy:azure",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 531895,
          "likes": 230
        },
        {
          "id": "Qwen/Qwen3-VL-2B-Thinking",
          "type": "hf.qwen3_vl",
          "name": "Qwen/Qwen3-VL-2B-Thinking",
          "repo_id": "Qwen/Qwen3-VL-2B-Thinking",
          "size_on_disk": 4266648849,
          "pipeline_tag": "image-text-to-text",
          "tags": [
            "transformers",
            "safetensors",
            "qwen3_vl",
            "image-to-text",
            "image-text-to-text",
            "conversational",
            "arxiv:2505.09388",
            "arxiv:2502.13923",
            "arxiv:2409.12191",
            "arxiv:2308.12966",
            "license:apache-2.0",
            "endpoints_compatible",
            "deploy:azure",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 35271,
          "likes": 90
        },
        {
          "id": "Qwen/Qwen3-VL-4B-Instruct",
          "type": "hf.qwen3_vl",
          "name": "Qwen/Qwen3-VL-4B-Instruct",
          "repo_id": "Qwen/Qwen3-VL-4B-Instruct",
          "size_on_disk": 8887292732,
          "pipeline_tag": "image-text-to-text",
          "tags": [
            "transformers",
            "safetensors",
            "qwen3_vl",
            "image-to-text",
            "image-text-to-text",
            "conversational",
            "arxiv:2505.09388",
            "arxiv:2502.13923",
            "arxiv:2409.12191",
            "arxiv:2308.12966",
            "license:apache-2.0",
            "endpoints_compatible",
            "deploy:azure",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 820074,
          "likes": 266
        },
        {
          "id": "Qwen/Qwen3-VL-4B-Thinking",
          "type": "hf.qwen3_vl",
          "name": "Qwen/Qwen3-VL-4B-Thinking",
          "repo_id": "Qwen/Qwen3-VL-4B-Thinking",
          "size_on_disk": 8887292623,
          "pipeline_tag": "image-text-to-text",
          "tags": [
            "transformers",
            "safetensors",
            "qwen3_vl",
            "image-to-text",
            "image-text-to-text",
            "conversational",
            "arxiv:2505.09388",
            "arxiv:2502.13923",
            "arxiv:2409.12191",
            "arxiv:2308.12966",
            "license:apache-2.0",
            "endpoints_compatible",
            "deploy:azure",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 48356,
          "likes": 89
        },
        {
          "id": "Qwen/Qwen3-VL-7B-Instruct",
          "type": "hf.qwen3_vl",
          "name": "Qwen/Qwen3-VL-7B-Instruct",
          "repo_id": "Qwen/Qwen3-VL-7B-Instruct"
        },
        {
          "id": "Qwen/Qwen3-VL-8B-Instruct",
          "type": "hf.qwen3_vl",
          "name": "Qwen/Qwen3-VL-8B-Instruct",
          "repo_id": "Qwen/Qwen3-VL-8B-Instruct",
          "size_on_disk": 17545915883,
          "pipeline_tag": "image-text-to-text",
          "tags": [
            "transformers",
            "safetensors",
            "qwen3_vl",
            "image-to-text",
            "image-text-to-text",
            "conversational",
            "arxiv:2505.09388",
            "arxiv:2502.13923",
            "arxiv:2409.12191",
            "arxiv:2308.12966",
            "license:apache-2.0",
            "endpoints_compatible",
            "deploy:azure",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 2546017,
          "likes": 547
        },
        {
          "id": "Qwen/Qwen3-VL-8B-Thinking",
          "type": "hf.qwen3_vl",
          "name": "Qwen/Qwen3-VL-8B-Thinking",
          "repo_id": "Qwen/Qwen3-VL-8B-Thinking",
          "size_on_disk": 17545915778,
          "pipeline_tag": "image-text-to-text",
          "tags": [
            "transformers",
            "safetensors",
            "qwen3_vl",
            "image-to-text",
            "conversational",
            "image-text-to-text",
            "arxiv:2505.09388",
            "arxiv:2502.13923",
            "arxiv:2409.12191",
            "arxiv:2308.12966",
            "license:apache-2.0",
            "endpoints_compatible",
            "deploy:azure",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 213643,
          "likes": 152
        },
        {
          "id": "Qwen/Qwen3-VL-30B-A3B-Instruct",
          "type": "hf.qwen3_vl",
          "name": "Qwen/Qwen3-VL-30B-A3B-Instruct",
          "repo_id": "Qwen/Qwen3-VL-30B-A3B-Instruct",
          "size_on_disk": 62153207155,
          "pipeline_tag": "image-text-to-text",
          "tags": [
            "transformers",
            "safetensors",
            "qwen3_vl_moe",
            "image-to-text",
            "conversational",
            "image-text-to-text",
            "arxiv:2505.09388",
            "arxiv:2502.13923",
            "arxiv:2409.12191",
            "arxiv:2308.12966",
            "license:apache-2.0",
            "endpoints_compatible",
            "deploy:azure",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 1288213,
          "likes": 440
        },
        {
          "id": "Qwen/Qwen3-VL-30B-A3B-Thinking",
          "type": "hf.qwen3_vl",
          "name": "Qwen/Qwen3-VL-30B-A3B-Thinking",
          "repo_id": "Qwen/Qwen3-VL-30B-A3B-Thinking",
          "size_on_disk": 62153206984,
          "pipeline_tag": "image-text-to-text",
          "tags": [
            "transformers",
            "safetensors",
            "qwen3_vl_moe",
            "image-to-text",
            "image-text-to-text",
            "conversational",
            "arxiv:2505.09388",
            "arxiv:2502.13923",
            "arxiv:2409.12191",
            "arxiv:2308.12966",
            "license:apache-2.0",
            "endpoints_compatible",
            "deploy:azure",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 56683,
          "likes": 163
        },
        {
          "id": "Qwen/Qwen3-VL-32B-Instruct",
          "type": "hf.qwen3_vl",
          "name": "Qwen/Qwen3-VL-32B-Instruct",
          "repo_id": "Qwen/Qwen3-VL-32B-Instruct",
          "size_on_disk": 66726519322,
          "pipeline_tag": "image-text-to-text",
          "tags": [
            "transformers",
            "safetensors",
            "qwen3_vl",
            "image-to-text",
            "image-text-to-text",
            "conversational",
            "arxiv:2505.09388",
            "arxiv:2502.13923",
            "arxiv:2409.12191",
            "arxiv:2308.12966",
            "license:apache-2.0",
            "endpoints_compatible",
            "deploy:azure",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 718619,
          "likes": 136
        },
        {
          "id": "Qwen/Qwen3-VL-32B-Thinking",
          "type": "hf.qwen3_vl",
          "name": "Qwen/Qwen3-VL-32B-Thinking",
          "repo_id": "Qwen/Qwen3-VL-32B-Thinking",
          "size_on_disk": 66726519215,
          "pipeline_tag": "image-text-to-text",
          "tags": [
            "transformers",
            "safetensors",
            "qwen3_vl",
            "image-to-text",
            "image-text-to-text",
            "conversational",
            "arxiv:2505.09388",
            "arxiv:2502.13923",
            "arxiv:2409.12191",
            "arxiv:2308.12966",
            "license:apache-2.0",
            "endpoints_compatible",
            "deploy:azure",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 443963,
          "likes": 69
        },
        {
          "id": "Qwen/Qwen3-VL-235B-A22B-Instruct",
          "type": "hf.qwen3_vl",
          "name": "Qwen/Qwen3-VL-235B-A22B-Instruct",
          "repo_id": "Qwen/Qwen3-VL-235B-A22B-Instruct",
          "size_on_disk": 471351862384,
          "pipeline_tag": "image-text-to-text",
          "tags": [
            "transformers",
            "safetensors",
            "qwen3_vl_moe",
            "image-to-text",
            "conversational",
            "arxiv:2505.09388",
            "arxiv:2502.13923",
            "arxiv:2409.12191",
            "arxiv:2308.12966",
            "license:apache-2.0",
            "endpoints_compatible",
            "deploy:azure",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 132001,
          "likes": 333
        },
        {
          "id": "Qwen/Qwen3-VL-235B-A22B-Thinking",
          "type": "hf.qwen3_vl",
          "name": "Qwen/Qwen3-VL-235B-A22B-Thinking",
          "repo_id": "Qwen/Qwen3-VL-235B-A22B-Thinking",
          "size_on_disk": 471351862204,
          "pipeline_tag": "image-text-to-text",
          "tags": [
            "transformers",
            "safetensors",
            "qwen3_vl_moe",
            "image-to-text",
            "conversational",
            "arxiv:2505.09388",
            "arxiv:2502.13923",
            "arxiv:2409.12191",
            "arxiv:2308.12966",
            "license:apache-2.0",
            "endpoints_compatible",
            "deploy:azure",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 5849,
          "likes": 343
        },
        {
          "id": "Qwen/Qwen3-VL-Demo",
          "type": "hf.qwen3_vl",
          "name": "Qwen/Qwen3-VL-Demo",
          "repo_id": "Qwen/Qwen3-VL-Demo"
        },
        {
          "id": "unsloth/Qwen3-VL-2B-Instruct-bnb-4bit",
          "type": "hf.qwen3_vl",
          "name": "unsloth/Qwen3-VL-2B-Instruct-bnb-4bit",
          "repo_id": "unsloth/Qwen3-VL-2B-Instruct-bnb-4bit",
          "size_on_disk": 2179855763,
          "pipeline_tag": "image-text-to-text",
          "tags": [
            "transformers",
            "safetensors",
            "qwen3_vl",
            "image-to-text",
            "unsloth",
            "image-text-to-text",
            "conversational",
            "arxiv:2505.09388",
            "arxiv:2502.13923",
            "arxiv:2409.12191",
            "arxiv:2308.12966",
            "base_model:Qwen/Qwen3-VL-2B-Instruct",
            "base_model:quantized:Qwen/Qwen3-VL-2B-Instruct",
            "license:apache-2.0",
            "endpoints_compatible",
            "4-bit",
            "bitsandbytes",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 2101,
          "likes": 1
        },
        {
          "id": "unsloth/Qwen3-VL-2B-Thinking-bnb-4bit",
          "type": "hf.qwen3_vl",
          "name": "unsloth/Qwen3-VL-2B-Thinking-bnb-4bit",
          "repo_id": "unsloth/Qwen3-VL-2B-Thinking-bnb-4bit",
          "size_on_disk": 2179855629,
          "pipeline_tag": "image-text-to-text",
          "tags": [
            "transformers",
            "safetensors",
            "qwen3_vl",
            "image-to-text",
            "unsloth",
            "image-text-to-text",
            "conversational",
            "arxiv:2505.09388",
            "arxiv:2502.13923",
            "arxiv:2409.12191",
            "arxiv:2308.12966",
            "base_model:Qwen/Qwen3-VL-2B-Thinking",
            "base_model:quantized:Qwen/Qwen3-VL-2B-Thinking",
            "license:apache-2.0",
            "endpoints_compatible",
            "4-bit",
            "bitsandbytes",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 246,
          "likes": 0
        },
        {
          "id": "unsloth/Qwen3-VL-4B-Instruct-bnb-4bit",
          "type": "hf.qwen3_vl",
          "name": "unsloth/Qwen3-VL-4B-Instruct-bnb-4bit",
          "repo_id": "unsloth/Qwen3-VL-4B-Instruct-bnb-4bit",
          "size_on_disk": 3499792257,
          "pipeline_tag": "image-text-to-text",
          "tags": [
            "safetensors",
            "qwen3_vl",
            "unsloth",
            "image-text-to-text",
            "conversational",
            "arxiv:2505.09388",
            "arxiv:2502.13923",
            "arxiv:2409.12191",
            "arxiv:2308.12966",
            "base_model:Qwen/Qwen3-VL-4B-Instruct",
            "base_model:quantized:Qwen/Qwen3-VL-4B-Instruct",
            "license:apache-2.0",
            "4-bit",
            "bitsandbytes",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 5046,
          "likes": 0
        },
        {
          "id": "unsloth/Qwen3-VL-4B-Thinking-bnb-4bit",
          "type": "hf.qwen3_vl",
          "name": "unsloth/Qwen3-VL-4B-Thinking-bnb-4bit",
          "repo_id": "unsloth/Qwen3-VL-4B-Thinking-bnb-4bit",
          "size_on_disk": 3499792057,
          "pipeline_tag": "image-text-to-text",
          "tags": [
            "safetensors",
            "qwen3_vl",
            "unsloth",
            "image-text-to-text",
            "conversational",
            "arxiv:2505.09388",
            "arxiv:2502.13923",
            "arxiv:2409.12191",
            "arxiv:2308.12966",
            "base_model:Qwen/Qwen3-VL-4B-Thinking",
            "base_model:quantized:Qwen/Qwen3-VL-4B-Thinking",
            "license:apache-2.0",
            "4-bit",
            "bitsandbytes",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 384,
          "likes": 1
        },
        {
          "id": "unsloth/Qwen3-VL-8B-Instruct-bnb-4bit",
          "type": "hf.qwen3_vl",
          "name": "unsloth/Qwen3-VL-8B-Instruct-bnb-4bit",
          "repo_id": "unsloth/Qwen3-VL-8B-Instruct-bnb-4bit",
          "size_on_disk": 7242509964,
          "pipeline_tag": "image-text-to-text",
          "tags": [
            "safetensors",
            "qwen3_vl",
            "unsloth",
            "image-text-to-text",
            "conversational",
            "arxiv:2505.09388",
            "arxiv:2502.13923",
            "arxiv:2409.12191",
            "arxiv:2308.12966",
            "base_model:Qwen/Qwen3-VL-8B-Instruct",
            "base_model:quantized:Qwen/Qwen3-VL-8B-Instruct",
            "license:apache-2.0",
            "4-bit",
            "bitsandbytes",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 11992,
          "likes": 2
        },
        {
          "id": "unsloth/Qwen3-VL-8B-Thinking-bnb-4bit",
          "type": "hf.qwen3_vl",
          "name": "unsloth/Qwen3-VL-8B-Thinking-bnb-4bit",
          "repo_id": "unsloth/Qwen3-VL-8B-Thinking-bnb-4bit",
          "size_on_disk": 7242509767,
          "pipeline_tag": "image-text-to-text",
          "tags": [
            "safetensors",
            "qwen3_vl",
            "unsloth",
            "image-text-to-text",
            "conversational",
            "arxiv:2505.09388",
            "arxiv:2502.13923",
            "arxiv:2409.12191",
            "arxiv:2308.12966",
            "base_model:Qwen/Qwen3-VL-8B-Thinking",
            "base_model:quantized:Qwen/Qwen3-VL-8B-Thinking",
            "license:apache-2.0",
            "4-bit",
            "bitsandbytes",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 308,
          "likes": 1
        },
        {
          "id": "unsloth/Qwen3-VL-32B-Instruct-bnb-4bit",
          "type": "hf.qwen3_vl",
          "name": "unsloth/Qwen3-VL-32B-Instruct-bnb-4bit",
          "repo_id": "unsloth/Qwen3-VL-32B-Instruct-bnb-4bit",
          "size_on_disk": 20418804032,
          "pipeline_tag": "image-text-to-text",
          "tags": [
            "transformers",
            "safetensors",
            "qwen3_vl",
            "image-to-text",
            "unsloth",
            "image-text-to-text",
            "conversational",
            "arxiv:2505.09388",
            "arxiv:2502.13923",
            "arxiv:2409.12191",
            "arxiv:2308.12966",
            "base_model:Qwen/Qwen3-VL-32B-Instruct",
            "base_model:quantized:Qwen/Qwen3-VL-32B-Instruct",
            "license:apache-2.0",
            "endpoints_compatible",
            "4-bit",
            "bitsandbytes",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 1779,
          "likes": 1
        },
        {
          "id": "unsloth/Qwen3-VL-32B-Thinking-bnb-4bit",
          "type": "hf.qwen3_vl",
          "name": "unsloth/Qwen3-VL-32B-Thinking-bnb-4bit",
          "repo_id": "unsloth/Qwen3-VL-32B-Thinking-bnb-4bit",
          "size_on_disk": 20418803911,
          "pipeline_tag": "image-text-to-text",
          "tags": [
            "transformers",
            "safetensors",
            "qwen3_vl",
            "image-to-text",
            "unsloth",
            "image-text-to-text",
            "conversational",
            "arxiv:2505.09388",
            "arxiv:2502.13923",
            "arxiv:2409.12191",
            "arxiv:2308.12966",
            "base_model:Qwen/Qwen3-VL-32B-Thinking",
            "base_model:quantized:Qwen/Qwen3-VL-32B-Thinking",
            "license:apache-2.0",
            "endpoints_compatible",
            "4-bit",
            "bitsandbytes",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 121,
          "likes": 0
        }
      ],
      "basic_fields": [
        "model",
        "quantization",
        "image",
        "prompt"
      ],
      "is_streaming_output": true
    },
    {
      "title": "Text Classifier",
      "description": "Classifies text into predefined categories using fine-tuned transformer models.\n    text, classification, sentiment, NLP, emotion\n\n    Use cases:\n    - Analyze sentiment in social media posts and reviews\n    - Detect emotions in customer feedback and conversations\n    - Classify support tickets by category or priority\n    - Filter spam or inappropriate content\n    - Categorize news articles by topic",
      "namespace": "huggingface.text_classification",
      "node_type": "huggingface.text_classification.TextClassifier",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.text_classification"
          },
          "default": {
            "type": "hf.text_classification",
            "repo_id": "",
            "path": null,
            "variant": null,
            "allow_patterns": null,
            "ignore_patterns": null
          },
          "title": "Model",
          "description": "The text classification model. Use sentiment models for opinion analysis; emotion models for feeling detection."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Input Text",
          "description": "The text to classify."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "float"
              }
            ]
          },
          "name": "output"
        }
      ],
      "recommended_models": [
        {
          "id": "cardiffnlp/twitter-roberta-base-sentiment-latest",
          "type": "hf.text_classification",
          "name": "cardiffnlp/twitter-roberta-base-sentiment-latest",
          "repo_id": "cardiffnlp/twitter-roberta-base-sentiment-latest",
          "allow_patterns": [
            "*.json",
            "*.txt",
            "*.bin"
          ],
          "size_on_disk": 502401839,
          "pipeline_tag": "text-classification",
          "tags": [
            "transformers",
            "pytorch",
            "tf",
            "roberta",
            "text-classification",
            "en",
            "dataset:tweet_eval",
            "arxiv:2202.03829",
            "license:cc-by-4.0",
            "endpoints_compatible",
            "deploy:azure",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 3529085,
          "likes": 743
        },
        {
          "id": "michellejieli/emotion_text_classifier",
          "type": "hf.text_classification",
          "name": "michellejieli/emotion_text_classifier",
          "repo_id": "michellejieli/emotion_text_classifier",
          "allow_patterns": [
            "*.json",
            "*.txt",
            "*.bin"
          ],
          "size_on_disk": 331902714,
          "pipeline_tag": "text-classification",
          "tags": [
            "transformers",
            "pytorch",
            "roberta",
            "text-classification",
            "distilroberta",
            "sentiment",
            "emotion",
            "twitter",
            "reddit",
            "en",
            "endpoints_compatible",
            "deploy:azure",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 750644,
          "likes": 148
        }
      ],
      "basic_fields": [
        "model",
        "prompt"
      ]
    },
    {
      "title": "Zero Shot Text Classifier",
      "description": "Classifies text into custom categories without requiring task-specific training data.\n    text, classification, zero-shot, NLP, flexible\n\n    Use cases:\n    - Classify text into custom, user-defined categories on the fly\n    - Detect topics in documents without predefined training\n    - Perform sentiment analysis with custom sentiment labels\n    - Build flexible intent classification for conversational AI\n    - Prototype classification systems with dynamic categories",
      "namespace": "huggingface.text_classification",
      "node_type": "huggingface.text_classification.ZeroShotTextClassifier",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.zero_shot_classification"
          },
          "default": {
            "type": "hf.zero_shot_classification",
            "repo_id": "",
            "path": null,
            "variant": null,
            "allow_patterns": null,
            "ignore_patterns": null
          },
          "title": "Model",
          "description": "The zero-shot classification model. BART-large-mnli is reliable; DeBERTa variants offer improved accuracy; mDeBERTa is multilingual."
        },
        {
          "name": "inputs",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Input Text",
          "description": "The text to classify."
        },
        {
          "name": "candidate_labels",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Candidate Labels",
          "description": "Comma-separated list of labels to classify against (e.g., 'positive,negative,neutral' or 'sports,politics,technology')."
        },
        {
          "name": "multi_label",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Multi-label Classification",
          "description": "Allow multiple labels to be assigned to the same text (useful when text can belong to multiple categories)."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "float"
              }
            ]
          },
          "name": "output"
        }
      ],
      "recommended_models": [
        {
          "id": "facebook/bart-large-mnli",
          "type": "hf.zero_shot_classification",
          "name": "facebook/bart-large-mnli",
          "repo_id": "facebook/bart-large-mnli",
          "allow_patterns": [
            "*.json",
            "*.txt",
            "*.safetensors"
          ],
          "size_on_disk": 1632149330,
          "pipeline_tag": "zero-shot-classification",
          "tags": [
            "transformers",
            "pytorch",
            "jax",
            "rust",
            "safetensors",
            "bart",
            "text-classification",
            "zero-shot-classification",
            "dataset:multi_nli",
            "arxiv:1910.13461",
            "arxiv:1909.00161",
            "license:mit",
            "endpoints_compatible",
            "deploy:azure",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 4112623,
          "likes": 1504
        },
        {
          "id": "MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli",
          "type": "hf.zero_shot_classification",
          "name": "MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli",
          "repo_id": "MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli",
          "allow_patterns": [
            "*.json",
            "*.txt",
            "*.safetensors"
          ],
          "size_on_disk": 377536975,
          "pipeline_tag": "zero-shot-classification",
          "tags": [
            "transformers",
            "pytorch",
            "safetensors",
            "deberta-v2",
            "text-classification",
            "zero-shot-classification",
            "en",
            "dataset:multi_nli",
            "dataset:facebook/anli",
            "dataset:fever",
            "arxiv:2006.03654",
            "license:mit",
            "model-index",
            "endpoints_compatible",
            "deploy:azure",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 45822,
          "likes": 220
        },
        {
          "id": "MoritzLaurer/mDeBERTa-v3-base-mnli-xnli",
          "type": "hf.zero_shot_classification",
          "name": "MoritzLaurer/mDeBERTa-v3-base-mnli-xnli",
          "repo_id": "MoritzLaurer/mDeBERTa-v3-base-mnli-xnli",
          "allow_patterns": [
            "*.json",
            "*.txt",
            "*.safetensors"
          ],
          "size_on_disk": 573986073,
          "pipeline_tag": "zero-shot-classification",
          "tags": [
            "transformers",
            "pytorch",
            "onnx",
            "safetensors",
            "deberta-v2",
            "text-classification",
            "zero-shot-classification",
            "nli",
            "multilingual",
            "en",
            "ar",
            "bg",
            "de",
            "el",
            "es",
            "fr",
            "hi",
            "ru",
            "sw",
            "th",
            "tr",
            "ur",
            "vi",
            "zh",
            "dataset:multi_nli",
            "dataset:xnli",
            "arxiv:2111.09543",
            "arxiv:1809.05053",
            "arxiv:1911.02116",
            "license:mit",
            "endpoints_compatible",
            "deploy:azure",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 254446,
          "likes": 275
        },
        {
          "id": "tasksource/ModernBERT-base-nli",
          "type": "hf.zero_shot_classification",
          "name": "tasksource/ModernBERT-base-nli",
          "repo_id": "tasksource/ModernBERT-base-nli",
          "allow_patterns": [
            "*.json",
            "*.txt",
            "*.safetensors"
          ],
          "size_on_disk": 602053461,
          "pipeline_tag": "zero-shot-classification",
          "tags": [
            "transformers",
            "safetensors",
            "modernbert",
            "text-classification",
            "instruct",
            "natural-language-inference",
            "nli",
            "mnli",
            "zero-shot-classification",
            "en",
            "dataset:nyu-mll/glue",
            "dataset:facebook/anli",
            "base_model:answerdotai/ModernBERT-base",
            "base_model:finetune:answerdotai/ModernBERT-base",
            "license:apache-2.0",
            "endpoints_compatible",
            "deploy:azure",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 1631,
          "likes": 22
        },
        {
          "id": "cross-encoder/nli-deberta-v3-base",
          "type": "hf.zero_shot_classification",
          "name": "cross-encoder/nli-deberta-v3-base",
          "repo_id": "cross-encoder/nli-deberta-v3-base",
          "allow_patterns": [
            "*.json",
            "*.txt",
            "*.safetensors"
          ],
          "size_on_disk": 746385901,
          "pipeline_tag": "zero-shot-classification",
          "tags": [
            "sentence-transformers",
            "pytorch",
            "onnx",
            "safetensors",
            "deberta-v2",
            "text-classification",
            "transformers",
            "zero-shot-classification",
            "en",
            "dataset:nyu-mll/multi_nli",
            "dataset:stanfordnlp/snli",
            "base_model:microsoft/deberta-v3-base",
            "base_model:quantized:microsoft/deberta-v3-base",
            "license:apache-2.0",
            "deploy:azure",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 29980,
          "likes": 38
        },
        {
          "id": "microsoft/deberta-v2-xlarge-mnli",
          "type": "hf.zero_shot_classification",
          "name": "microsoft/deberta-v2-xlarge-mnli",
          "repo_id": "microsoft/deberta-v2-xlarge-mnli",
          "allow_patterns": [
            "*.json",
            "*.txt",
            "*.safetensors"
          ],
          "size_on_disk": 1022,
          "pipeline_tag": "text-classification",
          "tags": [
            "transformers",
            "pytorch",
            "deberta-v2",
            "text-classification",
            "deberta",
            "deberta-mnli",
            "en",
            "arxiv:2006.03654",
            "license:mit",
            "endpoints_compatible",
            "deploy:azure",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 80209,
          "likes": 9
        },
        {
          "id": "roberta-large-mnli",
          "type": "hf.zero_shot_classification",
          "name": "roberta-large-mnli",
          "repo_id": "roberta-large-mnli",
          "allow_patterns": [
            "*.json",
            "*.txt",
            "*.safetensors"
          ],
          "size_on_disk": 1428409833,
          "pipeline_tag": "text-classification",
          "tags": [
            "transformers",
            "pytorch",
            "tf",
            "jax",
            "safetensors",
            "roberta",
            "text-classification",
            "autogenerated-modelcard",
            "en",
            "dataset:multi_nli",
            "dataset:wikipedia",
            "dataset:bookcorpus",
            "arxiv:1907.11692",
            "arxiv:1806.02847",
            "arxiv:1804.07461",
            "arxiv:1704.05426",
            "arxiv:1508.05326",
            "arxiv:1809.05053",
            "arxiv:1910.09700",
            "license:mit",
            "endpoints_compatible",
            "deploy:azure",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 455537,
          "likes": 202
        }
      ],
      "basic_fields": [
        "model",
        "inputs",
        "candidate_labels",
        "multi_label"
      ]
    },
    {
      "title": "Translation",
      "description": "Translates text between languages using sequence-to-sequence transformer models.\n    text, translation, NLP, multilingual, localization\n\n    Use cases:\n    - Translate content for multilingual websites and applications\n    - Enable cross-language communication in chat systems\n    - Localize documents and product descriptions\n    - Build multilingual content pipelines\n    - Support international user bases with automatic translation\n\n    **Note:** Different models support different language pairs. T5 models handle many language combinations.",
      "namespace": "huggingface.translation",
      "node_type": "huggingface.translation.Translation",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.translation"
          },
          "default": {
            "type": "hf.translation",
            "repo_id": "google-t5/t5-base",
            "path": null,
            "variant": null,
            "allow_patterns": [
              "*.json",
              "*.txt",
              "*.safetensors"
            ],
            "ignore_patterns": null
          },
          "title": "Model",
          "description": "The translation model. T5 models support many language pairs; larger variants offer better quality."
        },
        {
          "name": "inputs",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Input Text",
          "description": "The text to translate."
        },
        {
          "name": "source_lang",
          "type": {
            "type": "enum",
            "values": [
              "ar",
              "bn",
              "bs",
              "zh",
              "hr",
              "cs",
              "da",
              "nl",
              "en",
              "fil",
              "fi",
              "fr",
              "de",
              "el",
              "he",
              "hi",
              "id",
              "it",
              "ja",
              "ko",
              "ms",
              "me",
              "no",
              "pl",
              "pt",
              "pa",
              "ru",
              "ro",
              "sr",
              "sk",
              "sl",
              "es",
              "sv",
              "th",
              "tr",
              "vi"
            ],
            "type_name": "nodetool.nodes.huggingface.translation.Translation.LanguageCode"
          },
          "default": "en",
          "title": "Source Language",
          "description": "The language of the input text."
        },
        {
          "name": "target_lang",
          "type": {
            "type": "enum",
            "values": [
              "ar",
              "bn",
              "bs",
              "zh",
              "hr",
              "cs",
              "da",
              "nl",
              "en",
              "fil",
              "fi",
              "fr",
              "de",
              "el",
              "he",
              "hi",
              "id",
              "it",
              "ja",
              "ko",
              "ms",
              "me",
              "no",
              "pl",
              "pt",
              "pa",
              "ru",
              "ro",
              "sr",
              "sk",
              "sl",
              "es",
              "sv",
              "th",
              "tr",
              "vi"
            ],
            "type_name": "nodetool.nodes.huggingface.translation.Translation.LanguageCode"
          },
          "default": "fr",
          "title": "Target Language",
          "description": "The language to translate the text into."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "str"
          },
          "name": "output"
        }
      ],
      "recommended_models": [
        {
          "id": "google-t5/t5-base",
          "type": "hf.translation",
          "name": "google-t5/t5-base",
          "repo_id": "google-t5/t5-base",
          "allow_patterns": [
            "*.json",
            "*.txt",
            "*.safetensors"
          ],
          "size_on_disk": 893037098,
          "pipeline_tag": "translation",
          "tags": [
            "transformers",
            "pytorch",
            "tf",
            "jax",
            "rust",
            "safetensors",
            "t5",
            "text2text-generation",
            "summarization",
            "translation",
            "en",
            "fr",
            "ro",
            "de",
            "dataset:c4",
            "arxiv:1805.12471",
            "arxiv:1708.00055",
            "arxiv:1704.05426",
            "arxiv:1606.05250",
            "arxiv:1808.09121",
            "arxiv:1810.12885",
            "arxiv:1905.10044",
            "arxiv:1910.09700",
            "license:apache-2.0",
            "text-generation-inference",
            "endpoints_compatible",
            "deploy:azure",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 2248118,
          "likes": 758
        },
        {
          "id": "google-t5/t5-large",
          "type": "hf.translation",
          "name": "google-t5/t5-large",
          "repo_id": "google-t5/t5-large",
          "allow_patterns": [
            "*.json",
            "*.txt",
            "*.safetensors"
          ],
          "size_on_disk": 2952127439,
          "pipeline_tag": "translation",
          "tags": [
            "transformers",
            "pytorch",
            "tf",
            "jax",
            "safetensors",
            "t5",
            "text2text-generation",
            "summarization",
            "translation",
            "en",
            "fr",
            "ro",
            "de",
            "multilingual",
            "dataset:c4",
            "arxiv:1805.12471",
            "arxiv:1708.00055",
            "arxiv:1704.05426",
            "arxiv:1606.05250",
            "arxiv:1808.09121",
            "arxiv:1810.12885",
            "arxiv:1905.10044",
            "arxiv:1910.09700",
            "license:apache-2.0",
            "text-generation-inference",
            "endpoints_compatible",
            "deploy:azure",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 226514,
          "likes": 233
        },
        {
          "id": "google-t5/t5-small",
          "type": "hf.translation",
          "name": "google-t5/t5-small",
          "repo_id": "google-t5/t5-small",
          "allow_patterns": [
            "*.json",
            "*.txt",
            "*.safetensors"
          ],
          "size_on_disk": 243436086,
          "pipeline_tag": "translation",
          "tags": [
            "transformers",
            "pytorch",
            "tf",
            "jax",
            "rust",
            "onnx",
            "safetensors",
            "t5",
            "text2text-generation",
            "summarization",
            "translation",
            "en",
            "fr",
            "ro",
            "de",
            "multilingual",
            "dataset:c4",
            "arxiv:1805.12471",
            "arxiv:1708.00055",
            "arxiv:1704.05426",
            "arxiv:1606.05250",
            "arxiv:1808.09121",
            "arxiv:1810.12885",
            "arxiv:1905.10044",
            "arxiv:1910.09700",
            "license:apache-2.0",
            "text-generation-inference",
            "endpoints_compatible",
            "deploy:azure",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 2527724,
          "likes": 517
        }
      ],
      "basic_fields": [
        "model",
        "inputs",
        "source_lang",
        "target_lang"
      ]
    },
    {
      "title": "Reranker",
      "description": "Scores and ranks text pairs by semantic relevance using cross-encoder reranking models.\n    text, ranking, reranking, NLP, search\n\n    Use cases:\n    - Improve search result quality by reranking initial retrieval results\n    - Score question-answer pair relevance for QA systems\n    - Rank document relevance for information retrieval\n    - Build two-stage retrieval pipelines (retrieve then rerank)\n    - Filter and prioritize candidates in recommendation systems",
      "namespace": "huggingface.ranking",
      "node_type": "huggingface.ranking.Reranker",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.reranker"
          },
          "default": {
            "type": "hf.reranker",
            "repo_id": "",
            "path": null,
            "variant": null,
            "allow_patterns": null,
            "ignore_patterns": null
          },
          "title": "Model",
          "description": "The reranking model. BGE-reranker-v2-m3 is multilingual; base and large variants offer different speed/accuracy tradeoffs."
        },
        {
          "name": "query",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Query Text",
          "description": "The query or question to compare candidates against."
        },
        {
          "name": "candidates",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Candidate Texts",
          "description": "List of text candidates to rank by relevance to the query."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "float"
              }
            ]
          },
          "name": "output"
        }
      ],
      "recommended_models": [
        {
          "id": "BAAI/bge-reranker-v2-m3",
          "type": "hf.reranker",
          "name": "BAAI/bge-reranker-v2-m3",
          "repo_id": "BAAI/bge-reranker-v2-m3",
          "allow_patterns": [
            "*.safetensors",
            "*.txt",
            "*.json"
          ],
          "size_on_disk": 2288173057,
          "pipeline_tag": "text-classification",
          "tags": [
            "sentence-transformers",
            "safetensors",
            "xlm-roberta",
            "text-classification",
            "transformers",
            "text-embeddings-inference",
            "multilingual",
            "arxiv:2312.15503",
            "arxiv:2402.03216",
            "license:apache-2.0",
            "deploy:azure",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 3090306,
          "likes": 833
        },
        {
          "id": "BAAI/bge-reranker-base",
          "type": "hf.reranker",
          "name": "BAAI/bge-reranker-base",
          "repo_id": "BAAI/bge-reranker-base",
          "allow_patterns": [
            "*.safetensors",
            "*.txt",
            "*.json"
          ],
          "size_on_disk": 1129305768,
          "pipeline_tag": "text-classification",
          "tags": [
            "sentence-transformers",
            "pytorch",
            "onnx",
            "safetensors",
            "xlm-roberta",
            "mteb",
            "text-embeddings-inference",
            "text-classification",
            "en",
            "zh",
            "arxiv:2401.03462",
            "arxiv:2312.15503",
            "arxiv:2311.13534",
            "arxiv:2310.07554",
            "arxiv:2309.07597",
            "license:mit",
            "model-index",
            "deploy:azure",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 1304602,
          "likes": 219
        },
        {
          "id": "BAAI/bge-reranker-large",
          "type": "hf.reranker",
          "name": "BAAI/bge-reranker-large",
          "repo_id": "BAAI/bge-reranker-large",
          "allow_patterns": [
            "*.safetensors",
            "*.txt",
            "*.json"
          ],
          "size_on_disk": 2256718402,
          "pipeline_tag": "feature-extraction",
          "tags": [
            "transformers",
            "pytorch",
            "onnx",
            "safetensors",
            "xlm-roberta",
            "text-classification",
            "mteb",
            "feature-extraction",
            "en",
            "zh",
            "arxiv:2401.03462",
            "arxiv:2312.15503",
            "arxiv:2311.13534",
            "arxiv:2310.07554",
            "arxiv:2309.07597",
            "license:mit",
            "model-index",
            "text-embeddings-inference",
            "endpoints_compatible",
            "deploy:azure",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 1100130,
          "likes": 441
        }
      ],
      "basic_fields": [
        "model",
        "query",
        "candidates"
      ]
    },
    {
      "title": "Text Generation",
      "description": "Generates text continuations and responses from prompts using large language models.\n    text, generation, NLP, LLM, chatbot, creative-writing\n\n    Use cases:\n    - Generate creative writing, stories, and content\n    - Build chatbots and conversational AI assistants\n    - Create code completions and programming assistance\n    - Produce automated content for various applications\n    - Answer questions and provide explanations",
      "namespace": "huggingface.text_generation",
      "node_type": "huggingface.text_generation.TextGeneration",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.text_generation"
          },
          "default": {
            "type": "hf.text_generation",
            "repo_id": "",
            "path": null,
            "variant": null,
            "allow_patterns": null,
            "ignore_patterns": null
          },
          "title": "Model",
          "description": "The language model to use. Instruction-tuned models (Instruct) follow prompts better; Base models are for completion tasks. BNB-4bit variants reduce memory."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The input text to generate from. For instruct models, phrase as a request; for base models, provide text to continue."
        },
        {
          "name": "max_new_tokens",
          "type": {
            "type": "int"
          },
          "default": 512,
          "title": "Max New Tokens",
          "description": "Maximum number of tokens to generate in the response."
        },
        {
          "name": "temperature",
          "type": {
            "type": "float"
          },
          "default": 1.0,
          "title": "Temperature",
          "description": "Controls randomness: lower values (0.1-0.5) for focused responses, higher (0.7-1.5) for creative output.",
          "min": 0.0,
          "max": 2.0
        },
        {
          "name": "top_p",
          "type": {
            "type": "float"
          },
          "default": 1.0,
          "title": "Top P",
          "description": "Nucleus sampling: limits token selection to top probability mass. Lower values (0.1-0.5) increase focus.",
          "min": 0.0,
          "max": 1.0
        },
        {
          "name": "do_sample",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Do Sample",
          "description": "Enable sampling for varied outputs. Disable for deterministic, greedy decoding."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "str"
          },
          "name": "text"
        },
        {
          "type": {
            "type": "chunk"
          },
          "name": "chunk"
        }
      ],
      "recommended_models": [
        {
          "id": "nvidia/Nemotron-Orchestrator-8B",
          "type": "hf.text_generation",
          "name": "nvidia/Nemotron-Orchestrator-8B",
          "repo_id": "nvidia/Nemotron-Orchestrator-8B",
          "size_on_disk": 32778913883,
          "pipeline_tag": "text-generation",
          "tags": [
            "transformers",
            "safetensors",
            "qwen3",
            "text-generation",
            "conversational",
            "arxiv:2511.21689",
            "base_model:Qwen/Qwen3-8B",
            "base_model:finetune:Qwen/Qwen3-8B",
            "text-generation-inference",
            "endpoints_compatible",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 38116,
          "likes": 442
        },
        {
          "id": "EssentialAI/rnj-1-instruct",
          "type": "hf.text_generation",
          "name": "EssentialAI/rnj-1-instruct",
          "repo_id": "EssentialAI/rnj-1-instruct",
          "size_on_disk": 33259389620,
          "pipeline_tag": "text-generation",
          "tags": [
            "transformers",
            "safetensors",
            "gemma3_text",
            "text-generation",
            "conversational",
            "license:apache-2.0",
            "text-generation-inference",
            "endpoints_compatible",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 447364,
          "likes": 241
        },
        {
          "id": "EssentialAI/rnj-1",
          "type": "hf.text_generation",
          "name": "EssentialAI/rnj-1",
          "repo_id": "EssentialAI/rnj-1",
          "size_on_disk": 33259389531,
          "pipeline_tag": "text-generation",
          "tags": [
            "transformers",
            "safetensors",
            "gemma3_text",
            "text-generation",
            "conversational",
            "license:apache-2.0",
            "text-generation-inference",
            "endpoints_compatible",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 138771,
          "likes": 79
        },
        {
          "id": "meta-llama/Llama-3.1-8B-Instruct",
          "type": "hf.text_generation",
          "name": "meta-llama/Llama-3.1-8B-Instruct",
          "repo_id": "meta-llama/Llama-3.1-8B-Instruct",
          "size_on_disk": 32132582323,
          "pipeline_tag": "text-generation",
          "tags": [
            "transformers",
            "safetensors",
            "llama",
            "text-generation",
            "facebook",
            "meta",
            "pytorch",
            "llama-3",
            "conversational",
            "en",
            "de",
            "fr",
            "it",
            "pt",
            "hi",
            "es",
            "th",
            "arxiv:2204.05149",
            "base_model:meta-llama/Llama-3.1-8B",
            "base_model:finetune:meta-llama/Llama-3.1-8B",
            "license:llama3.1",
            "text-generation-inference",
            "endpoints_compatible",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 7540148,
          "likes": 5129
        },
        {
          "id": "arcee-ai/Trinity-Nano-Preview",
          "type": "hf.text_generation",
          "name": "arcee-ai/Trinity-Nano-Preview",
          "repo_id": "arcee-ai/Trinity-Nano-Preview",
          "size_on_disk": 12259308118,
          "pipeline_tag": "text-generation",
          "tags": [
            "transformers",
            "safetensors",
            "afmoe",
            "text-generation",
            "conversational",
            "custom_code",
            "en",
            "es",
            "fr",
            "de",
            "it",
            "pt",
            "ru",
            "ar",
            "hi",
            "ko",
            "zh",
            "base_model:arcee-ai/Trinity-Nano-Base",
            "base_model:finetune:arcee-ai/Trinity-Nano-Base",
            "license:apache-2.0",
            "endpoints_compatible",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 2041,
          "likes": 54
        },
        {
          "id": "open-thoughts/OpenThinker-Agent-v1",
          "type": "hf.text_generation",
          "name": "open-thoughts/OpenThinker-Agent-v1",
          "repo_id": "open-thoughts/OpenThinker-Agent-v1",
          "size_on_disk": 32778911827,
          "pipeline_tag": "text-generation",
          "tags": [
            "transformers",
            "safetensors",
            "qwen3",
            "text-generation",
            "agents",
            "terminal",
            "code",
            "software-engineering",
            "conversational",
            "dataset:OpenThoughts-Agent-v1-SFT",
            "dataset:OpenThoughts-Agent-v1-RL",
            "base_model:Qwen/Qwen3-8B",
            "base_model:finetune:Qwen/Qwen3-8B",
            "license:apache-2.0",
            "text-generation-inference",
            "endpoints_compatible",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 766,
          "likes": 78
        },
        {
          "id": "Qwen/Qwen3-0.6B",
          "type": "hf.text_generation",
          "name": "Qwen/Qwen3-0.6B",
          "repo_id": "Qwen/Qwen3-0.6B",
          "size_on_disk": 1519209243,
          "pipeline_tag": "text-generation",
          "tags": [
            "transformers",
            "safetensors",
            "qwen3",
            "text-generation",
            "conversational",
            "arxiv:2505.09388",
            "base_model:Qwen/Qwen3-0.6B-Base",
            "base_model:finetune:Qwen/Qwen3-0.6B-Base",
            "license:apache-2.0",
            "text-generation-inference",
            "endpoints_compatible",
            "deploy:azure",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 7528189,
          "likes": 871
        },
        {
          "id": "Qwen/Qwen3-4B-Instruct-2507",
          "type": "hf.text_generation",
          "name": "Qwen/Qwen3-4B-Instruct-2507",
          "repo_id": "Qwen/Qwen3-4B-Instruct-2507",
          "size_on_disk": 8060917568,
          "pipeline_tag": "text-generation",
          "tags": [
            "transformers",
            "safetensors",
            "qwen3",
            "text-generation",
            "conversational",
            "arxiv:2505.09388",
            "license:apache-2.0",
            "text-generation-inference",
            "endpoints_compatible",
            "deploy:azure",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 5266571,
          "likes": 553
        },
        {
          "id": "Qwen/Qwen2.5-7B-Instruct",
          "type": "hf.text_generation",
          "name": "Qwen/Qwen2.5-7B-Instruct",
          "repo_id": "Qwen/Qwen2.5-7B-Instruct",
          "size_on_disk": 15242807270,
          "pipeline_tag": "text-generation",
          "tags": [
            "transformers",
            "safetensors",
            "qwen2",
            "text-generation",
            "chat",
            "conversational",
            "en",
            "arxiv:2309.00071",
            "arxiv:2407.10671",
            "base_model:Qwen/Qwen2.5-7B",
            "base_model:finetune:Qwen/Qwen2.5-7B",
            "license:apache-2.0",
            "text-generation-inference",
            "endpoints_compatible",
            "deploy:azure",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 6722803,
          "likes": 942
        },
        {
          "id": "meta-llama/Llama-3.1-8B",
          "type": "hf.text_generation",
          "name": "meta-llama/Llama-3.1-8B",
          "repo_id": "meta-llama/Llama-3.1-8B",
          "size_on_disk": 32132578157,
          "pipeline_tag": "text-generation",
          "tags": [
            "transformers",
            "safetensors",
            "llama",
            "text-generation",
            "facebook",
            "meta",
            "pytorch",
            "llama-3",
            "en",
            "de",
            "fr",
            "it",
            "pt",
            "hi",
            "es",
            "th",
            "arxiv:2204.05149",
            "license:llama3.1",
            "text-generation-inference",
            "endpoints_compatible",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 728824,
          "likes": 1971
        },
        {
          "id": "allenai/Olmo-3-32B-Think",
          "type": "hf.text_generation",
          "name": "allenai/Olmo-3-32B-Think",
          "repo_id": "allenai/Olmo-3-32B-Think",
          "size_on_disk": 64476962889,
          "pipeline_tag": "text-generation",
          "tags": [
            "transformers",
            "safetensors",
            "olmo3",
            "text-generation",
            "conversational",
            "en",
            "dataset:allenai/Dolci-Think-RL",
            "base_model:allenai/Olmo-3-32B-Think-DPO",
            "base_model:finetune:allenai/Olmo-3-32B-Think-DPO",
            "license:apache-2.0",
            "endpoints_compatible",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 12714,
          "likes": 156
        },
        {
          "id": "Qwen/Qwen3-1.7B",
          "type": "hf.text_generation",
          "name": "Qwen/Qwen3-1.7B",
          "repo_id": "Qwen/Qwen3-1.7B",
          "size_on_disk": 4079450110,
          "pipeline_tag": "text-generation",
          "tags": [
            "transformers",
            "safetensors",
            "qwen3",
            "text-generation",
            "conversational",
            "arxiv:2505.09388",
            "base_model:Qwen/Qwen3-1.7B-Base",
            "base_model:finetune:Qwen/Qwen3-1.7B-Base",
            "license:apache-2.0",
            "text-generation-inference",
            "endpoints_compatible",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 4411477,
          "likes": 354
        },
        {
          "id": "HuggingFaceTB/SmolLM3-3B",
          "type": "hf.text_generation",
          "name": "HuggingFaceTB/SmolLM3-3B",
          "repo_id": "HuggingFaceTB/SmolLM3-3B",
          "size_on_disk": 6167865576,
          "pipeline_tag": "text-generation",
          "tags": [
            "transformers",
            "safetensors",
            "smollm3",
            "text-generation",
            "conversational",
            "en",
            "fr",
            "es",
            "it",
            "pt",
            "zh",
            "ar",
            "ru",
            "base_model:HuggingFaceTB/SmolLM3-3B-Base",
            "base_model:finetune:HuggingFaceTB/SmolLM3-3B-Base",
            "license:apache-2.0",
            "endpoints_compatible",
            "deploy:azure",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 89571,
          "likes": 838
        },
        {
          "id": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
          "type": "hf.text_generation",
          "name": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
          "repo_id": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
          "size_on_disk": 2202470207,
          "pipeline_tag": "text-generation",
          "tags": [
            "transformers",
            "safetensors",
            "llama",
            "text-generation",
            "conversational",
            "en",
            "dataset:cerebras/SlimPajama-627B",
            "dataset:bigcode/starcoderdata",
            "dataset:HuggingFaceH4/ultrachat_200k",
            "dataset:HuggingFaceH4/ultrafeedback_binarized",
            "license:apache-2.0",
            "text-generation-inference",
            "endpoints_compatible",
            "deploy:azure",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 1800290,
          "likes": 1480
        },
        {
          "id": "unsloth/Qwen3-32B-bnb-4bit",
          "type": "hf.text_generation",
          "name": "unsloth/Qwen3-32B-bnb-4bit",
          "repo_id": "unsloth/Qwen3-32B-bnb-4bit",
          "size_on_disk": 19228117804,
          "tags": [
            "safetensors",
            "qwen3",
            "unsloth",
            "arxiv:2309.00071",
            "base_model:Qwen/Qwen3-32B",
            "base_model:quantized:Qwen/Qwen3-32B",
            "4-bit",
            "bitsandbytes",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 44406,
          "likes": 7
        },
        {
          "id": "unsloth/Qwen3-30B-A3B-bnb-4bit",
          "type": "hf.text_generation",
          "name": "unsloth/Qwen3-30B-A3B-bnb-4bit",
          "repo_id": "unsloth/Qwen3-30B-A3B-bnb-4bit",
          "size_on_disk": 16739877170,
          "tags": [
            "safetensors",
            "qwen3_moe",
            "unsloth",
            "arxiv:2309.00071",
            "base_model:Qwen/Qwen3-30B-A3B",
            "base_model:quantized:Qwen/Qwen3-30B-A3B",
            "4-bit",
            "bitsandbytes",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 346,
          "likes": 18
        },
        {
          "id": "unsloth/Qwen3-14B-bnb-4bit",
          "type": "hf.text_generation",
          "name": "unsloth/Qwen3-14B-bnb-4bit",
          "repo_id": "unsloth/Qwen3-14B-bnb-4bit",
          "size_on_disk": 9944843575,
          "tags": [
            "safetensors",
            "qwen3",
            "unsloth",
            "arxiv:2309.00071",
            "base_model:Qwen/Qwen3-14B",
            "base_model:quantized:Qwen/Qwen3-14B",
            "license:apache-2.0",
            "4-bit",
            "bitsandbytes",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 3167,
          "likes": 3
        },
        {
          "id": "unsloth/Qwen3-8B-bnb-4bit",
          "type": "hf.text_generation",
          "name": "unsloth/Qwen3-8B-bnb-4bit",
          "repo_id": "unsloth/Qwen3-8B-bnb-4bit",
          "size_on_disk": 6089465198,
          "tags": [
            "safetensors",
            "qwen3",
            "unsloth",
            "arxiv:2309.00071",
            "base_model:Qwen/Qwen3-8B",
            "base_model:quantized:Qwen/Qwen3-8B",
            "license:apache-2.0",
            "4-bit",
            "bitsandbytes",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 97992,
          "likes": 6
        },
        {
          "id": "unsloth/Qwen3-4B-bnb-4bit",
          "type": "hf.text_generation",
          "name": "unsloth/Qwen3-4B-bnb-4bit",
          "repo_id": "unsloth/Qwen3-4B-bnb-4bit",
          "size_on_disk": 2669040650,
          "tags": [
            "safetensors",
            "qwen3",
            "unsloth",
            "arxiv:2309.00071",
            "base_model:Qwen/Qwen3-4B",
            "base_model:quantized:Qwen/Qwen3-4B",
            "4-bit",
            "bitsandbytes",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 15213,
          "likes": 2
        },
        {
          "id": "unsloth/Qwen3-1.7B-bnb-4bit",
          "type": "hf.text_generation",
          "name": "unsloth/Qwen3-1.7B-bnb-4bit",
          "repo_id": "unsloth/Qwen3-1.7B-bnb-4bit",
          "size_on_disk": 1365887534,
          "tags": [
            "safetensors",
            "qwen3",
            "unsloth",
            "base_model:Qwen/Qwen3-1.7B",
            "base_model:quantized:Qwen/Qwen3-1.7B",
            "4-bit",
            "bitsandbytes",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 2000,
          "likes": 3
        },
        {
          "id": "unsloth/Qwen3-0.6B-bnb-4bit",
          "type": "hf.text_generation",
          "name": "unsloth/Qwen3-0.6B-bnb-4bit",
          "repo_id": "unsloth/Qwen3-0.6B-bnb-4bit",
          "size_on_disk": 554787688,
          "tags": [
            "safetensors",
            "qwen3",
            "unsloth",
            "base_model:Qwen/Qwen3-0.6B",
            "base_model:quantized:Qwen/Qwen3-0.6B",
            "4-bit",
            "bitsandbytes",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 12839,
          "likes": 3
        },
        {
          "id": "unsloth/Qwen3-30B-A3B-Base-bnb-4bit",
          "type": "hf.text_generation",
          "name": "unsloth/Qwen3-30B-A3B-Base-bnb-4bit",
          "repo_id": "unsloth/Qwen3-30B-A3B-Base-bnb-4bit",
          "size_on_disk": 16739853430,
          "tags": [
            "safetensors",
            "qwen3_moe",
            "unsloth",
            "base_model:Qwen/Qwen3-30B-A3B-Base",
            "base_model:quantized:Qwen/Qwen3-30B-A3B-Base",
            "4-bit",
            "bitsandbytes",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 79,
          "likes": 0
        },
        {
          "id": "unsloth/Qwen3-14B-Base-bnb-4bit",
          "type": "hf.text_generation",
          "name": "unsloth/Qwen3-14B-Base-bnb-4bit",
          "repo_id": "unsloth/Qwen3-14B-Base-bnb-4bit",
          "size_on_disk": 9944822551,
          "pipeline_tag": "text-generation",
          "tags": [
            "transformers",
            "safetensors",
            "qwen3",
            "text-generation",
            "unsloth",
            "arxiv:2505.09388",
            "base_model:Qwen/Qwen3-14B-Base",
            "base_model:quantized:Qwen/Qwen3-14B-Base",
            "license:apache-2.0",
            "text-generation-inference",
            "endpoints_compatible",
            "4-bit",
            "bitsandbytes",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 356,
          "likes": 0
        },
        {
          "id": "unsloth/Qwen3-8B-Base-bnb-4bit",
          "type": "hf.text_generation",
          "name": "unsloth/Qwen3-8B-Base-bnb-4bit",
          "repo_id": "unsloth/Qwen3-8B-Base-bnb-4bit",
          "size_on_disk": 6089443915,
          "pipeline_tag": "text-generation",
          "tags": [
            "transformers",
            "safetensors",
            "qwen3",
            "text-generation",
            "unsloth",
            "arxiv:2505.09388",
            "base_model:Qwen/Qwen3-8B-Base",
            "base_model:quantized:Qwen/Qwen3-8B-Base",
            "license:apache-2.0",
            "text-generation-inference",
            "endpoints_compatible",
            "4-bit",
            "bitsandbytes",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 3262,
          "likes": 2
        },
        {
          "id": "unsloth/Qwen3-4B-Base-bnb-4bit",
          "type": "hf.text_generation",
          "name": "unsloth/Qwen3-4B-Base-bnb-4bit",
          "repo_id": "unsloth/Qwen3-4B-Base-bnb-4bit",
          "size_on_disk": 2669017930,
          "pipeline_tag": "text-generation",
          "tags": [
            "transformers",
            "safetensors",
            "qwen3",
            "text-generation",
            "unsloth",
            "base_model:Qwen/Qwen3-4B-Base",
            "base_model:quantized:Qwen/Qwen3-4B-Base",
            "license:apache-2.0",
            "text-generation-inference",
            "endpoints_compatible",
            "4-bit",
            "bitsandbytes",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 157,
          "likes": 0
        },
        {
          "id": "unsloth/Qwen3-1.7B-Base-bnb-4bit",
          "type": "hf.text_generation",
          "name": "unsloth/Qwen3-1.7B-Base-bnb-4bit",
          "repo_id": "unsloth/Qwen3-1.7B-Base-bnb-4bit",
          "size_on_disk": 1365867554,
          "pipeline_tag": "text-generation",
          "tags": [
            "transformers",
            "safetensors",
            "qwen3",
            "text-generation",
            "unsloth",
            "base_model:Qwen/Qwen3-1.7B-Base",
            "base_model:quantized:Qwen/Qwen3-1.7B-Base",
            "license:apache-2.0",
            "text-generation-inference",
            "endpoints_compatible",
            "4-bit",
            "bitsandbytes",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 536,
          "likes": 0
        },
        {
          "id": "unsloth/Qwen3-0.6B-Base-bnb-4bit",
          "type": "hf.text_generation",
          "name": "unsloth/Qwen3-0.6B-Base-bnb-4bit",
          "repo_id": "unsloth/Qwen3-0.6B-Base-bnb-4bit",
          "size_on_disk": 554769089,
          "pipeline_tag": "text-generation",
          "tags": [
            "transformers",
            "safetensors",
            "qwen3",
            "text-generation",
            "unsloth",
            "arxiv:2505.09388",
            "base_model:Qwen/Qwen3-0.6B-Base",
            "base_model:quantized:Qwen/Qwen3-0.6B-Base",
            "license:apache-2.0",
            "text-generation-inference",
            "endpoints_compatible",
            "4-bit",
            "bitsandbytes",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 266,
          "likes": 0
        },
        {
          "id": "unsloth/gemma-3-270m-it-bnb-4bit",
          "type": "hf.text_generation",
          "name": "unsloth/gemma-3-270m-it-bnb-4bit",
          "repo_id": "unsloth/gemma-3-270m-it-bnb-4bit",
          "size_on_disk": 426907267,
          "pipeline_tag": "text-generation",
          "tags": [
            "transformers",
            "safetensors",
            "gemma3_text",
            "text-generation",
            "gemma3",
            "unsloth",
            "gemma",
            "google",
            "conversational",
            "arxiv:2503.19786",
            "arxiv:1905.07830",
            "arxiv:1905.10044",
            "arxiv:1911.11641",
            "arxiv:1705.03551",
            "arxiv:1911.01547",
            "arxiv:1907.10641",
            "arxiv:2311.07911",
            "arxiv:2311.12022",
            "arxiv:2411.04368",
            "arxiv:1904.09728",
            "arxiv:1903.00161",
            "arxiv:2009.03300",
            "arxiv:2304.06364",
            "arxiv:2103.03874",
            "arxiv:2110.14168",
            "arxiv:2108.07732",
            "arxiv:2107.03374",
            "arxiv:2403.07974",
            "arxiv:2305.03111",
            "arxiv:2405.04520",
            "arxiv:2210.03057",
            "arxiv:2106.03193",
            "arxiv:1910.11856",
            "arxiv:2502.12404",
            "arxiv:2502.21228",
            "arxiv:2404.16816",
            "arxiv:2104.12756",
            "arxiv:2311.16502",
            "arxiv:2203.10244",
            "arxiv:2404.12390",
            "arxiv:1810.12440",
            "arxiv:1908.02660",
            "arxiv:2310.02255",
            "arxiv:2312.11805",
            "base_model:google/gemma-3-270m-it",
            "base_model:quantized:google/gemma-3-270m-it",
            "license:gemma",
            "text-generation-inference",
            "endpoints_compatible",
            "4-bit",
            "bitsandbytes",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 29139,
          "likes": 3
        },
        {
          "id": "unsloth/gemma-3-1b-it-bnb-4bit",
          "type": "hf.text_generation",
          "name": "unsloth/gemma-3-1b-it-bnb-4bit",
          "repo_id": "unsloth/gemma-3-1b-it-bnb-4bit",
          "size_on_disk": 1003838628,
          "pipeline_tag": "text-generation",
          "tags": [
            "transformers",
            "safetensors",
            "gemma3_text",
            "text-generation",
            "unsloth",
            "gemma3",
            "gemma",
            "google",
            "conversational",
            "en",
            "arxiv:1905.07830",
            "arxiv:1905.10044",
            "arxiv:1911.11641",
            "arxiv:1904.09728",
            "arxiv:1705.03551",
            "arxiv:1911.01547",
            "arxiv:1907.10641",
            "arxiv:1903.00161",
            "arxiv:2009.03300",
            "arxiv:2304.06364",
            "arxiv:2103.03874",
            "arxiv:2110.14168",
            "arxiv:2311.12022",
            "arxiv:2108.07732",
            "arxiv:2107.03374",
            "arxiv:2210.03057",
            "arxiv:2106.03193",
            "arxiv:1910.11856",
            "arxiv:2502.12404",
            "arxiv:2502.21228",
            "arxiv:2404.16816",
            "arxiv:2104.12756",
            "arxiv:2311.16502",
            "arxiv:2203.10244",
            "arxiv:2404.12390",
            "arxiv:1810.12440",
            "arxiv:1908.02660",
            "arxiv:2312.11805",
            "base_model:google/gemma-3-1b-it",
            "base_model:quantized:google/gemma-3-1b-it",
            "license:gemma",
            "text-generation-inference",
            "endpoints_compatible",
            "4-bit",
            "bitsandbytes",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 3784,
          "likes": 9
        },
        {
          "id": "unsloth/gemma-3-4b-it-bnb-4bit",
          "type": "hf.text_generation",
          "name": "unsloth/gemma-3-4b-it-bnb-4bit",
          "repo_id": "unsloth/gemma-3-4b-it-bnb-4bit",
          "size_on_disk": 3268199247,
          "pipeline_tag": "any-to-any",
          "tags": [
            "transformers",
            "safetensors",
            "gemma3",
            "any-to-any",
            "unsloth",
            "gemma",
            "google",
            "en",
            "arxiv:1905.07830",
            "arxiv:1905.10044",
            "arxiv:1911.11641",
            "arxiv:1904.09728",
            "arxiv:1705.03551",
            "arxiv:1911.01547",
            "arxiv:1907.10641",
            "arxiv:1903.00161",
            "arxiv:2009.03300",
            "arxiv:2304.06364",
            "arxiv:2103.03874",
            "arxiv:2110.14168",
            "arxiv:2311.12022",
            "arxiv:2108.07732",
            "arxiv:2107.03374",
            "arxiv:2210.03057",
            "arxiv:2106.03193",
            "arxiv:1910.11856",
            "arxiv:2502.12404",
            "arxiv:2502.21228",
            "arxiv:2404.16816",
            "arxiv:2104.12756",
            "arxiv:2311.16502",
            "arxiv:2203.10244",
            "arxiv:2404.12390",
            "arxiv:1810.12440",
            "arxiv:1908.02660",
            "arxiv:2312.11805",
            "base_model:google/gemma-3-4b-it",
            "base_model:quantized:google/gemma-3-4b-it",
            "license:gemma",
            "text-generation-inference",
            "endpoints_compatible",
            "4-bit",
            "bitsandbytes",
            "deploy:azure",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 10538,
          "likes": 11
        },
        {
          "id": "unsloth/gemma-3-12b-it-bnb-4bit",
          "type": "hf.text_generation",
          "name": "unsloth/gemma-3-12b-it-bnb-4bit",
          "repo_id": "unsloth/gemma-3-12b-it-bnb-4bit",
          "size_on_disk": 7838487378,
          "pipeline_tag": "any-to-any",
          "tags": [
            "transformers",
            "safetensors",
            "gemma3",
            "any-to-any",
            "unsloth",
            "gemma",
            "google",
            "en",
            "arxiv:1905.07830",
            "arxiv:1905.10044",
            "arxiv:1911.11641",
            "arxiv:1904.09728",
            "arxiv:1705.03551",
            "arxiv:1911.01547",
            "arxiv:1907.10641",
            "arxiv:1903.00161",
            "arxiv:2009.03300",
            "arxiv:2304.06364",
            "arxiv:2103.03874",
            "arxiv:2110.14168",
            "arxiv:2311.12022",
            "arxiv:2108.07732",
            "arxiv:2107.03374",
            "arxiv:2210.03057",
            "arxiv:2106.03193",
            "arxiv:1910.11856",
            "arxiv:2502.12404",
            "arxiv:2502.21228",
            "arxiv:2404.16816",
            "arxiv:2104.12756",
            "arxiv:2311.16502",
            "arxiv:2203.10244",
            "arxiv:2404.12390",
            "arxiv:1810.12440",
            "arxiv:1908.02660",
            "arxiv:2312.11805",
            "base_model:google/gemma-3-12b-it",
            "base_model:quantized:google/gemma-3-12b-it",
            "license:gemma",
            "text-generation-inference",
            "endpoints_compatible",
            "4-bit",
            "bitsandbytes",
            "deploy:azure",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 5538,
          "likes": 5
        },
        {
          "id": "unsloth/gemma-3-27b-it-bnb-4bit",
          "type": "hf.text_generation",
          "name": "unsloth/gemma-3-27b-it-bnb-4bit",
          "repo_id": "unsloth/gemma-3-27b-it-bnb-4bit",
          "size_on_disk": 16304476759,
          "pipeline_tag": "any-to-any",
          "tags": [
            "transformers",
            "safetensors",
            "gemma3",
            "any-to-any",
            "unsloth",
            "gemma",
            "google",
            "en",
            "arxiv:1905.07830",
            "arxiv:1905.10044",
            "arxiv:1911.11641",
            "arxiv:1904.09728",
            "arxiv:1705.03551",
            "arxiv:1911.01547",
            "arxiv:1907.10641",
            "arxiv:1903.00161",
            "arxiv:2009.03300",
            "arxiv:2304.06364",
            "arxiv:2103.03874",
            "arxiv:2110.14168",
            "arxiv:2311.12022",
            "arxiv:2108.07732",
            "arxiv:2107.03374",
            "arxiv:2210.03057",
            "arxiv:2106.03193",
            "arxiv:1910.11856",
            "arxiv:2502.12404",
            "arxiv:2502.21228",
            "arxiv:2404.16816",
            "arxiv:2104.12756",
            "arxiv:2311.16502",
            "arxiv:2203.10244",
            "arxiv:2404.12390",
            "arxiv:1810.12440",
            "arxiv:1908.02660",
            "arxiv:2312.11805",
            "base_model:google/gemma-3-27b-it",
            "base_model:quantized:google/gemma-3-27b-it",
            "license:gemma",
            "text-generation-inference",
            "endpoints_compatible",
            "4-bit",
            "bitsandbytes",
            "deploy:azure",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 5766,
          "likes": 18
        },
        {
          "id": "unsloth/gemma-3-1b-pt-bnb-4bit",
          "type": "hf.text_generation",
          "name": "unsloth/gemma-3-1b-pt-bnb-4bit",
          "repo_id": "unsloth/gemma-3-1b-pt-bnb-4bit",
          "size_on_disk": 1003835467,
          "pipeline_tag": "text-generation",
          "tags": [
            "transformers",
            "safetensors",
            "gemma3_text",
            "text-generation",
            "unsloth",
            "gemma3",
            "gemma",
            "google",
            "en",
            "arxiv:1905.07830",
            "arxiv:1905.10044",
            "arxiv:1911.11641",
            "arxiv:1904.09728",
            "arxiv:1705.03551",
            "arxiv:1911.01547",
            "arxiv:1907.10641",
            "arxiv:1903.00161",
            "arxiv:2009.03300",
            "arxiv:2304.06364",
            "arxiv:2103.03874",
            "arxiv:2110.14168",
            "arxiv:2311.12022",
            "arxiv:2108.07732",
            "arxiv:2107.03374",
            "arxiv:2210.03057",
            "arxiv:2106.03193",
            "arxiv:1910.11856",
            "arxiv:2502.12404",
            "arxiv:2502.21228",
            "arxiv:2404.16816",
            "arxiv:2104.12756",
            "arxiv:2311.16502",
            "arxiv:2203.10244",
            "arxiv:2404.12390",
            "arxiv:1810.12440",
            "arxiv:1908.02660",
            "arxiv:2312.11805",
            "base_model:google/gemma-3-1b-pt",
            "base_model:quantized:google/gemma-3-1b-pt",
            "license:gemma",
            "text-generation-inference",
            "endpoints_compatible",
            "4-bit",
            "bitsandbytes",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 121,
          "likes": 3
        },
        {
          "id": "unsloth/gemma-3-4b-pt-bnb-4bit",
          "type": "hf.text_generation",
          "name": "unsloth/gemma-3-4b-pt-bnb-4bit",
          "repo_id": "unsloth/gemma-3-4b-pt-bnb-4bit",
          "size_on_disk": 3268193026,
          "pipeline_tag": "any-to-any",
          "tags": [
            "transformers",
            "safetensors",
            "gemma3",
            "any-to-any",
            "unsloth",
            "gemma",
            "google",
            "en",
            "arxiv:1905.07830",
            "arxiv:1905.10044",
            "arxiv:1911.11641",
            "arxiv:1904.09728",
            "arxiv:1705.03551",
            "arxiv:1911.01547",
            "arxiv:1907.10641",
            "arxiv:1903.00161",
            "arxiv:2009.03300",
            "arxiv:2304.06364",
            "arxiv:2103.03874",
            "arxiv:2110.14168",
            "arxiv:2311.12022",
            "arxiv:2108.07732",
            "arxiv:2107.03374",
            "arxiv:2210.03057",
            "arxiv:2106.03193",
            "arxiv:1910.11856",
            "arxiv:2502.12404",
            "arxiv:2502.21228",
            "arxiv:2404.16816",
            "arxiv:2104.12756",
            "arxiv:2311.16502",
            "arxiv:2203.10244",
            "arxiv:2404.12390",
            "arxiv:1810.12440",
            "arxiv:1908.02660",
            "arxiv:2312.11805",
            "base_model:google/gemma-3-4b-pt",
            "base_model:quantized:google/gemma-3-4b-pt",
            "license:gemma",
            "text-generation-inference",
            "endpoints_compatible",
            "4-bit",
            "bitsandbytes",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 1527,
          "likes": 1
        },
        {
          "id": "unsloth/gemma-3-12b-pt-bnb-4bit",
          "type": "hf.text_generation",
          "name": "unsloth/gemma-3-12b-pt-bnb-4bit",
          "repo_id": "unsloth/gemma-3-12b-pt-bnb-4bit",
          "size_on_disk": 7838481139,
          "pipeline_tag": "any-to-any",
          "tags": [
            "transformers",
            "safetensors",
            "gemma3",
            "any-to-any",
            "unsloth",
            "gemma",
            "google",
            "en",
            "arxiv:1905.07830",
            "arxiv:1905.10044",
            "arxiv:1911.11641",
            "arxiv:1904.09728",
            "arxiv:1705.03551",
            "arxiv:1911.01547",
            "arxiv:1907.10641",
            "arxiv:1903.00161",
            "arxiv:2009.03300",
            "arxiv:2304.06364",
            "arxiv:2103.03874",
            "arxiv:2110.14168",
            "arxiv:2311.12022",
            "arxiv:2108.07732",
            "arxiv:2107.03374",
            "arxiv:2210.03057",
            "arxiv:2106.03193",
            "arxiv:1910.11856",
            "arxiv:2502.12404",
            "arxiv:2502.21228",
            "arxiv:2404.16816",
            "arxiv:2104.12756",
            "arxiv:2311.16502",
            "arxiv:2203.10244",
            "arxiv:2404.12390",
            "arxiv:1810.12440",
            "arxiv:1908.02660",
            "arxiv:2312.11805",
            "base_model:google/gemma-3-12b-pt",
            "base_model:quantized:google/gemma-3-12b-pt",
            "license:gemma",
            "text-generation-inference",
            "endpoints_compatible",
            "4-bit",
            "bitsandbytes",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 312,
          "likes": 1
        },
        {
          "id": "unsloth/gemma-3-27b-pt-bnb-4bit",
          "type": "hf.text_generation",
          "name": "unsloth/gemma-3-27b-pt-bnb-4bit",
          "repo_id": "unsloth/gemma-3-27b-pt-bnb-4bit",
          "size_on_disk": 16304470526,
          "pipeline_tag": "any-to-any",
          "tags": [
            "transformers",
            "safetensors",
            "gemma3",
            "any-to-any",
            "unsloth",
            "gemma",
            "google",
            "en",
            "arxiv:1905.07830",
            "arxiv:1905.10044",
            "arxiv:1911.11641",
            "arxiv:1904.09728",
            "arxiv:1705.03551",
            "arxiv:1911.01547",
            "arxiv:1907.10641",
            "arxiv:1903.00161",
            "arxiv:2009.03300",
            "arxiv:2304.06364",
            "arxiv:2103.03874",
            "arxiv:2110.14168",
            "arxiv:2311.12022",
            "arxiv:2108.07732",
            "arxiv:2107.03374",
            "arxiv:2210.03057",
            "arxiv:2106.03193",
            "arxiv:1910.11856",
            "arxiv:2502.12404",
            "arxiv:2502.21228",
            "arxiv:2404.16816",
            "arxiv:2104.12756",
            "arxiv:2311.16502",
            "arxiv:2203.10244",
            "arxiv:2404.12390",
            "arxiv:1810.12440",
            "arxiv:1908.02660",
            "arxiv:2312.11805",
            "base_model:google/gemma-3-27b-pt",
            "base_model:quantized:google/gemma-3-27b-pt",
            "license:gemma",
            "text-generation-inference",
            "endpoints_compatible",
            "4-bit",
            "bitsandbytes",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 135,
          "likes": 1
        },
        {
          "id": "unsloth/Ministral-3-14B-Instruct-2512-bnb-4bit",
          "type": "hf.text_generation",
          "name": "unsloth/Ministral-3-14B-Instruct-2512-bnb-4bit",
          "repo_id": "unsloth/Ministral-3-14B-Instruct-2512-bnb-4bit",
          "size_on_disk": 9855986286,
          "tags": [
            "vllm",
            "safetensors",
            "mistral3",
            "mistral-common",
            "mistral",
            "unsloth",
            "en",
            "fr",
            "es",
            "de",
            "it",
            "pt",
            "nl",
            "zh",
            "ja",
            "ko",
            "ar",
            "base_model:mistralai/Ministral-3-14B-Instruct-2512",
            "base_model:quantized:mistralai/Ministral-3-14B-Instruct-2512",
            "license:apache-2.0",
            "4-bit",
            "bitsandbytes",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 320,
          "likes": 1
        },
        {
          "id": "unsloth/Ministral-3-14B-Instruct-2512-unsloth-bnb-4bit",
          "type": "hf.text_generation",
          "name": "unsloth/Ministral-3-14B-Instruct-2512-unsloth-bnb-4bit",
          "repo_id": "unsloth/Ministral-3-14B-Instruct-2512-unsloth-bnb-4bit",
          "size_on_disk": 13839776308,
          "tags": [
            "vllm",
            "safetensors",
            "mistral3",
            "mistral-common",
            "mistral",
            "unsloth",
            "en",
            "fr",
            "es",
            "de",
            "it",
            "pt",
            "nl",
            "zh",
            "ja",
            "ko",
            "ar",
            "base_model:mistralai/Ministral-3-14B-Instruct-2512",
            "base_model:quantized:mistralai/Ministral-3-14B-Instruct-2512",
            "license:apache-2.0",
            "4-bit",
            "bitsandbytes",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 183,
          "likes": 0
        },
        {
          "id": "unsloth/Ministral-3-14B-Reasoning-2512-bnb-4bit",
          "type": "hf.text_generation",
          "name": "unsloth/Ministral-3-14B-Reasoning-2512-bnb-4bit",
          "repo_id": "unsloth/Ministral-3-14B-Reasoning-2512-bnb-4bit",
          "size_on_disk": 9855860589,
          "tags": [
            "vllm",
            "safetensors",
            "mistral3",
            "mistral-common",
            "unsloth",
            "en",
            "fr",
            "es",
            "de",
            "it",
            "pt",
            "nl",
            "zh",
            "ja",
            "ko",
            "ar",
            "base_model:mistralai/Ministral-3-14B-Reasoning-2512",
            "base_model:quantized:mistralai/Ministral-3-14B-Reasoning-2512",
            "license:apache-2.0",
            "4-bit",
            "bitsandbytes",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 580,
          "likes": 0
        },
        {
          "id": "unsloth/Ministral-3-14B-Reasoning-2512-unsloth-bnb-4bit",
          "type": "hf.text_generation",
          "name": "unsloth/Ministral-3-14B-Reasoning-2512-unsloth-bnb-4bit",
          "repo_id": "unsloth/Ministral-3-14B-Reasoning-2512-unsloth-bnb-4bit",
          "size_on_disk": 13761849375,
          "tags": [
            "vllm",
            "safetensors",
            "mistral3",
            "mistral-common",
            "unsloth",
            "en",
            "fr",
            "es",
            "de",
            "it",
            "pt",
            "nl",
            "zh",
            "ja",
            "ko",
            "ar",
            "base_model:mistralai/Ministral-3-14B-Reasoning-2512",
            "base_model:quantized:mistralai/Ministral-3-14B-Reasoning-2512",
            "license:apache-2.0",
            "4-bit",
            "bitsandbytes",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 169,
          "likes": 0
        },
        {
          "id": "unsloth/Ministral-3-8B-Instruct-2512-bnb-4bit",
          "type": "hf.text_generation",
          "name": "unsloth/Ministral-3-8B-Instruct-2512-bnb-4bit",
          "repo_id": "unsloth/Ministral-3-8B-Instruct-2512-bnb-4bit",
          "size_on_disk": 6848255496,
          "tags": [
            "vllm",
            "safetensors",
            "mistral3",
            "mistral-common",
            "mistral",
            "unsloth",
            "en",
            "fr",
            "es",
            "de",
            "it",
            "pt",
            "nl",
            "zh",
            "ja",
            "ko",
            "ar",
            "base_model:mistralai/Ministral-3-8B-Instruct-2512",
            "base_model:quantized:mistralai/Ministral-3-8B-Instruct-2512",
            "license:apache-2.0",
            "4-bit",
            "bitsandbytes",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 67,
          "likes": 0
        },
        {
          "id": "unsloth/Ministral-3-8B-Instruct-2512-unsloth-bnb-4bit",
          "type": "hf.text_generation",
          "name": "unsloth/Ministral-3-8B-Instruct-2512-unsloth-bnb-4bit",
          "repo_id": "unsloth/Ministral-3-8B-Instruct-2512-unsloth-bnb-4bit",
          "size_on_disk": 7421075892,
          "tags": [
            "vllm",
            "safetensors",
            "mistral3",
            "mistral-common",
            "mistral",
            "unsloth",
            "en",
            "fr",
            "es",
            "de",
            "it",
            "pt",
            "nl",
            "zh",
            "ja",
            "ko",
            "ar",
            "base_model:mistralai/Ministral-3-8B-Instruct-2512",
            "base_model:quantized:mistralai/Ministral-3-8B-Instruct-2512",
            "license:apache-2.0",
            "4-bit",
            "bitsandbytes",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 181,
          "likes": 1
        },
        {
          "id": "unsloth/Ministral-3-8B-Reasoning-2512-bnb-4bit",
          "type": "hf.text_generation",
          "name": "unsloth/Ministral-3-8B-Reasoning-2512-bnb-4bit",
          "repo_id": "unsloth/Ministral-3-8B-Reasoning-2512-bnb-4bit",
          "size_on_disk": 6848129651,
          "tags": [
            "vllm",
            "safetensors",
            "mistral3",
            "mistral-common",
            "unsloth",
            "en",
            "fr",
            "es",
            "de",
            "it",
            "pt",
            "nl",
            "zh",
            "ja",
            "ko",
            "ar",
            "base_model:mistralai/Ministral-3-8B-Reasoning-2512",
            "base_model:quantized:mistralai/Ministral-3-8B-Reasoning-2512",
            "license:apache-2.0",
            "4-bit",
            "bitsandbytes",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 199,
          "likes": 0
        },
        {
          "id": "unsloth/Ministral-3-8B-Reasoning-2512-unsloth-bnb-4bit",
          "type": "hf.text_generation",
          "name": "unsloth/Ministral-3-8B-Reasoning-2512-unsloth-bnb-4bit",
          "repo_id": "unsloth/Ministral-3-8B-Reasoning-2512-unsloth-bnb-4bit",
          "size_on_disk": 7358711000,
          "tags": [
            "vllm",
            "safetensors",
            "mistral3",
            "mistral-common",
            "unsloth",
            "en",
            "fr",
            "es",
            "de",
            "it",
            "pt",
            "nl",
            "zh",
            "ja",
            "ko",
            "ar",
            "base_model:mistralai/Ministral-3-8B-Reasoning-2512",
            "base_model:quantized:mistralai/Ministral-3-8B-Reasoning-2512",
            "license:apache-2.0",
            "4-bit",
            "bitsandbytes",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 391,
          "likes": 0
        },
        {
          "id": "unsloth/Ministral-3-3B-Instruct-2512-bnb-4bit",
          "type": "hf.text_generation",
          "name": "unsloth/Ministral-3-3B-Instruct-2512-bnb-4bit",
          "repo_id": "unsloth/Ministral-3-3B-Instruct-2512-bnb-4bit",
          "size_on_disk": 3224617500,
          "tags": [
            "vllm",
            "safetensors",
            "mistral3",
            "mistral-common",
            "mistral",
            "unsloth",
            "en",
            "fr",
            "es",
            "de",
            "it",
            "pt",
            "nl",
            "zh",
            "ja",
            "ko",
            "ar",
            "base_model:mistralai/Ministral-3-3B-Instruct-2512",
            "base_model:quantized:mistralai/Ministral-3-3B-Instruct-2512",
            "license:apache-2.0",
            "4-bit",
            "bitsandbytes",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 373,
          "likes": 0
        },
        {
          "id": "unsloth/Ministral-3-3B-Instruct-2512-unsloth-bnb-4bit",
          "type": "hf.text_generation",
          "name": "unsloth/Ministral-3-3B-Instruct-2512-unsloth-bnb-4bit",
          "repo_id": "unsloth/Ministral-3-3B-Instruct-2512-unsloth-bnb-4bit",
          "size_on_disk": 3224617559,
          "tags": [
            "vllm",
            "safetensors",
            "mistral3",
            "mistral-common",
            "mistral",
            "unsloth",
            "en",
            "fr",
            "es",
            "de",
            "it",
            "pt",
            "nl",
            "zh",
            "ja",
            "ko",
            "ar",
            "base_model:mistralai/Ministral-3-3B-Instruct-2512",
            "base_model:quantized:mistralai/Ministral-3-3B-Instruct-2512",
            "license:apache-2.0",
            "4-bit",
            "bitsandbytes",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 284,
          "likes": 1
        },
        {
          "id": "unsloth/Ministral-3-3B-Reasoning-2512-bnb-4bit",
          "type": "hf.text_generation",
          "name": "unsloth/Ministral-3-3B-Reasoning-2512-bnb-4bit",
          "repo_id": "unsloth/Ministral-3-3B-Reasoning-2512-bnb-4bit",
          "size_on_disk": 3224485705,
          "tags": [
            "vllm",
            "safetensors",
            "mistral3",
            "mistral-common",
            "unsloth",
            "en",
            "fr",
            "es",
            "de",
            "it",
            "pt",
            "nl",
            "zh",
            "ja",
            "ko",
            "ar",
            "base_model:mistralai/Ministral-3-3B-Reasoning-2512",
            "base_model:quantized:mistralai/Ministral-3-3B-Reasoning-2512",
            "license:apache-2.0",
            "4-bit",
            "bitsandbytes",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 145,
          "likes": 0
        },
        {
          "id": "unsloth/Ministral-3-3B-Reasoning-2512-unsloth-bnb-4bit",
          "type": "hf.text_generation",
          "name": "unsloth/Ministral-3-3B-Reasoning-2512-unsloth-bnb-4bit",
          "repo_id": "unsloth/Ministral-3-3B-Reasoning-2512-unsloth-bnb-4bit",
          "size_on_disk": 3350534928,
          "tags": [
            "vllm",
            "safetensors",
            "mistral3",
            "mistral-common",
            "unsloth",
            "en",
            "fr",
            "es",
            "de",
            "it",
            "pt",
            "nl",
            "zh",
            "ja",
            "ko",
            "ar",
            "base_model:mistralai/Ministral-3-3B-Reasoning-2512",
            "base_model:quantized:mistralai/Ministral-3-3B-Reasoning-2512",
            "license:apache-2.0",
            "4-bit",
            "bitsandbytes",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 1921,
          "likes": 0
        },
        {
          "id": "unsloth/Ministral-3-14B-Base-2512-bnb-4bit",
          "type": "hf.text_generation",
          "name": "unsloth/Ministral-3-14B-Base-2512-bnb-4bit",
          "repo_id": "unsloth/Ministral-3-14B-Base-2512-bnb-4bit",
          "size_on_disk": 9855836945,
          "tags": [
            "vllm",
            "safetensors",
            "mistral3",
            "mistral-common",
            "unsloth",
            "en",
            "fr",
            "es",
            "de",
            "it",
            "pt",
            "nl",
            "zh",
            "ja",
            "ko",
            "ar",
            "base_model:mistralai/Ministral-3-14B-Base-2512",
            "base_model:quantized:mistralai/Ministral-3-14B-Base-2512",
            "license:apache-2.0",
            "4-bit",
            "bitsandbytes",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 149,
          "likes": 2
        },
        {
          "id": "unsloth/Ministral-3-14B-Base-2512-unsloth-bnb-4bit",
          "type": "hf.text_generation",
          "name": "unsloth/Ministral-3-14B-Base-2512-unsloth-bnb-4bit",
          "repo_id": "unsloth/Ministral-3-14B-Base-2512-unsloth-bnb-4bit",
          "size_on_disk": 13170455458,
          "tags": [
            "vllm",
            "safetensors",
            "mistral3",
            "mistral-common",
            "unsloth",
            "en",
            "fr",
            "es",
            "de",
            "it",
            "pt",
            "nl",
            "zh",
            "ja",
            "ko",
            "ar",
            "base_model:mistralai/Ministral-3-14B-Base-2512",
            "base_model:quantized:mistralai/Ministral-3-14B-Base-2512",
            "license:apache-2.0",
            "4-bit",
            "bitsandbytes",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 104,
          "likes": 0
        },
        {
          "id": "unsloth/Ministral-3-8B-Base-2512-bnb-4bit",
          "type": "hf.text_generation",
          "name": "unsloth/Ministral-3-8B-Base-2512-bnb-4bit",
          "repo_id": "unsloth/Ministral-3-8B-Base-2512-bnb-4bit",
          "size_on_disk": 6848106041,
          "tags": [
            "vllm",
            "safetensors",
            "mistral3",
            "mistral-common",
            "unsloth",
            "en",
            "fr",
            "es",
            "de",
            "it",
            "pt",
            "nl",
            "zh",
            "ja",
            "ko",
            "ar",
            "base_model:mistralai/Ministral-3-8B-Base-2512",
            "base_model:quantized:mistralai/Ministral-3-8B-Base-2512",
            "license:apache-2.0",
            "4-bit",
            "bitsandbytes",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 50,
          "likes": 1
        },
        {
          "id": "unsloth/Ministral-3-8B-Base-2512-unsloth-bnb-4bit",
          "type": "hf.text_generation",
          "name": "unsloth/Ministral-3-8B-Base-2512-unsloth-bnb-4bit",
          "repo_id": "unsloth/Ministral-3-8B-Base-2512-unsloth-bnb-4bit",
          "size_on_disk": 8329719465,
          "tags": [
            "vllm",
            "safetensors",
            "mistral3",
            "mistral-common",
            "unsloth",
            "en",
            "fr",
            "es",
            "de",
            "it",
            "pt",
            "nl",
            "zh",
            "ja",
            "ko",
            "ar",
            "base_model:mistralai/Ministral-3-8B-Base-2512",
            "base_model:quantized:mistralai/Ministral-3-8B-Base-2512",
            "license:apache-2.0",
            "4-bit",
            "bitsandbytes",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 199,
          "likes": 0
        },
        {
          "id": "unsloth/Ministral-3-3B-Base-2512-bnb-4bit",
          "type": "hf.text_generation",
          "name": "unsloth/Ministral-3-3B-Base-2512-bnb-4bit",
          "repo_id": "unsloth/Ministral-3-3B-Base-2512-bnb-4bit",
          "size_on_disk": 3224468129,
          "tags": [
            "vllm",
            "safetensors",
            "mistral3",
            "mistral-common",
            "unsloth",
            "en",
            "fr",
            "es",
            "de",
            "it",
            "pt",
            "nl",
            "zh",
            "ja",
            "ko",
            "ar",
            "base_model:mistralai/Ministral-3-3B-Base-2512",
            "base_model:quantized:mistralai/Ministral-3-3B-Base-2512",
            "license:apache-2.0",
            "4-bit",
            "bitsandbytes",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 413,
          "likes": 1
        },
        {
          "id": "unsloth/Ministral-3-3B-Base-2512-unsloth-bnb-4bit",
          "type": "hf.text_generation",
          "name": "unsloth/Ministral-3-3B-Base-2512-unsloth-bnb-4bit",
          "repo_id": "unsloth/Ministral-3-3B-Base-2512-unsloth-bnb-4bit",
          "size_on_disk": 3789332363,
          "tags": [
            "vllm",
            "safetensors",
            "mistral3",
            "mistral-common",
            "unsloth",
            "en",
            "fr",
            "es",
            "de",
            "it",
            "pt",
            "nl",
            "zh",
            "ja",
            "ko",
            "ar",
            "base_model:mistralai/Ministral-3-3B-Base-2512",
            "base_model:quantized:mistralai/Ministral-3-3B-Base-2512",
            "license:apache-2.0",
            "4-bit",
            "bitsandbytes",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 30,
          "likes": 0
        }
      ],
      "basic_fields": [
        "model",
        "prompt"
      ],
      "is_streaming_output": true
    },
    {
      "title": "Chunks To SRT",
      "description": "Converts Whisper audio chunks to SubRip Subtitle (SRT) format for video captioning.\n    subtitle, srt, whisper, transcription, captions\n\n    Use cases:\n    - Generate .srt subtitle files for video players\n    - Create closed captions for accessibility compliance\n    - Convert Whisper transcription output to industry-standard format\n    - Build automated subtitle generation pipelines",
      "namespace": "huggingface.automatic_speech_recognition",
      "node_type": "huggingface.automatic_speech_recognition.ChunksToSRT",
      "properties": [
        {
          "name": "chunks",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "audio_chunk"
              }
            ]
          },
          "default": [],
          "title": "Audio Chunks",
          "description": "List of timestamped audio chunks from Whisper transcription output."
        },
        {
          "name": "time_offset",
          "type": {
            "type": "float"
          },
          "default": 0.0,
          "title": "Time Offset",
          "description": "Offset in seconds to add to all timestamps (useful when audio is from a clip within a longer video)."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "str"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "chunks",
        "time_offset"
      ]
    },
    {
      "title": "Whisper",
      "description": "Converts speech to text using OpenAI's Whisper models with multilingual support.\n    asr, automatic-speech-recognition, speech-to-text, translate, transcribe, audio, huggingface\n\n    Use cases:\n    - Transcribe audio files into text for documentation or analysis\n    - Enable voice input for chatbots and virtual assistants\n    - Create subtitles and closed captions for videos\n    - Translate speech from one language to English\n    - Build voice-controlled applications\n\n    **Note:** Language selection follows Whisper's FLEURS benchmark word error rate ranking.\n    Multiple model variants are available, optimized for different speed/accuracy trade-offs.\n\n    **Links:**\n    - https://github.com/openai/whisper\n    - https://platform.openai.com/docs/guides/speech-to-text/supported-languages",
      "namespace": "huggingface.automatic_speech_recognition",
      "node_type": "huggingface.automatic_speech_recognition.Whisper",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.automatic_speech_recognition"
          },
          "default": {
            "type": "hf.automatic_speech_recognition",
            "repo_id": "",
            "path": null,
            "variant": null,
            "allow_patterns": null,
            "ignore_patterns": null
          },
          "title": "Model",
          "description": "The Whisper model variant to use. Larger models (large-v3) offer better accuracy; smaller models (small, tiny) are faster. Turbo variants balance speed and quality."
        },
        {
          "name": "audio",
          "type": {
            "type": "audio"
          },
          "default": {
            "type": "audio",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Audio Input",
          "description": "The audio file to transcribe. Supports WAV, MP3, FLAC and other common formats."
        },
        {
          "name": "task",
          "type": {
            "type": "enum",
            "values": [
              "transcribe",
              "translate"
            ],
            "type_name": "nodetool.nodes.huggingface.automatic_speech_recognition.Task"
          },
          "default": "transcribe",
          "title": "Task",
          "description": "Choose 'transcribe' for speech-to-text in the original language, or 'translate' to convert any language to English."
        },
        {
          "name": "language",
          "type": {
            "type": "enum",
            "values": [
              "auto_detect",
              "spanish",
              "italian",
              "korean",
              "portuguese",
              "english",
              "japanese",
              "german",
              "russian",
              "dutch",
              "polish",
              "catalan",
              "french",
              "indonesian",
              "ukrainian",
              "turkish",
              "malay",
              "swedish",
              "mandarin",
              "finnish",
              "norwegian",
              "romanian",
              "thai",
              "vietnamese",
              "slovak",
              "arabic",
              "czech",
              "croatian",
              "greek",
              "serbian",
              "danish",
              "bulgarian",
              "hungarian",
              "filipino",
              "bosnian",
              "galician",
              "macedonian",
              "hindi",
              "estonian",
              "slovenian",
              "tamil",
              "latvian",
              "azerbaijani",
              "urdu",
              "lithuanian",
              "hebrew",
              "welsh",
              "persian",
              "icelandic",
              "kazakh",
              "afrikaans",
              "kannada",
              "marathi",
              "swahili",
              "telugu",
              "maori",
              "nepali",
              "armenian",
              "belarusian",
              "gujarati",
              "punjabi",
              "bengali"
            ],
            "type_name": "nodetool.nodes.huggingface.automatic_speech_recognition.WhisperLanguage"
          },
          "default": "auto_detect",
          "title": "Language",
          "description": "Specify the audio's language for better accuracy, or use 'auto_detect' to let the model identify it automatically."
        },
        {
          "name": "timestamps",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "word",
              "sentence"
            ],
            "type_name": "nodetool.nodes.huggingface.automatic_speech_recognition.Timestamps"
          },
          "default": "none",
          "title": "Timestamps",
          "description": "Choose 'word' for word-level timing, 'sentence' for phrase-level timing, or 'none' to disable timestamps."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "str"
          },
          "name": "text"
        },
        {
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "audio_chunk"
              }
            ]
          },
          "name": "chunks"
        }
      ],
      "recommended_models": [
        {
          "id": "openai/whisper-large-v3",
          "type": "hf.automatic_speech_recognition",
          "name": "openai/whisper-large-v3",
          "repo_id": "openai/whisper-large-v3",
          "allow_patterns": [
            "model.safetensors",
            "*.json",
            "*.txt"
          ],
          "size_on_disk": 3091755614,
          "pipeline_tag": "automatic-speech-recognition",
          "tags": [
            "transformers",
            "pytorch",
            "jax",
            "safetensors",
            "whisper",
            "automatic-speech-recognition",
            "audio",
            "hf-asr-leaderboard",
            "en",
            "zh",
            "de",
            "es",
            "ru",
            "ko",
            "fr",
            "ja",
            "pt",
            "tr",
            "pl",
            "ca",
            "nl",
            "ar",
            "sv",
            "it",
            "id",
            "hi",
            "fi",
            "vi",
            "he",
            "uk",
            "el",
            "ms",
            "cs",
            "ro",
            "da",
            "hu",
            "ta",
            "no",
            "th",
            "ur",
            "hr",
            "bg",
            "lt",
            "la",
            "mi",
            "ml",
            "cy",
            "sk",
            "te",
            "fa",
            "lv",
            "bn",
            "sr",
            "az",
            "sl",
            "kn",
            "et",
            "mk",
            "br",
            "eu",
            "is",
            "hy",
            "ne",
            "mn",
            "bs",
            "kk",
            "sq",
            "sw",
            "gl",
            "mr",
            "pa",
            "si",
            "km",
            "sn",
            "yo",
            "so",
            "af",
            "oc",
            "ka",
            "be",
            "tg",
            "sd",
            "gu",
            "am",
            "yi",
            "lo",
            "uz",
            "fo",
            "ht",
            "ps",
            "tk",
            "nn",
            "mt",
            "sa",
            "lb",
            "my",
            "bo",
            "tl",
            "mg",
            "as",
            "tt",
            "haw",
            "ln",
            "ha",
            "ba",
            "jw",
            "su",
            "arxiv:2212.04356",
            "license:apache-2.0",
            "endpoints_compatible",
            "deploy:azure",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 5201239,
          "likes": 5194
        },
        {
          "id": "openai/whisper-large-v3-turbo",
          "type": "hf.automatic_speech_recognition",
          "name": "openai/whisper-large-v3-turbo",
          "repo_id": "openai/whisper-large-v3-turbo",
          "allow_patterns": [
            "model.safetensors",
            "*.json",
            "*.txt"
          ],
          "size_on_disk": 1622443339,
          "pipeline_tag": "automatic-speech-recognition",
          "tags": [
            "transformers",
            "safetensors",
            "whisper",
            "automatic-speech-recognition",
            "audio",
            "en",
            "zh",
            "de",
            "es",
            "ru",
            "ko",
            "fr",
            "ja",
            "pt",
            "tr",
            "pl",
            "ca",
            "nl",
            "ar",
            "sv",
            "it",
            "id",
            "hi",
            "fi",
            "vi",
            "he",
            "uk",
            "el",
            "ms",
            "cs",
            "ro",
            "da",
            "hu",
            "ta",
            "no",
            "th",
            "ur",
            "hr",
            "bg",
            "lt",
            "la",
            "mi",
            "ml",
            "cy",
            "sk",
            "te",
            "fa",
            "lv",
            "bn",
            "sr",
            "az",
            "sl",
            "kn",
            "et",
            "mk",
            "br",
            "eu",
            "is",
            "hy",
            "ne",
            "mn",
            "bs",
            "kk",
            "sq",
            "sw",
            "gl",
            "mr",
            "pa",
            "si",
            "km",
            "sn",
            "yo",
            "so",
            "af",
            "oc",
            "ka",
            "be",
            "tg",
            "sd",
            "gu",
            "am",
            "yi",
            "lo",
            "uz",
            "fo",
            "ht",
            "ps",
            "tk",
            "nn",
            "mt",
            "sa",
            "lb",
            "my",
            "bo",
            "tl",
            "mg",
            "as",
            "tt",
            "haw",
            "ln",
            "ha",
            "ba",
            "jw",
            "su",
            "arxiv:2212.04356",
            "base_model:openai/whisper-large-v3",
            "base_model:finetune:openai/whisper-large-v3",
            "license:mit",
            "endpoints_compatible",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 4535225,
          "likes": 2722
        },
        {
          "id": "openai/whisper-large-v2",
          "type": "hf.automatic_speech_recognition",
          "name": "openai/whisper-large-v2",
          "repo_id": "openai/whisper-large-v2",
          "allow_patterns": [
            "model.safetensors",
            "*.json",
            "*.txt"
          ],
          "size_on_disk": 6177743461,
          "pipeline_tag": "automatic-speech-recognition",
          "tags": [
            "transformers",
            "pytorch",
            "tf",
            "jax",
            "safetensors",
            "whisper",
            "automatic-speech-recognition",
            "audio",
            "hf-asr-leaderboard",
            "en",
            "zh",
            "de",
            "es",
            "ru",
            "ko",
            "fr",
            "ja",
            "pt",
            "tr",
            "pl",
            "ca",
            "nl",
            "ar",
            "sv",
            "it",
            "id",
            "hi",
            "fi",
            "vi",
            "he",
            "uk",
            "el",
            "ms",
            "cs",
            "ro",
            "da",
            "hu",
            "ta",
            "no",
            "th",
            "ur",
            "hr",
            "bg",
            "lt",
            "la",
            "mi",
            "ml",
            "cy",
            "sk",
            "te",
            "fa",
            "lv",
            "bn",
            "sr",
            "az",
            "sl",
            "kn",
            "et",
            "mk",
            "br",
            "eu",
            "is",
            "hy",
            "ne",
            "mn",
            "bs",
            "kk",
            "sq",
            "sw",
            "gl",
            "mr",
            "pa",
            "si",
            "km",
            "sn",
            "yo",
            "so",
            "af",
            "oc",
            "ka",
            "be",
            "tg",
            "sd",
            "gu",
            "am",
            "yi",
            "lo",
            "uz",
            "fo",
            "ht",
            "ps",
            "tk",
            "nn",
            "mt",
            "sa",
            "lb",
            "my",
            "bo",
            "tl",
            "mg",
            "as",
            "tt",
            "haw",
            "ln",
            "ha",
            "ba",
            "jw",
            "su",
            "arxiv:2212.04356",
            "license:apache-2.0",
            "endpoints_compatible",
            "deploy:azure",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 126200,
          "likes": 1777
        },
        {
          "id": "openai/whisper-medium",
          "type": "hf.automatic_speech_recognition",
          "name": "openai/whisper-medium",
          "repo_id": "openai/whisper-medium",
          "allow_patterns": [
            "model.safetensors",
            "*.json",
            "*.txt"
          ],
          "size_on_disk": 3059917072,
          "pipeline_tag": "automatic-speech-recognition",
          "tags": [
            "transformers",
            "pytorch",
            "tf",
            "jax",
            "safetensors",
            "whisper",
            "automatic-speech-recognition",
            "audio",
            "hf-asr-leaderboard",
            "en",
            "zh",
            "de",
            "es",
            "ru",
            "ko",
            "fr",
            "ja",
            "pt",
            "tr",
            "pl",
            "ca",
            "nl",
            "ar",
            "sv",
            "it",
            "id",
            "hi",
            "fi",
            "vi",
            "he",
            "uk",
            "el",
            "ms",
            "cs",
            "ro",
            "da",
            "hu",
            "ta",
            "no",
            "th",
            "ur",
            "hr",
            "bg",
            "lt",
            "la",
            "mi",
            "ml",
            "cy",
            "sk",
            "te",
            "fa",
            "lv",
            "bn",
            "sr",
            "az",
            "sl",
            "kn",
            "et",
            "mk",
            "br",
            "eu",
            "is",
            "hy",
            "ne",
            "mn",
            "bs",
            "kk",
            "sq",
            "sw",
            "gl",
            "mr",
            "pa",
            "si",
            "km",
            "sn",
            "yo",
            "so",
            "af",
            "oc",
            "ka",
            "be",
            "tg",
            "sd",
            "gu",
            "am",
            "yi",
            "lo",
            "uz",
            "fo",
            "ht",
            "ps",
            "tk",
            "nn",
            "mt",
            "sa",
            "lb",
            "my",
            "bo",
            "tl",
            "mg",
            "as",
            "tt",
            "haw",
            "ln",
            "ha",
            "ba",
            "jw",
            "su",
            "arxiv:2212.04356",
            "license:apache-2.0",
            "model-index",
            "endpoints_compatible",
            "deploy:azure",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 920323,
          "likes": 270
        },
        {
          "id": "openai/whisper-small",
          "type": "hf.automatic_speech_recognition",
          "name": "openai/whisper-small",
          "repo_id": "openai/whisper-small",
          "allow_patterns": [
            "model.safetensors",
            "*.json",
            "*.txt"
          ],
          "size_on_disk": 971367937,
          "pipeline_tag": "automatic-speech-recognition",
          "tags": [
            "transformers",
            "pytorch",
            "tf",
            "jax",
            "safetensors",
            "whisper",
            "automatic-speech-recognition",
            "audio",
            "hf-asr-leaderboard",
            "en",
            "zh",
            "de",
            "es",
            "ru",
            "ko",
            "fr",
            "ja",
            "pt",
            "tr",
            "pl",
            "ca",
            "nl",
            "ar",
            "sv",
            "it",
            "id",
            "hi",
            "fi",
            "vi",
            "he",
            "uk",
            "el",
            "ms",
            "cs",
            "ro",
            "da",
            "hu",
            "ta",
            "no",
            "th",
            "ur",
            "hr",
            "bg",
            "lt",
            "la",
            "mi",
            "ml",
            "cy",
            "sk",
            "te",
            "fa",
            "lv",
            "bn",
            "sr",
            "az",
            "sl",
            "kn",
            "et",
            "mk",
            "br",
            "eu",
            "is",
            "hy",
            "ne",
            "mn",
            "bs",
            "kk",
            "sq",
            "sw",
            "gl",
            "mr",
            "pa",
            "si",
            "km",
            "sn",
            "yo",
            "so",
            "af",
            "oc",
            "ka",
            "be",
            "tg",
            "sd",
            "gu",
            "am",
            "yi",
            "lo",
            "uz",
            "fo",
            "ht",
            "ps",
            "tk",
            "nn",
            "mt",
            "sa",
            "lb",
            "my",
            "bo",
            "tl",
            "mg",
            "as",
            "tt",
            "haw",
            "ln",
            "ha",
            "ba",
            "jw",
            "su",
            "arxiv:2212.04356",
            "license:apache-2.0",
            "model-index",
            "endpoints_compatible",
            "deploy:azure",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 4350495,
          "likes": 497
        },
        {
          "id": "Systran/faster-whisper-large-v3",
          "type": "hf.automatic_speech_recognition",
          "name": "Systran/faster-whisper-large-v3",
          "repo_id": "Systran/faster-whisper-large-v3",
          "allow_patterns": [
            "model.bin",
            "*.json",
            "*.txt"
          ],
          "size_on_disk": 3090835702,
          "pipeline_tag": "automatic-speech-recognition",
          "tags": [
            "ctranslate2",
            "audio",
            "automatic-speech-recognition",
            "en",
            "zh",
            "de",
            "es",
            "ru",
            "ko",
            "fr",
            "ja",
            "pt",
            "tr",
            "pl",
            "ca",
            "nl",
            "ar",
            "sv",
            "it",
            "id",
            "hi",
            "fi",
            "vi",
            "he",
            "uk",
            "el",
            "ms",
            "cs",
            "ro",
            "da",
            "hu",
            "ta",
            "no",
            "th",
            "ur",
            "hr",
            "bg",
            "lt",
            "la",
            "mi",
            "ml",
            "cy",
            "sk",
            "te",
            "fa",
            "lv",
            "bn",
            "sr",
            "az",
            "sl",
            "kn",
            "et",
            "mk",
            "br",
            "eu",
            "is",
            "hy",
            "ne",
            "mn",
            "bs",
            "kk",
            "sq",
            "sw",
            "gl",
            "mr",
            "pa",
            "si",
            "km",
            "sn",
            "yo",
            "so",
            "af",
            "oc",
            "ka",
            "be",
            "tg",
            "sd",
            "gu",
            "am",
            "yi",
            "lo",
            "uz",
            "fo",
            "ht",
            "ps",
            "tk",
            "nn",
            "mt",
            "sa",
            "lb",
            "my",
            "bo",
            "tl",
            "mg",
            "as",
            "tt",
            "haw",
            "ln",
            "ha",
            "ba",
            "jw",
            "su",
            "yue",
            "license:mit",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 465280,
          "likes": 482
        }
      ],
      "basic_fields": [
        "model",
        "audio",
        "task"
      ]
    },
    {
      "title": "LoRA Selector",
      "description": "Selects up to 5 LoRA models to apply to a Stable Diffusion model.\n    lora, model customization, fine-tuning, SD\n\n    Use cases:\n    - Combining multiple LoRA models for unique image styles\n    - Fine-tuning Stable Diffusion models with specific attributes\n    - Experimenting with different LoRA combinations",
      "namespace": "huggingface.lora",
      "node_type": "huggingface.lora.LoRASelector",
      "properties": [
        {
          "name": "lora1",
          "type": {
            "type": "hf.lora_sd"
          },
          "default": {
            "type": "hf.lora_sd",
            "repo_id": "",
            "path": null,
            "variant": null,
            "allow_patterns": null,
            "ignore_patterns": null
          },
          "title": "Lora1",
          "description": "First LoRA model"
        },
        {
          "name": "strength1",
          "type": {
            "type": "float"
          },
          "default": 1.0,
          "title": "Strength1",
          "description": "Strength for first LoRA",
          "min": 0.0,
          "max": 2.0
        },
        {
          "name": "lora2",
          "type": {
            "type": "hf.lora_sd"
          },
          "default": {
            "type": "hf.lora_sd",
            "repo_id": "",
            "path": null,
            "variant": null,
            "allow_patterns": null,
            "ignore_patterns": null
          },
          "title": "Lora2",
          "description": "Second LoRA model"
        },
        {
          "name": "strength2",
          "type": {
            "type": "float"
          },
          "default": 1.0,
          "title": "Strength2",
          "description": "Strength for second LoRA",
          "min": 0.0,
          "max": 2.0
        },
        {
          "name": "lora3",
          "type": {
            "type": "hf.lora_sd"
          },
          "default": {
            "type": "hf.lora_sd",
            "repo_id": "",
            "path": null,
            "variant": null,
            "allow_patterns": null,
            "ignore_patterns": null
          },
          "title": "Lora3",
          "description": "Third LoRA model"
        },
        {
          "name": "strength3",
          "type": {
            "type": "float"
          },
          "default": 1.0,
          "title": "Strength3",
          "description": "Strength for third LoRA",
          "min": 0.0,
          "max": 2.0
        },
        {
          "name": "lora4",
          "type": {
            "type": "hf.lora_sd"
          },
          "default": {
            "type": "hf.lora_sd",
            "repo_id": "",
            "path": null,
            "variant": null,
            "allow_patterns": null,
            "ignore_patterns": null
          },
          "title": "Lora4",
          "description": "Fourth LoRA model"
        },
        {
          "name": "strength4",
          "type": {
            "type": "float"
          },
          "default": 1.0,
          "title": "Strength4",
          "description": "Strength for fourth LoRA",
          "min": 0.0,
          "max": 2.0
        },
        {
          "name": "lora5",
          "type": {
            "type": "hf.lora_sd"
          },
          "default": {
            "type": "hf.lora_sd",
            "repo_id": "",
            "path": null,
            "variant": null,
            "allow_patterns": null,
            "ignore_patterns": null
          },
          "title": "Lora5",
          "description": "Fifth LoRA model"
        },
        {
          "name": "strength5",
          "type": {
            "type": "float"
          },
          "default": 1.0,
          "title": "Strength5",
          "description": "Strength for fifth LoRA",
          "min": 0.0,
          "max": 2.0
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "hf.lora_sd_config"
              }
            ]
          },
          "name": "output"
        }
      ],
      "recommended_models": [
        {
          "id": "danbrown/loras:2d_sprite.safetensors",
          "type": "hf.lora_sd",
          "name": "danbrown/loras",
          "repo_id": "danbrown/loras",
          "path": "2d_sprite.safetensors",
          "size_on_disk": 37863537,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 10
        },
        {
          "id": "danbrown/loras:ghibli_scenery.safetensors",
          "type": "hf.lora_sd",
          "name": "danbrown/loras",
          "repo_id": "danbrown/loras",
          "path": "ghibli_scenery.safetensors",
          "size_on_disk": 151111762,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 10
        },
        {
          "id": "danbrown/loras:add_detail.safetensors",
          "type": "hf.lora_sd",
          "name": "danbrown/loras",
          "repo_id": "danbrown/loras",
          "path": "add_detail.safetensors",
          "size_on_disk": 37861176,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 10
        },
        {
          "id": "danbrown/loras:colorwater.safetensors",
          "type": "hf.lora_sd",
          "name": "danbrown/loras",
          "repo_id": "danbrown/loras",
          "path": "colorwater.safetensors",
          "size_on_disk": 151111114,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 10
        },
        {
          "id": "danbrown/loras:sxz_game_assets.safetensors",
          "type": "hf.lora_sd",
          "name": "danbrown/loras",
          "repo_id": "danbrown/loras",
          "path": "sxz_game_assets.safetensors",
          "size_on_disk": 151114099,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 10
        },
        {
          "id": "danbrown/loras:3Danaglyph.safetensors",
          "type": "hf.lora_sd",
          "name": "danbrown/loras",
          "repo_id": "danbrown/loras",
          "path": "3Danaglyph.safetensors",
          "size_on_disk": 151108831,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 10
        },
        {
          "id": "danbrown/loras:akiratoriyama_style.safetensors",
          "type": "hf.lora_sd",
          "name": "danbrown/loras",
          "repo_id": "danbrown/loras",
          "path": "akiratoriyama_style.safetensors",
          "size_on_disk": 302069805,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 10
        },
        {
          "id": "danbrown/loras:animeoutlineV4.safetensors",
          "type": "hf.lora_sd",
          "name": "danbrown/loras",
          "repo_id": "danbrown/loras",
          "path": "animeoutlineV4.safetensors",
          "size_on_disk": 18986312,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 10
        },
        {
          "id": "danbrown/loras:aqua_konosuba.safetensors",
          "type": "hf.lora_sd",
          "name": "danbrown/loras",
          "repo_id": "danbrown/loras",
          "path": "aqua_konosuba.safetensors",
          "size_on_disk": 151074560,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 10
        },
        {
          "id": "danbrown/loras:arakihirohiko_style.safetensors",
          "type": "hf.lora_sd",
          "name": "danbrown/loras",
          "repo_id": "danbrown/loras",
          "path": "arakihirohiko_style.safetensors",
          "size_on_disk": 78025834,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 10
        },
        {
          "id": "danbrown/loras:arcane_style.safetensors",
          "type": "hf.lora_sd",
          "name": "danbrown/loras",
          "repo_id": "danbrown/loras",
          "path": "arcane_style.safetensors",
          "size_on_disk": 302104229,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 10
        },
        {
          "id": "danbrown/loras:canetaazul.safetensors",
          "type": "hf.lora_sd",
          "name": "danbrown/loras",
          "repo_id": "danbrown/loras",
          "path": "canetaazul.safetensors",
          "size_on_disk": 151109011,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 10
        },
        {
          "id": "danbrown/loras:cyberpunk_tarot.safetensors",
          "type": "hf.lora_sd",
          "name": "danbrown/loras",
          "repo_id": "danbrown/loras",
          "path": "cyberpunk_tarot.safetensors",
          "size_on_disk": 151125126,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 10
        },
        {
          "id": "danbrown/loras:discoelysium_style.safetensors",
          "type": "hf.lora_sd",
          "name": "danbrown/loras",
          "repo_id": "danbrown/loras",
          "path": "discoelysium_style.safetensors",
          "size_on_disk": 151112313,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 10
        },
        {
          "id": "danbrown/loras:esdeath_akamegakill.safetensors",
          "type": "hf.lora_sd",
          "name": "danbrown/loras",
          "repo_id": "danbrown/loras",
          "path": "esdeath_akamegakill.safetensors",
          "size_on_disk": 236046182,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 10
        },
        {
          "id": "danbrown/loras:fire_vfx.safetensors",
          "type": "hf.lora_sd",
          "name": "danbrown/loras",
          "repo_id": "danbrown/loras",
          "path": "fire_vfx.safetensors",
          "size_on_disk": 172342464,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 10
        },
        {
          "id": "danbrown/loras:flamingeye.safetensors",
          "type": "hf.lora_sd",
          "name": "danbrown/loras",
          "repo_id": "danbrown/loras",
          "path": "flamingeye.safetensors",
          "size_on_disk": 151121110,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 10
        },
        {
          "id": "danbrown/loras:funnycreatures.safetensors",
          "type": "hf.lora_sd",
          "name": "danbrown/loras",
          "repo_id": "danbrown/loras",
          "path": "funnycreatures.safetensors",
          "size_on_disk": 151108831,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 10
        },
        {
          "id": "danbrown/loras:gacha_splash.safetensors",
          "type": "hf.lora_sd",
          "name": "danbrown/loras",
          "repo_id": "danbrown/loras",
          "path": "gacha_splash.safetensors",
          "size_on_disk": 151126110,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 10
        },
        {
          "id": "danbrown/loras:gigachad.safetensors",
          "type": "hf.lora_sd",
          "name": "danbrown/loras",
          "repo_id": "danbrown/loras",
          "path": "gigachad.safetensors",
          "size_on_disk": 151111885,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 10
        },
        {
          "id": "danbrown/loras:gyokai_style.safetensors",
          "type": "hf.lora_sd",
          "name": "danbrown/loras",
          "repo_id": "danbrown/loras",
          "path": "gyokai_style.safetensors",
          "size_on_disk": 151125722,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 10
        },
        {
          "id": "danbrown/loras:harold.safetensors",
          "type": "hf.lora_sd",
          "name": "danbrown/loras",
          "repo_id": "danbrown/loras",
          "path": "harold.safetensors",
          "size_on_disk": 75613063,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 10
        },
        {
          "id": "danbrown/loras:hiderohoribes_style.safetensors",
          "type": "hf.lora_sd",
          "name": "danbrown/loras",
          "repo_id": "danbrown/loras",
          "path": "hiderohoribes_style.safetensors",
          "size_on_disk": 26031740,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 10
        },
        {
          "id": "danbrown/loras:ilyakuvshinov_style.safetensors",
          "type": "hf.lora_sd",
          "name": "danbrown/loras",
          "repo_id": "danbrown/loras",
          "path": "ilyakuvshinov_style.safetensors",
          "size_on_disk": 37881890,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 10
        },
        {
          "id": "danbrown/loras:jacksparrow.safetensors",
          "type": "hf.lora_sd",
          "name": "danbrown/loras",
          "repo_id": "danbrown/loras",
          "path": "jacksparrow.safetensors",
          "size_on_disk": 151108831,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 10
        },
        {
          "id": "danbrown/loras:jimlee_style.safetensors",
          "type": "hf.lora_sd",
          "name": "danbrown/loras",
          "repo_id": "danbrown/loras",
          "path": "jimlee_style.safetensors",
          "size_on_disk": 151108831,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 10
        },
        {
          "id": "danbrown/loras:komowataharuka_chibiart.safetensors",
          "type": "hf.lora_sd",
          "name": "danbrown/loras",
          "repo_id": "danbrown/loras",
          "path": "komowataharuka_chibiart.safetensors",
          "size_on_disk": 151130160,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 10
        },
        {
          "id": "danbrown/loras:lightning_vfx.safetensors",
          "type": "hf.lora_sd",
          "name": "danbrown/loras",
          "repo_id": "danbrown/loras",
          "path": "lightning_vfx.safetensors",
          "size_on_disk": 80330354,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 10
        },
        {
          "id": "danbrown/loras:lucy_cyberpunk.safetensors",
          "type": "hf.lora_sd",
          "name": "danbrown/loras",
          "repo_id": "danbrown/loras",
          "path": "lucy_cyberpunk.safetensors",
          "size_on_disk": 151074558,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 10
        },
        {
          "id": "danbrown/loras:luisap_pixelart.safetensors",
          "type": "hf.lora_sd",
          "name": "danbrown/loras",
          "repo_id": "danbrown/loras",
          "path": "luisap_pixelart.safetensors",
          "size_on_disk": 932809,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 10
        },
        {
          "id": "danbrown/loras:mumei_kabaneri.safetensors",
          "type": "hf.lora_sd",
          "name": "danbrown/loras",
          "repo_id": "danbrown/loras",
          "path": "mumei_kabaneri.safetensors",
          "size_on_disk": 151112410,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 10
        },
        {
          "id": "danbrown/loras:myheroacademia_style.safetensors",
          "type": "hf.lora_sd",
          "name": "danbrown/loras",
          "repo_id": "danbrown/loras",
          "path": "myheroacademia_style.safetensors",
          "size_on_disk": 302069805,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 10
        },
        {
          "id": "danbrown/loras:neoartcore.safetensors",
          "type": "hf.lora_sd",
          "name": "danbrown/loras",
          "repo_id": "danbrown/loras",
          "path": "neoartcore.safetensors",
          "size_on_disk": 151108831,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 10
        },
        {
          "id": "danbrown/loras:ochakouraraka.safetensors",
          "type": "hf.lora_sd",
          "name": "danbrown/loras",
          "repo_id": "danbrown/loras",
          "path": "ochakouraraka.safetensors",
          "size_on_disk": 9565173,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 10
        },
        {
          "id": "danbrown/loras:onepiece_style.safetensors",
          "type": "hf.lora_sd",
          "name": "danbrown/loras",
          "repo_id": "danbrown/loras",
          "path": "onepiece_style.safetensors",
          "size_on_disk": 302068911,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 10
        },
        {
          "id": "danbrown/loras:paimon_genshinimpact.safetensors",
          "type": "hf.lora_sd",
          "name": "danbrown/loras",
          "repo_id": "danbrown/loras",
          "path": "paimon_genshinimpact.safetensors",
          "size_on_disk": 151110127,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 10
        },
        {
          "id": "danbrown/loras:peanutscomics_style.safetensors",
          "type": "hf.lora_sd",
          "name": "danbrown/loras",
          "repo_id": "danbrown/loras",
          "path": "peanutscomics_style.safetensors",
          "size_on_disk": 54357317,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 10
        },
        {
          "id": "danbrown/loras:pepefrog.safetensors",
          "type": "hf.lora_sd",
          "name": "danbrown/loras",
          "repo_id": "danbrown/loras",
          "path": "pepefrog.safetensors",
          "size_on_disk": 151260201,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 10
        },
        {
          "id": "danbrown/loras:persona5_portraits.safetensors",
          "type": "hf.lora_sd",
          "name": "danbrown/loras",
          "repo_id": "danbrown/loras",
          "path": "persona5_portraits.safetensors",
          "size_on_disk": 19624598,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 10
        },
        {
          "id": "danbrown/loras:persona5_style.safetensors",
          "type": "hf.lora_sd",
          "name": "danbrown/loras",
          "repo_id": "danbrown/loras",
          "path": "persona5_style.safetensors",
          "size_on_disk": 151116853,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 10
        },
        {
          "id": "danbrown/loras:pixhell.safetensors",
          "type": "hf.lora_sd",
          "name": "danbrown/loras",
          "repo_id": "danbrown/loras",
          "path": "pixhell.safetensors",
          "size_on_disk": 151109839,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 10
        },
        {
          "id": "danbrown/loras:princesszelda.safetensors",
          "type": "hf.lora_sd",
          "name": "danbrown/loras",
          "repo_id": "danbrown/loras",
          "path": "princesszelda.safetensors",
          "size_on_disk": 302069806,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 10
        },
        {
          "id": "danbrown/loras:satoshiuruchihara_style.safetensors",
          "type": "hf.lora_sd",
          "name": "danbrown/loras",
          "repo_id": "danbrown/loras",
          "path": "satoshiuruchihara_style.safetensors",
          "size_on_disk": 453157754,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 10
        },
        {
          "id": "danbrown/loras:shinobu_demonslayer.safetensors",
          "type": "hf.lora_sd",
          "name": "danbrown/loras",
          "repo_id": "danbrown/loras",
          "path": "shinobu_demonslayer.safetensors",
          "size_on_disk": 151111222,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 10
        },
        {
          "id": "danbrown/loras:sokolov_style.safetensors",
          "type": "hf.lora_sd",
          "name": "danbrown/loras",
          "repo_id": "danbrown/loras",
          "path": "sokolov_style.safetensors",
          "size_on_disk": 151108831,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 10
        },
        {
          "id": "danbrown/loras:standingbackgroundv1.safetensors",
          "type": "hf.lora_sd",
          "name": "danbrown/loras",
          "repo_id": "danbrown/loras",
          "path": "standingbackgroundv1.safetensors",
          "size_on_disk": 37861172,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 10
        },
        {
          "id": "danbrown/loras:sun_shadow_style.safetensors",
          "type": "hf.lora_sd",
          "name": "danbrown/loras",
          "repo_id": "danbrown/loras",
          "path": "sun_shadow_style.safetensors",
          "size_on_disk": 151115842,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 10
        },
        {
          "id": "danbrown/loras:thickeranimelines.safetensors",
          "type": "hf.lora_sd",
          "name": "danbrown/loras",
          "repo_id": "danbrown/loras",
          "path": "thickeranimelines.safetensors",
          "size_on_disk": 151108831,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 10
        },
        {
          "id": "danbrown/loras:threesidedview.safetensors",
          "type": "hf.lora_sd",
          "name": "danbrown/loras",
          "repo_id": "danbrown/loras",
          "path": "threesidedview.safetensors",
          "size_on_disk": 151110147,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 10
        },
        {
          "id": "danbrown/loras:twitch_emotes.safetensors",
          "type": "hf.lora_sd",
          "name": "danbrown/loras",
          "repo_id": "danbrown/loras",
          "path": "twitch_emotes.safetensors",
          "size_on_disk": 9549785,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 10
        },
        {
          "id": "danbrown/loras:water_vfx.safetensors",
          "type": "hf.lora_sd",
          "name": "danbrown/loras",
          "repo_id": "danbrown/loras",
          "path": "water_vfx.safetensors",
          "size_on_disk": 4833133,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 10
        },
        {
          "id": "danbrown/loras:wlop_style.safetensors",
          "type": "hf.lora_sd",
          "name": "danbrown/loras",
          "repo_id": "danbrown/loras",
          "path": "wlop_style.safetensors",
          "size_on_disk": 19004975,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 10
        },
        {
          "id": "danbrown/loras:zerotwo_darling.safetensors",
          "type": "hf.lora_sd",
          "name": "danbrown/loras",
          "repo_id": "danbrown/loras",
          "path": "zerotwo_darling.safetensors",
          "size_on_disk": 151108831,
          "tags": [
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 0,
          "likes": 10
        }
      ],
      "basic_fields": [
        "lora1",
        "strength1",
        "lora2",
        "strength2",
        "lora3",
        "strength3",
        "lora4",
        "strength4",
        "lora5",
        "strength5"
      ]
    },
    {
      "title": "LoRA XL Selector",
      "description": "Selects up to 5 LoRA models to apply to a Stable Diffusion XL model.\n    lora, model customization, fine-tuning, SDXL\n\n    Use cases:\n    - Combining multiple LoRA models for unique image styles\n    - Fine-tuning Stable Diffusion XL models with specific attributes\n    - Experimenting with different LoRA combinations",
      "namespace": "huggingface.lora",
      "node_type": "huggingface.lora.LoRASelectorXL",
      "properties": [
        {
          "name": "lora1",
          "type": {
            "type": "hf.lora_sdxl"
          },
          "default": {
            "type": "hf.lora_sdxl",
            "repo_id": "",
            "path": null,
            "variant": null,
            "allow_patterns": null,
            "ignore_patterns": null
          },
          "title": "Lora1",
          "description": "First LoRA model"
        },
        {
          "name": "strength1",
          "type": {
            "type": "float"
          },
          "default": 1.0,
          "title": "Strength1",
          "description": "Strength for first LoRA",
          "min": 0.0,
          "max": 2.0
        },
        {
          "name": "lora2",
          "type": {
            "type": "hf.lora_sdxl"
          },
          "default": {
            "type": "hf.lora_sdxl",
            "repo_id": "",
            "path": null,
            "variant": null,
            "allow_patterns": null,
            "ignore_patterns": null
          },
          "title": "Lora2",
          "description": "Second LoRA model"
        },
        {
          "name": "strength2",
          "type": {
            "type": "float"
          },
          "default": 1.0,
          "title": "Strength2",
          "description": "Strength for second LoRA",
          "min": 0.0,
          "max": 2.0
        },
        {
          "name": "lora3",
          "type": {
            "type": "hf.lora_sdxl"
          },
          "default": {
            "type": "hf.lora_sdxl",
            "repo_id": "",
            "path": null,
            "variant": null,
            "allow_patterns": null,
            "ignore_patterns": null
          },
          "title": "Lora3",
          "description": "Third LoRA model"
        },
        {
          "name": "strength3",
          "type": {
            "type": "float"
          },
          "default": 1.0,
          "title": "Strength3",
          "description": "Strength for third LoRA",
          "min": 0.0,
          "max": 2.0
        },
        {
          "name": "lora4",
          "type": {
            "type": "hf.lora_sdxl"
          },
          "default": {
            "type": "hf.lora_sdxl",
            "repo_id": "",
            "path": null,
            "variant": null,
            "allow_patterns": null,
            "ignore_patterns": null
          },
          "title": "Lora4",
          "description": "Fourth LoRA model"
        },
        {
          "name": "strength4",
          "type": {
            "type": "float"
          },
          "default": 1.0,
          "title": "Strength4",
          "description": "Strength for fourth LoRA",
          "min": 0.0,
          "max": 2.0
        },
        {
          "name": "lora5",
          "type": {
            "type": "hf.lora_sdxl"
          },
          "default": {
            "type": "hf.lora_sdxl",
            "repo_id": "",
            "path": null,
            "variant": null,
            "allow_patterns": null,
            "ignore_patterns": null
          },
          "title": "Lora5",
          "description": "Fifth LoRA model"
        },
        {
          "name": "strength5",
          "type": {
            "type": "float"
          },
          "default": 1.0,
          "title": "Strength5",
          "description": "Strength for fifth LoRA",
          "min": 0.0,
          "max": 2.0
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "hf.lora_sdxl_config"
              }
            ]
          },
          "name": "output"
        }
      ],
      "recommended_models": [
        {
          "id": "CiroN2022/toy-face:toy_face_sdxl.safetensors",
          "type": "hf.lora_sdxl",
          "name": "CiroN2022/toy-face",
          "repo_id": "CiroN2022/toy-face",
          "path": "toy_face_sdxl.safetensors",
          "size_on_disk": 170543292,
          "pipeline_tag": "text-to-image",
          "tags": [
            "diffusers",
            "text-to-image",
            "stable-diffusion",
            "lora",
            "base_model:stabilityai/stable-diffusion-xl-base-1.0",
            "base_model:adapter:stabilityai/stable-diffusion-xl-base-1.0",
            "license:other",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 64,
          "likes": 15
        },
        {
          "id": "nerijs/pixel-art-xl:pixel-art-xl.safetensors",
          "type": "hf.lora_sdxl",
          "name": "nerijs/pixel-art-xl",
          "repo_id": "nerijs/pixel-art-xl",
          "path": "pixel-art-xl.safetensors",
          "size_on_disk": 170543052,
          "pipeline_tag": "text-to-image",
          "tags": [
            "diffusers",
            "text-to-image",
            "stable-diffusion",
            "lora",
            "base_model:stabilityai/stable-diffusion-xl-base-1.0",
            "base_model:adapter:stabilityai/stable-diffusion-xl-base-1.0",
            "license:creativeml-openrail-m",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 4176,
          "likes": 569
        },
        {
          "id": "goofyai/3d_render_style_xl:3d_render_style_xl.safetensors",
          "type": "hf.lora_sdxl",
          "name": "goofyai/3d_render_style_xl",
          "repo_id": "goofyai/3d_render_style_xl",
          "path": "3d_render_style_xl.safetensors",
          "size_on_disk": 85450700,
          "pipeline_tag": "text-to-image",
          "tags": [
            "diffusers",
            "text-to-image",
            "stable-diffusion",
            "lora",
            "base_model:stabilityai/stable-diffusion-xl-base-1.0",
            "base_model:adapter:stabilityai/stable-diffusion-xl-base-1.0",
            "license:apache-2.0",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 305,
          "likes": 132
        },
        {
          "id": "artificialguybr/CuteCartoonRedmond-V2:CuteCartoonRedmond-CuteCartoon-CuteCartoonAF.safetensors",
          "type": "hf.lora_sdxl",
          "name": "artificialguybr/CuteCartoonRedmond-V2",
          "repo_id": "artificialguybr/CuteCartoonRedmond-V2",
          "path": "CuteCartoonRedmond-CuteCartoon-CuteCartoonAF.safetensors",
          "size_on_disk": 170540036,
          "pipeline_tag": "text-to-image",
          "tags": [
            "diffusers",
            "text-to-image",
            "stable-diffusion",
            "lora",
            "base_model:stabilityai/stable-diffusion-xl-base-1.0",
            "base_model:adapter:stabilityai/stable-diffusion-xl-base-1.0",
            "license:creativeml-openrail-m",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 24,
          "likes": 9
        },
        {
          "id": "blink7630/graphic-novel-illustration:Graphic_Novel_Illustration-000007.safetensors",
          "type": "hf.lora_sdxl",
          "name": "blink7630/graphic-novel-illustration",
          "repo_id": "blink7630/graphic-novel-illustration",
          "path": "Graphic_Novel_Illustration-000007.safetensors",
          "size_on_disk": 456580292,
          "pipeline_tag": "text-to-image",
          "tags": [
            "diffusers",
            "text-to-image",
            "stable-diffusion",
            "lora",
            "template:sd-lora",
            "comic book",
            "style",
            "graphic novel",
            "illustration",
            "base_model:stabilityai/stable-diffusion-xl-base-1.0",
            "base_model:adapter:stabilityai/stable-diffusion-xl-base-1.0",
            "license:other",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 50,
          "likes": 39
        },
        {
          "id": "robert123231/coloringbookgenerator:ColoringBookRedmond-ColoringBook-ColoringBookAF.safetensors",
          "type": "hf.lora_sdxl",
          "name": "robert123231/coloringbookgenerator",
          "repo_id": "robert123231/coloringbookgenerator",
          "path": "ColoringBookRedmond-ColoringBook-ColoringBookAF.safetensors",
          "size_on_disk": 170540036,
          "pipeline_tag": "text-to-image",
          "tags": [
            "diffusers",
            "text-to-image",
            "stable-diffusion",
            "lora",
            "template:sd-lora",
            "base_model:stabilityai/stable-diffusion-xl-base-1.0",
            "base_model:adapter:stabilityai/stable-diffusion-xl-base-1.0",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 21,
          "likes": 8
        }
      ],
      "basic_fields": [
        "lora1",
        "strength1",
        "lora2",
        "strength2",
        "lora3",
        "strength3",
        "lora4",
        "strength4",
        "lora5",
        "strength5"
      ]
    },
    {
      "title": "Split into Sentences",
      "description": "Splits text into sentences using LangChain's SentenceTransformersTokenTextSplitter.\n    sentences, split, nlp\n\n    Use cases:\n    - Natural sentence-based text splitting\n    - Creating semantically meaningful chunks\n    - Processing text for sentence-level analysis",
      "namespace": "huggingface.sentence_transformers",
      "node_type": "huggingface.sentence_transformers.SplitSentences",
      "properties": [
        {
          "name": "document",
          "type": {
            "type": "document"
          },
          "default": {
            "type": "document",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Document"
        },
        {
          "name": "chunk_size",
          "type": {
            "type": "int"
          },
          "default": 40,
          "title": "Chunk Size",
          "description": "Maximum number of tokens per chunk"
        },
        {
          "name": "chunk_overlap",
          "type": {
            "type": "int"
          },
          "default": 5,
          "title": "Chunk Overlap",
          "description": "Number of tokens to overlap between chunks"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "str"
          },
          "name": "text"
        },
        {
          "type": {
            "type": "str"
          },
          "name": "source_id"
        },
        {
          "type": {
            "type": "int"
          },
          "name": "start_index"
        }
      ],
      "basic_fields": [
        "document",
        "chunk_size",
        "chunk_overlap"
      ],
      "is_streaming_output": true
    },
    {
      "title": "Object Detection",
      "description": "Detects and localizes objects in images with bounding boxes and confidence scores.\n    image, object-detection, bounding-boxes, huggingface, computer-vision\n\n    Use cases:\n    - Count and identify objects in photographs and videos\n    - Locate specific items in complex scenes for robotics\n    - Analyze security camera footage for monitoring systems\n    - Detect tables and structures in documents\n    - Build automated inventory and inspection systems",
      "namespace": "huggingface.object_detection",
      "node_type": "huggingface.object_detection.ObjectDetection",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.object_detection"
          },
          "default": {
            "type": "hf.object_detection",
            "repo_id": "facebook/detr-resnet-50",
            "path": null,
            "variant": null,
            "allow_patterns": null,
            "ignore_patterns": null
          },
          "title": "Model",
          "description": "The object detection model. DETR models offer high accuracy; YOLOS variants are faster. Specialized models exist for tables and fashion items."
        },
        {
          "name": "image",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image",
          "description": "The image to detect objects in. Supports common formats like JPEG, PNG."
        },
        {
          "name": "threshold",
          "type": {
            "type": "float"
          },
          "default": 0.9,
          "title": "Confidence Threshold",
          "description": "Minimum confidence score (0-1) for detected objects. Higher values return fewer but more certain detections."
        },
        {
          "name": "top_k",
          "type": {
            "type": "int"
          },
          "default": 5,
          "title": "Top K",
          "description": "Maximum number of detected objects to return, sorted by confidence."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "object_detection_result"
              }
            ]
          },
          "name": "output"
        }
      ],
      "recommended_models": [
        {
          "id": "facebook/detr-resnet-50",
          "type": "hf.object_detection",
          "name": "facebook/detr-resnet-50",
          "repo_id": "facebook/detr-resnet-50",
          "allow_patterns": [
            "README.md",
            "*.bin",
            "*.json",
            "**/*.json"
          ],
          "size_on_disk": 166742644,
          "pipeline_tag": "object-detection",
          "tags": [
            "transformers",
            "pytorch",
            "safetensors",
            "detr",
            "object-detection",
            "vision",
            "dataset:coco",
            "arxiv:2005.12872",
            "license:apache-2.0",
            "endpoints_compatible",
            "deploy:azure",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 1905078,
          "likes": 914
        },
        {
          "id": "facebook/detr-resnet-101",
          "type": "hf.object_detection",
          "name": "facebook/detr-resnet-101",
          "repo_id": "facebook/detr-resnet-101",
          "allow_patterns": [
            "README.md",
            "*.safetensors",
            "*.json",
            "**/*.json"
          ],
          "size_on_disk": 242808179,
          "pipeline_tag": "object-detection",
          "tags": [
            "transformers",
            "pytorch",
            "safetensors",
            "detr",
            "object-detection",
            "vision",
            "dataset:coco",
            "arxiv:2005.12872",
            "license:apache-2.0",
            "endpoints_compatible",
            "deploy:azure",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 44267,
          "likes": 125
        },
        {
          "id": "hustvl/yolos-tiny",
          "type": "hf.object_detection",
          "name": "hustvl/yolos-tiny",
          "repo_id": "hustvl/yolos-tiny",
          "allow_patterns": [
            "README.md",
            "*.safetensors",
            "*.json",
            "**/*.json"
          ],
          "size_on_disk": 25987930,
          "pipeline_tag": "object-detection",
          "tags": [
            "transformers",
            "pytorch",
            "safetensors",
            "yolos",
            "object-detection",
            "vision",
            "dataset:coco",
            "arxiv:2106.00666",
            "license:apache-2.0",
            "endpoints_compatible",
            "deploy:azure",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 109435,
          "likes": 272
        },
        {
          "id": "hustvl/yolos-small",
          "type": "hf.object_detection",
          "name": "hustvl/yolos-small",
          "repo_id": "hustvl/yolos-small",
          "allow_patterns": [
            "README.md",
            "*.safetensors",
            "*.json",
            "**/*.json"
          ],
          "size_on_disk": 122771871,
          "pipeline_tag": "object-detection",
          "tags": [
            "transformers",
            "pytorch",
            "safetensors",
            "yolos",
            "object-detection",
            "vision",
            "dataset:coco",
            "arxiv:2106.00666",
            "license:apache-2.0",
            "endpoints_compatible",
            "deploy:azure",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 724083,
          "likes": 71
        },
        {
          "id": "microsoft/table-transformer-detection",
          "type": "hf.object_detection",
          "name": "microsoft/table-transformer-detection",
          "repo_id": "microsoft/table-transformer-detection",
          "allow_patterns": [
            "README.md",
            "*.bin",
            "*.json",
            "**/*.json"
          ],
          "size_on_disk": 115395920,
          "pipeline_tag": "object-detection",
          "tags": [
            "transformers",
            "pytorch",
            "safetensors",
            "table-transformer",
            "object-detection",
            "arxiv:2110.00061",
            "license:mit",
            "endpoints_compatible",
            "deploy:azure",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 2127506,
          "likes": 384
        },
        {
          "id": "microsoft/table-transformer-structure-recognition-v1.1-all",
          "type": "hf.object_detection",
          "name": "microsoft/table-transformer-structure-recognition-v1.1-all",
          "repo_id": "microsoft/table-transformer-structure-recognition-v1.1-all",
          "allow_patterns": [
            "README.md",
            "*.bin",
            "*.json",
            "**/*.json"
          ],
          "size_on_disk": 78191,
          "pipeline_tag": "object-detection",
          "tags": [
            "transformers",
            "safetensors",
            "table-transformer",
            "object-detection",
            "arxiv:2303.00716",
            "license:mit",
            "endpoints_compatible",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 319047,
          "likes": 77
        },
        {
          "id": "valentinafeve/yolos-fashionpedia",
          "type": "hf.object_detection",
          "name": "valentinafeve/yolos-fashionpedia",
          "repo_id": "valentinafeve/yolos-fashionpedia",
          "allow_patterns": [
            "README.md",
            "*.bin",
            "*.json",
            "**/*.json"
          ],
          "size_on_disk": 122742861,
          "pipeline_tag": "object-detection",
          "tags": [
            "transformers",
            "pytorch",
            "yolos",
            "object-detection",
            "YOLOS",
            "Object detection",
            "en",
            "dataset:detection-datasets/fashionpedia",
            "endpoints_compatible",
            "deploy:azure",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 5508,
          "likes": 139
        }
      ],
      "basic_fields": [
        "model",
        "image"
      ]
    },
    {
      "title": "Visualize Object Detection",
      "description": "Renders object detection results as labeled bounding boxes overlaid on the original image.\n    image, object-detection, bounding-boxes, visualization, annotation\n\n    Use cases:\n    - Visualize and verify object detection model outputs\n    - Create annotated images for documentation and presentations\n    - Debug and analyze detection accuracy and coverage\n    - Generate labeled images for training data review",
      "namespace": "huggingface.object_detection",
      "node_type": "huggingface.object_detection.VisualizeObjectDetection",
      "properties": [
        {
          "name": "image",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image",
          "description": "The original image to draw detection boxes on."
        },
        {
          "name": "objects",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "object_detection_result"
              }
            ]
          },
          "default": {},
          "title": "Detected Objects",
          "description": "List of detected objects from ObjectDetection or ZeroShotObjectDetection nodes."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "objects"
      ]
    },
    {
      "title": "Zero-Shot Object Detection",
      "description": "Detects objects in images using custom labels without requiring task-specific training.\n    image, object-detection, bounding-boxes, zero-shot, flexible\n\n    Use cases:\n    - Detect custom objects without training a specialized model\n    - Search for specific items described in natural language\n    - Build flexible object detection systems with dynamic categories\n    - Prototype detection applications with arbitrary object classes",
      "namespace": "huggingface.object_detection",
      "node_type": "huggingface.object_detection.ZeroShotObjectDetection",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.zero_shot_object_detection"
          },
          "default": {
            "type": "hf.zero_shot_object_detection",
            "repo_id": "google/owlv2-base-patch16",
            "path": null,
            "variant": null,
            "allow_patterns": null,
            "ignore_patterns": null
          },
          "title": "Model",
          "description": "The zero-shot detection model. OWL-ViT/OWLv2 models use CLIP for flexible label matching; Grounding-DINO offers strong performance."
        },
        {
          "name": "image",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image",
          "description": "The image to detect objects in."
        },
        {
          "name": "threshold",
          "type": {
            "type": "float"
          },
          "default": 0.1,
          "title": "Confidence Threshold",
          "description": "Minimum confidence score (0-1) for detections. Lower values find more objects but may include false positives."
        },
        {
          "name": "top_k",
          "type": {
            "type": "int"
          },
          "default": 5,
          "title": "Top K",
          "description": "Maximum number of detected objects to return per label."
        },
        {
          "name": "candidate_labels",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Candidate Labels",
          "description": "Comma-separated list of object labels to detect (e.g., 'cat,dog,person,car'). Use descriptive phrases for better results."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "object_detection_result"
              }
            ]
          },
          "name": "output"
        }
      ],
      "recommended_models": [
        {
          "id": "google/owlvit-base-patch32",
          "type": "hf.zero_shot_object_detection",
          "name": "google/owlvit-base-patch32",
          "repo_id": "google/owlvit-base-patch32",
          "allow_patterns": [
            "README.md",
            "*.bin",
            "*.json",
            "**/*.json",
            "txt"
          ],
          "size_on_disk": 614120310,
          "pipeline_tag": "zero-shot-object-detection",
          "tags": [
            "transformers",
            "pytorch",
            "safetensors",
            "owlvit",
            "zero-shot-object-detection",
            "vision",
            "arxiv:2205.06230",
            "license:apache-2.0",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 166178,
          "likes": 143
        },
        {
          "id": "google/owlvit-large-patch14",
          "type": "hf.zero_shot_object_detection",
          "name": "google/owlvit-large-patch14",
          "repo_id": "google/owlvit-large-patch14",
          "allow_patterns": [
            "README.md",
            "*.bin",
            "*.json",
            "**/*.json",
            "txt"
          ],
          "size_on_disk": 1736825755,
          "pipeline_tag": "zero-shot-object-detection",
          "tags": [
            "transformers",
            "pytorch",
            "owlvit",
            "zero-shot-object-detection",
            "vision",
            "arxiv:2205.06230",
            "license:apache-2.0",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 15283,
          "likes": 28
        },
        {
          "id": "google/owlvit-base-patch16",
          "type": "hf.zero_shot_object_detection",
          "name": "google/owlvit-base-patch16",
          "repo_id": "google/owlvit-base-patch16",
          "allow_patterns": [
            "README.md",
            "*.bin",
            "*.json",
            "**/*.json",
            "txt"
          ],
          "size_on_disk": 612364662,
          "pipeline_tag": "zero-shot-object-detection",
          "tags": [
            "transformers",
            "pytorch",
            "owlvit",
            "zero-shot-object-detection",
            "vision",
            "arxiv:2205.06230",
            "license:apache-2.0",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 18979,
          "likes": 13
        },
        {
          "id": "google/owlv2-base-patch16",
          "type": "hf.zero_shot_object_detection",
          "name": "google/owlv2-base-patch16",
          "repo_id": "google/owlv2-base-patch16",
          "allow_patterns": [
            "README.md",
            "*.bin",
            "*.json",
            "**/*.json",
            "txt"
          ],
          "size_on_disk": 621074340,
          "pipeline_tag": "zero-shot-object-detection",
          "tags": [
            "transformers",
            "pytorch",
            "safetensors",
            "owlv2",
            "zero-shot-object-detection",
            "vision",
            "arxiv:2306.09683",
            "license:apache-2.0",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 40181,
          "likes": 29
        },
        {
          "id": "google/owlv2-base-patch16-ensemble",
          "type": "hf.zero_shot_object_detection",
          "name": "google/owlv2-base-patch16-ensemble",
          "repo_id": "google/owlv2-base-patch16-ensemble",
          "allow_patterns": [
            "README.md",
            "*.bin",
            "*.json",
            "**/*.json",
            "txt"
          ],
          "size_on_disk": 621073668,
          "pipeline_tag": "zero-shot-object-detection",
          "tags": [
            "transformers",
            "pytorch",
            "safetensors",
            "owlv2",
            "zero-shot-object-detection",
            "vision",
            "arxiv:2306.09683",
            "license:apache-2.0",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 693630,
          "likes": 113
        },
        {
          "id": "IDEA-Research/grounding-dino-tiny",
          "type": "hf.zero_shot_object_detection",
          "name": "IDEA-Research/grounding-dino-tiny",
          "repo_id": "IDEA-Research/grounding-dino-tiny",
          "allow_patterns": [
            "README.md",
            "*.bin",
            "*.json",
            "**/*.json",
            "txt"
          ],
          "size_on_disk": 692632123,
          "pipeline_tag": "zero-shot-object-detection",
          "tags": [
            "transformers",
            "pytorch",
            "safetensors",
            "grounding-dino",
            "zero-shot-object-detection",
            "vision",
            "arxiv:2303.05499",
            "license:apache-2.0",
            "region:us"
          ],
          "has_model_index": false,
          "downloads": 950202,
          "likes": 88
        }
      ],
      "basic_fields": [
        "model",
        "image",
        "threshold",
        "top_k",
        "candidate_labels"
      ]
    },
    {
      "title": "Token Classification",
      "description": "Performs token-level classification tasks such as Named Entity Recognition (NER).\n    text, token-classification, NER, NLP, entity-extraction\n\n    Use cases:\n    - Extract named entities (people, organizations, locations) from text\n    - Identify parts of speech in sentences\n    - Perform chunking and shallow parsing for text analysis\n    - Extract structured information from unstructured documents\n    - Build information extraction pipelines for documents",
      "namespace": "huggingface.token_classification",
      "node_type": "huggingface.token_classification.TokenClassification",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.token_classification"
          },
          "default": {
            "type": "hf.token_classification",
            "repo_id": "dbmdz/bert-large-cased-finetuned-conll03-english",
            "path": null,
            "variant": null,
            "allow_patterns": [
              "*.bin",
              "*.json",
              "**/*.json",
              "*.safetensors"
            ],
            "ignore_patterns": null
          },
          "title": "Model",
          "description": "The token classification model. BERT-large-cased-finetuned-conll03 offers high-quality NER for English text."
        },
        {
          "name": "inputs",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Input Text",
          "description": "The text to extract entities from."
        },
        {
          "name": "aggregation_strategy",
          "type": {
            "type": "enum",
            "values": [
              "simple",
              "first",
              "average",
              "max"
            ],
            "type_name": "nodetool.nodes.huggingface.token_classification.TokenClassification.AggregationStrategy"
          },
          "default": "simple",
          "title": "Aggregation Strategy",
          "description": "How to combine token predictions into entities: 'simple' merges adjacent tokens; 'first'/'average'/'max' control subword handling."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dataframe"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "model",
        "inputs",
        "aggregation_strategy"
      ]
    }
  ],
  "assets": [
    {
      "package_name": "nodetool-huggingface",
      "name": "Summarize Audio.jpg",
      "path": ""
    },
    {
      "package_name": "nodetool-huggingface",
      "name": "Controlnet.jpg",
      "path": ""
    },
    {
      "package_name": "nodetool-huggingface",
      "name": "Upscaling.jpg",
      "path": ""
    },
    {
      "package_name": "nodetool-huggingface",
      "name": "Object Detection.jpg",
      "path": ""
    },
    {
      "package_name": "nodetool-huggingface",
      "name": "Stable Diffusion.jpg",
      "path": ""
    },
    {
      "package_name": "nodetool-huggingface",
      "name": "Piano Track.jpg",
      "path": ""
    },
    {
      "package_name": "nodetool-huggingface",
      "name": "Segmentation.jpg",
      "path": ""
    },
    {
      "package_name": "nodetool-huggingface",
      "name": "Audio To Image.jpg",
      "path": ""
    },
    {
      "package_name": "nodetool-huggingface",
      "name": "Re-Imagine.jpg",
      "path": ""
    },
    {
      "package_name": "nodetool-huggingface",
      "name": "Style Transfer.jpg",
      "path": ""
    },
    {
      "package_name": "nodetool-huggingface",
      "name": "Transcribe Audio.jpg",
      "path": ""
    },
    {
      "package_name": "nodetool-huggingface",
      "name": "Movie Posters.jpg",
      "path": ""
    },
    {
      "package_name": "nodetool-huggingface",
      "name": "Audio To Spectrogram.jpg",
      "path": ""
    },
    {
      "package_name": "nodetool-huggingface",
      "name": "Depth Estimation.jpg",
      "path": ""
    },
    {
      "package_name": "nodetool-huggingface",
      "name": "Add Subtitles To Video.jpg",
      "path": ""
    },
    {
      "package_name": "nodetool-huggingface",
      "name": "stable_diffusion_xl.jpg",
      "path": ""
    },
    {
      "package_name": "nodetool-huggingface",
      "name": "Pokemon Maker.jpg",
      "path": ""
    },
    {
      "package_name": "nodetool-huggingface",
      "name": "Image to Image.jpg",
      "path": ""
    }
  ],
  "examples": [
    {
      "id": "controlnet",
      "name": "Controlnet",
      "description": "",
      "tags": [
        "start",
        "image"
      ]
    },
    {
      "id": "001b40e05a6d11f0aea400001cbe54c8",
      "name": "Re-Imagine",
      "description": "",
      "tags": [
        "start",
        "image"
      ]
    },
    {
      "id": "add_subtitles_to_video",
      "name": "Add Subtitles To Video",
      "description": "This workflow automatically transcribes speech in videos and adds subtitles. It extracts audio from the input video, uses OpenAI's Whisper model to generate word-level timestamps and transcriptions, and then renders the subtitles back onto the original video. Perfect for creating accessible content, adding captions to social media videos, or transcribing presentations.",
      "tags": [
        "start",
        "video",
        "huggingface"
      ]
    },
    {
      "id": "style_transfer",
      "name": "Style Transfer",
      "description": "Transform your images by applying artistic styles from reference images. This workflow relies on ControlNet to preserve structure while prompts and model choices guide the style. Perfect for creating artistic variations of portraits or other images.",
      "tags": [
        "huggingface",
        "image",
        "start"
      ]
    },
    {
      "id": "d6d4ffd859da11f094f9000001d6cfc2",
      "name": "Image to Image",
      "description": "",
      "tags": [
        "start",
        "image"
      ]
    },
    {
      "id": "object_detection",
      "name": "Object Detection",
      "description": "Detect objects in an image and visualize the detections",
      "tags": [
        "huggingface",
        "start"
      ]
    },
    {
      "id": "segmentation",
      "name": "Segmentation",
      "description": "Segment images and visualize the segments",
      "tags": [
        "huggingface",
        "image"
      ]
    },
    {
      "id": "3b0fb4e4988711f0b47b00001790bda3",
      "name": "Movie Posters",
      "description": "Create cinematic movie posters using AI image generation",
      "tags": [
        "start",
        "image",
        "huggingface"
      ]
    },
    {
      "id": "transcribe_audio",
      "name": "Transcribe Audio",
      "description": "Convert speech to text using Whisper model with word-level timestamps",
      "tags": [
        "start",
        "audio",
        "huggingface"
      ]
    },
    {
      "id": "8675bdaa388a11f0951800006f96a7c6",
      "name": "Pokemon Maker",
      "description": "",
      "tags": []
    },
    {
      "id": "3ae8a1cd7aa511f08640000015d0df1b",
      "name": "Audio To Image",
      "description": "Transform spoken descriptions into images with this workflow. Record or upload audio, which is transcribed by Whisper and then visualized by Stable Diffusion. Perfect for quickly generating images from verbal ideas without typing.",
      "tags": [
        "huggingface",
        "multimodal",
        "start"
      ]
    },
    {
      "id": "3dc7a22e12f311f0a84600004c6eb2d5",
      "name": "Audio To Spectrogram",
      "description": "Create a spectrogram from an audio file and use creative upscaling to transform it into wall-worthy art.",
      "tags": [
        "audio",
        "multimodal",
        "start",
        "huggingface"
      ]
    },
    {
      "id": "depth_estimation",
      "name": "Depth Estimation",
      "description": "Estimate the depth of an image",
      "tags": [
        "image",
        "huggingface"
      ]
    },
    {
      "id": "f1d42e6a12fb11f0901100001aeb0d2f",
      "name": "Summarize Audio",
      "description": "Transcribe an audio file and summarize the text.",
      "tags": [
        "audio",
        "start",
        "huggingface"
      ]
    },
    {
      "id": "dfff77a8f38911ef919400004a056799",
      "name": "Upscaling",
      "description": "Upscale low-resolution images to higher quality using RealESRGAN, a powerful AI model that enhances details and clarity without artifacts.",
      "tags": [
        "image",
        "start",
        "huggingface"
      ]
    }
  ]
}