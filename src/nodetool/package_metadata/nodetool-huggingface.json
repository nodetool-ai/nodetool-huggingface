{
  "name": "nodetool-huggingface",
  "description": "HuggingFace nodes for Nodetool",
  "version": "0.6.0",
  "authors": [
    "Matthias Georgi <matti.georgi@gmail.com>"
  ],
  "repo_id": "nodetool-ai/nodetool-huggingface",
  "nodes": [
    {
      "title": "Audio LDM",
      "description": "Generates audio using the AudioLDM model based on text prompts.\n    audio, generation, AI, text-to-audio\n\n    Use cases:\n    - Create custom music or sound effects from text descriptions\n    - Generate background audio for videos, games, or other media\n    - Produce audio content for creative projects\n    - Explore AI-generated audio for music production or sound design",
      "namespace": "huggingface.text_to_audio",
      "node_type": "huggingface.text_to_audio.AudioLDM",
      "layout": "default",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "Techno music with a strong, upbeat tempo and high melodic riffs",
          "title": "Prompt",
          "description": "A text prompt describing the desired audio."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 10,
          "title": "Num Inference Steps",
          "description": "Number of denoising steps. More steps generally improve quality but increase generation time.",
          "min": 1.0,
          "max": 100.0
        },
        {
          "name": "audio_length_in_s",
          "type": {
            "type": "float"
          },
          "default": 5.0,
          "title": "Audio Length In S",
          "description": "The desired duration of the generated audio in seconds.",
          "min": 1.0,
          "max": 30.0
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Seed",
          "description": "Seed for the random number generator. Use -1 for a random seed.",
          "min": -1.0
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "the_model_info": {},
      "recommended_models": [
        {
          "type": "hf.text_to_audio",
          "repo_id": "cvssp/audioldm-s-full-v2",
          "allow_patterns": [
            "*.safetensors",
            "*.json",
            "*.txt"
          ]
        }
      ],
      "basic_fields": [
        "model",
        "prompt"
      ],
      "is_dynamic": false
    },
    {
      "title": "Audio LDM 2",
      "description": "Generates audio using the AudioLDM2 model based on text prompts.\n    audio, generation, AI, text-to-audio\n\n    Use cases:\n    - Create custom sound effects based on textual descriptions\n    - Generate background audio for videos or games\n    - Produce audio content for multimedia projects\n    - Explore AI-generated audio for creative sound design",
      "namespace": "huggingface.text_to_audio",
      "node_type": "huggingface.text_to_audio.AudioLDM2",
      "layout": "default",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "The sound of a hammer hitting a wooden surface.",
          "title": "Prompt",
          "description": "A text prompt describing the desired audio."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "Low quality.",
          "title": "Negative Prompt",
          "description": "A text prompt describing what you don't want in the audio."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 200,
          "title": "Num Inference Steps",
          "description": "Number of denoising steps. More steps generally improve quality but increase generation time.",
          "min": 50.0,
          "max": 500.0
        },
        {
          "name": "audio_length_in_s",
          "type": {
            "type": "float"
          },
          "default": 10.0,
          "title": "Audio Length In S",
          "description": "The desired duration of the generated audio in seconds.",
          "min": 1.0,
          "max": 30.0
        },
        {
          "name": "num_waveforms_per_prompt",
          "type": {
            "type": "int"
          },
          "default": 3,
          "title": "Num Waveforms Per Prompt",
          "description": "Number of audio samples to generate per prompt.",
          "min": 1.0,
          "max": 5.0
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Seed",
          "description": "Seed for the random number generator. Use -1 for a random seed.",
          "min": -1.0
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "the_model_info": {},
      "recommended_models": [
        {
          "type": "hf.text_to_audio",
          "repo_id": "cvssp/audioldm2",
          "allow_patterns": [
            "*.safetensors",
            "*.json",
            "*.txt"
          ]
        }
      ],
      "basic_fields": [
        "model",
        "prompt"
      ],
      "is_dynamic": false
    },
    {
      "title": "Dance Diffusion",
      "description": "Generates audio using the DanceDiffusion model.\n    audio, generation, AI, music, text-to-audio\n\n    Use cases:\n    - Create AI-generated music samples\n    - Produce background music for videos or games\n    - Generate audio content for creative projects\n    - Explore AI-composed musical ideas",
      "namespace": "huggingface.text_to_audio",
      "node_type": "huggingface.text_to_audio.DanceDiffusion",
      "layout": "default",
      "properties": [
        {
          "name": "audio_length_in_s",
          "type": {
            "type": "float"
          },
          "default": 4.0,
          "title": "Audio Length In S",
          "description": "The desired duration of the generated audio in seconds.",
          "min": 1.0,
          "max": 30.0
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 50,
          "title": "Num Inference Steps",
          "description": "Number of denoising steps. More steps generally improve quality but increase generation time.",
          "min": 1.0,
          "max": 1000.0
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Seed",
          "description": "Seed for the random number generator. Use -1 for a random seed.",
          "min": -1.0
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "the_model_info": {},
      "recommended_models": [
        {
          "type": "hf.text_to_audio",
          "repo_id": "harmonai/maestro-150k",
          "allow_patterns": [
            "*.bin",
            "*.json",
            "*.txt"
          ]
        }
      ],
      "basic_fields": [
        "model",
        "prompt"
      ],
      "is_dynamic": false
    },
    {
      "title": "Music Gen",
      "description": "Generates audio (music or sound effects) from text descriptions.\n    audio, music, generation, huggingface, text-to-audio\n\n    Use cases:\n    - Create custom background music for videos or games\n    - Generate sound effects based on textual descriptions\n    - Prototype musical ideas quickly",
      "namespace": "huggingface.text_to_audio",
      "node_type": "huggingface.text_to_audio.MusicGen",
      "layout": "default",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.text_to_audio"
          },
          "default": {},
          "title": "Model ID on Huggingface",
          "description": "The model ID to use for the audio generation"
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Inputs",
          "description": "The input text to the model"
        },
        {
          "name": "max_new_tokens",
          "type": {
            "type": "int"
          },
          "default": 1024,
          "title": "Max New Tokens",
          "description": "The maximum number of tokens to generate"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "the_model_info": {},
      "recommended_models": [
        {
          "type": "hf.text_to_audio",
          "repo_id": "facebook/musicgen-small",
          "allow_patterns": [
            "*.safetensors",
            "*.json",
            "*.model"
          ]
        },
        {
          "type": "hf.text_to_audio",
          "repo_id": "facebook/musicgen-medium",
          "allow_patterns": [
            "*.safetensors",
            "*.json",
            "*.model"
          ]
        },
        {
          "type": "hf.text_to_audio",
          "repo_id": "facebook/musicgen-large",
          "allow_patterns": [
            "*.safetensors",
            "*.json",
            "*.model"
          ]
        },
        {
          "type": "hf.text_to_audio",
          "repo_id": "facebook/musicgen-melody",
          "allow_patterns": [
            "*.safetensors",
            "*.json",
            "*.model"
          ]
        },
        {
          "type": "hf.text_to_audio",
          "repo_id": "facebook/musicgen-stereo-small",
          "allow_patterns": [
            "*.safetensors",
            "*.json",
            "*.model"
          ]
        },
        {
          "type": "hf.text_to_audio",
          "repo_id": "facebook/musicgen-stereo-large",
          "allow_patterns": [
            "*.safetensors",
            "*.json",
            "*.model"
          ]
        }
      ],
      "basic_fields": [
        "model",
        "prompt"
      ],
      "is_dynamic": false
    },
    {
      "title": "Music LDM",
      "description": "Generates audio (music or sound effects) from text descriptions.\n    audio, music, generation, huggingface, text-to-audio\n\n    Use cases:\n    - Create custom background music for videos or games\n    - Generate sound effects based on textual descriptions\n    - Prototype musical ideas quickly",
      "namespace": "huggingface.text_to_audio",
      "node_type": "huggingface.text_to_audio.MusicLDM",
      "layout": "default",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.text_to_audio"
          },
          "default": {},
          "title": "Model ID on Huggingface",
          "description": "The model ID to use for the audio generation"
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Inputs",
          "description": "The input text to the model"
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 10,
          "title": "Number of Inference Steps",
          "description": "The number of inference steps to use for the generation"
        },
        {
          "name": "audio_length_in_s",
          "type": {
            "type": "float"
          },
          "default": 5.0,
          "title": "Audio Length",
          "description": "The length of the generated audio in seconds"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "the_model_info": {},
      "recommended_models": [
        {
          "type": "hf.text_to_audio",
          "repo_id": "ucsd-reach/musicldm",
          "allow_patterns": [
            "*.safetensors",
            "*.json",
            "*.txt"
          ]
        }
      ],
      "basic_fields": [
        "model",
        "prompt"
      ],
      "is_dynamic": false
    },
    {
      "title": "Stable Audio",
      "description": "Generate audio using Stable Audio model based on text prompts. Features high-quality audio synthesis with configurable parameters.\n    audio, generation, synthesis, text-to-audio, text-to-audio\n\n    Use cases:\n    - Create custom audio content from text\n    - Generate background music and sounds\n    - Produce audio for multimedia projects\n    - Create sound effects and ambience\n    - Generate experimental audio content",
      "namespace": "huggingface.text_to_audio",
      "node_type": "huggingface.text_to_audio.StableAudio",
      "layout": "default",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "A peaceful piano melody.",
          "title": "Prompt",
          "description": "A text prompt describing the desired audio."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "Low quality.",
          "title": "Negative Prompt",
          "description": "A text prompt describing what you don't want in the audio."
        },
        {
          "name": "duration",
          "type": {
            "type": "float"
          },
          "default": 10.0,
          "title": "Duration",
          "description": "The desired duration of the generated audio in seconds.",
          "min": 1.0,
          "max": 300.0
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 200,
          "title": "Num Inference Steps",
          "description": "Number of denoising steps. More steps generally improve quality but increase generation time.",
          "min": 50.0,
          "max": 500.0
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Seed",
          "description": "Seed for the random number generator. Use -1 for a random seed.",
          "min": -1.0
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "the_model_info": {},
      "recommended_models": [
        {
          "type": "hf.text_to_audio",
          "repo_id": "stabilityai/stable-audio-open-1.0",
          "allow_patterns": [
            "*.safetensors",
            "*.json",
            "*.txt"
          ]
        }
      ],
      "basic_fields": [
        "model",
        "prompt"
      ],
      "is_dynamic": false
    },
    {
      "title": "Question Answering",
      "description": "Answers questions based on a given context.\n    text, question answering, natural language processing\n\n    Use cases:\n    - Automated customer support\n    - Information retrieval from documents\n    - Reading comprehension tasks\n    - Enhancing search functionality",
      "namespace": "huggingface.question_answering",
      "node_type": "huggingface.question_answering.QuestionAnswering",
      "layout": "default",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.question_answering"
          },
          "default": {},
          "title": "Model ID on Huggingface",
          "description": "The model ID to use for question answering"
        },
        {
          "name": "context",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Context",
          "description": "The context or passage to answer questions from"
        },
        {
          "name": "question",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Question",
          "description": "The question to be answered based on the context"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "the_model_info": {},
      "recommended_models": [
        {
          "type": "hf.question_answering",
          "repo_id": "distilbert-base-cased-distilled-squad",
          "allow_patterns": [
            "*.json",
            "*.txt",
            "*.safetensors"
          ]
        },
        {
          "type": "hf.question_answering",
          "repo_id": "bert-large-uncased-whole-word-masking-finetuned-squad",
          "allow_patterns": [
            "*.json",
            "*.txt",
            "*.safetensors"
          ]
        },
        {
          "type": "hf.question_answering",
          "repo_id": "deepset/roberta-base-squad2",
          "allow_patterns": [
            "*.json",
            "*.txt",
            "*.safetensors"
          ]
        },
        {
          "type": "hf.question_answering",
          "repo_id": "distilbert-base-uncased-distilled-squad",
          "allow_patterns": [
            "*.json",
            "*.txt",
            "*.safetensors"
          ]
        }
      ],
      "basic_fields": [
        "model",
        "context",
        "question"
      ],
      "is_dynamic": false
    },
    {
      "title": "Table Question Answering",
      "description": "Answers questions based on tabular data.\n    table, question answering, natural language processing\n\n    Use cases:\n    - Querying databases using natural language\n    - Analyzing spreadsheet data with questions\n    - Extracting insights from tabular reports\n    - Automated data exploration",
      "namespace": "huggingface.question_answering",
      "node_type": "huggingface.question_answering.TableQuestionAnswering",
      "layout": "default",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.table_question_answering"
          },
          "default": {},
          "title": "Model ID on Huggingface",
          "description": "The model ID to use for table question answering"
        },
        {
          "name": "dataframe",
          "type": {
            "type": "dataframe"
          },
          "default": {},
          "title": "Table",
          "description": "The input table to query"
        },
        {
          "name": "question",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Question",
          "description": "The question to be answered based on the table"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "str"
          },
          "name": "answer"
        },
        {
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "tuple",
                "type_args": [
                  {
                    "type": "int"
                  },
                  {
                    "type": "int"
                  }
                ]
              }
            ]
          },
          "name": "coordinates"
        },
        {
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "name": "cells"
        },
        {
          "type": {
            "type": "str"
          },
          "name": "aggregator"
        }
      ],
      "the_model_info": {},
      "recommended_models": [
        {
          "type": "hf.table_question_answering",
          "repo_id": "google/tapas-base-finetuned-wtq",
          "allow_patterns": [
            "*.json",
            "*.txt",
            "*.safetensors"
          ]
        },
        {
          "type": "hf.table_question_answering",
          "repo_id": "google/tapas-large-finetuned-wtq",
          "allow_patterns": [
            "*.json",
            "*.txt",
            "*.safetensors"
          ]
        },
        {
          "type": "hf.table_question_answering",
          "repo_id": "microsoft/tapex-large-finetuned-tabfact",
          "allow_patterns": [
            "*.json",
            "*.txt",
            "*.safetensors"
          ]
        },
        {
          "type": "hf.table_question_answering",
          "repo_id": "google/tapas-large-finetuned-squad",
          "allow_patterns": [
            "*.json",
            "*.txt",
            "*.safetensors"
          ]
        }
      ],
      "basic_fields": [
        "model",
        "dataframe",
        "question"
      ],
      "is_dynamic": false
    },
    {
      "title": "Find Segment",
      "description": "Extracts a specific segment from a list of segmentation masks.\n    image, segmentation, object detection, mask",
      "namespace": "huggingface.image_segmentation",
      "node_type": "huggingface.image_segmentation.FindSegment",
      "layout": "default",
      "properties": [
        {
          "name": "segments",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "image_segmentation_result"
              }
            ]
          },
          "default": {},
          "title": "Segmentation Masks",
          "description": "The segmentation masks to search"
        },
        {
          "name": "segment_label",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Label",
          "description": "The label of the segment to extract"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "the_model_info": {},
      "recommended_models": [],
      "basic_fields": [
        "segments",
        "segment_label"
      ],
      "is_dynamic": false
    },
    {
      "title": "SAM2 Segmentation",
      "description": "Performs semantic segmentation on images using SAM2 (Segment Anything Model 2).\n    image, segmentation, object detection, scene parsing, mask\n\n    Use cases:\n    - Automatic segmentation of objects in images\n    - Instance segmentation for computer vision tasks\n    - Interactive segmentation with point prompts\n    - Scene understanding and object detection",
      "namespace": "huggingface.image_segmentation",
      "node_type": "huggingface.image_segmentation.SAM2Segmentation",
      "layout": "default",
      "properties": [
        {
          "name": "image",
          "type": {
            "type": "image"
          },
          "default": {},
          "title": "Input Image",
          "description": "The input image to segment"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "image"
              }
            ]
          },
          "name": "output"
        }
      ],
      "the_model_info": {},
      "recommended_models": [
        {
          "repo_id": "facebook/sam2-hiera-large"
        }
      ],
      "basic_fields": [
        "image"
      ],
      "is_dynamic": false
    },
    {
      "title": "Image Segmentation",
      "description": "Performs semantic segmentation on images, identifying and labeling different regions.\n    image, segmentation, object detection, scene parsing\n\n    Use cases:\n    - Segmenting objects in images\n    - Segmenting facial features in images",
      "namespace": "huggingface.image_segmentation",
      "node_type": "huggingface.image_segmentation.Segmentation",
      "layout": "default",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.image_segmentation"
          },
          "default": {
            "repo_id": "nvidia/segformer-b3-finetuned-ade-512-512"
          },
          "title": "Model ID on Huggingface",
          "description": "The model ID to use for the segmentation"
        },
        {
          "name": "image",
          "type": {
            "type": "image"
          },
          "default": {},
          "title": "Image",
          "description": "The input image to segment"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "image_segmentation_result"
              }
            ]
          },
          "name": "output"
        }
      ],
      "the_model_info": {},
      "recommended_models": [
        {
          "type": "hf.image_segmentation",
          "repo_id": "nvidia/segformer-b3-finetuned-ade-512-512",
          "allow_patterns": [
            "README.md",
            "*.bin",
            "*.json",
            "**/*.json"
          ]
        },
        {
          "type": "hf.image_segmentation",
          "repo_id": "mattmdjaga/segformer_b2_clothes",
          "allow_patterns": [
            "README.md",
            "*.bin",
            "*.json",
            "**/*.json"
          ]
        }
      ],
      "basic_fields": [
        "model",
        "image"
      ],
      "is_dynamic": false
    },
    {
      "title": "Visualize Segmentation",
      "description": "Visualizes segmentation masks on images with labels.\n    image, segmentation, visualization, mask\n\n    Use cases:\n    - Visualize results of image segmentation models\n    - Analyze and compare different segmentation techniques\n    - Create labeled images for presentations or reports",
      "namespace": "huggingface.image_segmentation",
      "node_type": "huggingface.image_segmentation.VisualizeSegmentation",
      "layout": "default",
      "properties": [
        {
          "name": "image",
          "type": {
            "type": "image"
          },
          "default": {},
          "title": "Image",
          "description": "The input image to visualize"
        },
        {
          "name": "segments",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "image_segmentation_result"
              }
            ]
          },
          "default": [],
          "title": "Segmentation Masks",
          "description": "The segmentation masks to visualize"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "the_model_info": {},
      "recommended_models": [],
      "basic_fields": [
        "image",
        "segments"
      ],
      "is_dynamic": false
    },
    {
      "title": "Stable Diffusion",
      "description": "Generates images from text prompts using Stable Diffusion.\n    image, generation, AI, text-to-image, SD\n\n    Use cases:\n    - Creating custom illustrations for various projects\n    - Generating concept art for creative endeavors\n    - Producing unique visual content for marketing materials\n    - Exploring AI-generated art for personal or professional use",
      "namespace": "huggingface.text_to_image",
      "node_type": "huggingface.text_to_image.StableDiffusion",
      "layout": "default",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.stable_diffusion"
          },
          "default": {},
          "title": "Model",
          "description": "The model to use for image generation."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt for image generation."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "(blurry, low quality, deformed, mutated, bad anatomy, extra limbs, bad proportions, text, watermark, grainy, pixelated, disfigured face, missing fingers, cropped image, bad lighting",
          "title": "Negative Prompt",
          "description": "The negative prompt to guide what should not appear in the generated image."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for the random number generator. Use -1 for a random seed.",
          "min": -1.0,
          "max": 4294967295.0
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 25,
          "title": "Num Inference Steps",
          "description": "Number of denoising steps.",
          "min": 1.0,
          "max": 100.0
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 7.5,
          "title": "Guidance Scale",
          "description": "Guidance scale for generation.",
          "min": 1.0,
          "max": 20.0
        },
        {
          "name": "scheduler",
          "type": {
            "type": "enum",
            "values": [
              "DPMSolverSDEScheduler",
              "EulerDiscreteScheduler",
              "LMSDiscreteScheduler",
              "DDIMScheduler",
              "DDPMScheduler",
              "HeunDiscreteScheduler",
              "DPMSolverMultistepScheduler",
              "DEISMultistepScheduler",
              "PNDMScheduler",
              "EulerAncestralDiscreteScheduler",
              "UniPCMultistepScheduler",
              "KDPM2DiscreteScheduler",
              "DPMSolverSinglestepScheduler",
              "KDPM2AncestralDiscreteScheduler"
            ],
            "type_name": "nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionScheduler"
          },
          "default": "EulerDiscreteScheduler",
          "title": "Scheduler",
          "description": "The scheduler to use for the diffusion process."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "hf.lora_sd_config"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRA models to use for image processing"
        },
        {
          "name": "ip_adapter_model",
          "type": {
            "type": "hf.ip_adapter"
          },
          "default": {},
          "title": "Ip Adapter Model",
          "description": "The IP adapter model to use for image processing"
        },
        {
          "name": "ip_adapter_image",
          "type": {
            "type": "image"
          },
          "default": {},
          "title": "Ip Adapter Image",
          "description": "When provided the image will be fed into the IP adapter"
        },
        {
          "name": "ip_adapter_scale",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Ip Adapter Scale",
          "description": "The strength of the IP adapter",
          "min": 0.0,
          "max": 1.0
        },
        {
          "name": "detail_level",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Detail Level",
          "description": "Level of detail for the hi-res pass. 0.0 is low detail, 1.0 is high detail.",
          "min": 0.0,
          "max": 1.0
        },
        {
          "name": "enable_tiling",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Tiling",
          "description": "Enable tiling for the VAE. This can reduce VRAM usage."
        },
        {
          "name": "enable_cpu_offload",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Cpu Offload",
          "description": "Enable CPU offload for the pipeline. This can reduce VRAM usage."
        },
        {
          "name": "upscaler",
          "type": {
            "type": "enum",
            "values": [
              "None",
              "Latent",
              "Bicubic"
            ],
            "type_name": "nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionUpscaler"
          },
          "default": "None",
          "title": "Upscaler",
          "description": "The upscaler to use for 2-pass generation."
        },
        {
          "name": "width",
          "type": {
            "type": "int"
          },
          "default": 512,
          "title": "Width",
          "description": "Width of the generated image.",
          "min": 256.0,
          "max": 1024.0
        },
        {
          "name": "height",
          "type": {
            "type": "int"
          },
          "default": 512,
          "title": "Height",
          "description": "Height of the generated image",
          "min": 256.0,
          "max": 1024.0
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "the_model_info": {},
      "recommended_models": [
        {
          "type": "hf.ip_adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "models/ip-adapter_sd15.bin"
        },
        {
          "type": "hf.ip_adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "models/ip-adapter_sd15_light.bin"
        },
        {
          "type": "hf.ip_adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "models/ip-adapter_sd15_vit-G.bin"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "SG161222/Realistic_Vision_V5.1_noVAE",
          "path": "Realistic_Vision_V5.1_fp16-no-ema.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "digiplay/majicMIX_realistic_v7",
          "path": "majicmixRealistic_v7.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "philz1337x/epicrealism",
          "path": "epicrealism_naturalSinRC1VAE.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "Lykon/DreamShaper",
          "path": "DreamShaper_5_beta2_noVae_half_pruned.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "Lykon/DreamShaper",
          "path": "DreamShaper_4BakedVae_fp16.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "XpucT/Deliberate",
          "path": "Deliberate_v6.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "XpucT/Deliberate",
          "path": "Deliberate_v6-inpainting.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "Lykon/AbsoluteReality",
          "path": "AbsoluteReality_1.8.1_pruned.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "Lykon/AbsoluteReality",
          "path": "AbsoluteReality_1.8.1_INPAINTING.inpainting.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "gsdf/Counterfeit-V2.5",
          "path": "Counterfeit-V2.5_fp16.safetensors"
        },
        {
          "type": "hf.ip_adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "models/ip-adapter_sd15.bin"
        },
        {
          "type": "hf.ip_adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "models/ip-adapter_sd15_light.bin"
        },
        {
          "type": "hf.ip_adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "models/ip-adapter_sd15_vit-G.bin"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "stabilityai/sd-x2-latent-upscaler",
          "allow_patterns": [
            "README.md",
            "**/*.safetensors",
            "**/*.json",
            "**/*.txt",
            "*.json"
          ]
        }
      ],
      "basic_fields": [
        "model",
        "prompt",
        "width",
        "height"
      ],
      "is_dynamic": false
    },
    {
      "title": "Stable Diffusion XL",
      "description": "Generates images from text prompts using Stable Diffusion XL.\n    image, generation, AI, text-to-image, SDXL\n\n    Use cases:\n    - Creating custom illustrations for marketing materials\n    - Generating concept art for game and film development\n    - Producing unique stock imagery for websites and publications\n    - Visualizing interior design concepts for clients",
      "namespace": "huggingface.text_to_image",
      "node_type": "huggingface.text_to_image.StableDiffusionXL",
      "layout": "default",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.stable_diffusion_xl"
          },
          "default": {},
          "title": "Model",
          "description": "The Stable Diffusion XL model to use for generation."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt for image generation."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to guide what should not appear in the generated image."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for the random number generator.",
          "min": -1.0,
          "max": 1000000.0
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 25,
          "title": "Num Inference Steps",
          "description": "Number of inference steps.",
          "min": 1.0,
          "max": 100.0
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 7.0,
          "title": "Guidance Scale",
          "description": "Guidance scale for generation.",
          "min": 0.0,
          "max": 20.0
        },
        {
          "name": "width",
          "type": {
            "type": "int"
          },
          "default": 1024,
          "title": "Width",
          "description": "Width of the generated image.",
          "min": 64.0,
          "max": 2048.0
        },
        {
          "name": "height",
          "type": {
            "type": "int"
          },
          "default": 1024,
          "title": "Height",
          "description": "Height of the generated image",
          "min": 64.0,
          "max": 2048.0
        },
        {
          "name": "scheduler",
          "type": {
            "type": "enum",
            "values": [
              "DPMSolverSDEScheduler",
              "EulerDiscreteScheduler",
              "LMSDiscreteScheduler",
              "DDIMScheduler",
              "DDPMScheduler",
              "HeunDiscreteScheduler",
              "DPMSolverMultistepScheduler",
              "DEISMultistepScheduler",
              "PNDMScheduler",
              "EulerAncestralDiscreteScheduler",
              "UniPCMultistepScheduler",
              "KDPM2DiscreteScheduler",
              "DPMSolverSinglestepScheduler",
              "KDPM2AncestralDiscreteScheduler"
            ],
            "type_name": "nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionScheduler"
          },
          "default": "EulerDiscreteScheduler",
          "title": "Scheduler",
          "description": "The scheduler to use for the diffusion process."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "hf.lora_sdxl_config"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRA models to use for image processing"
        },
        {
          "name": "lora_scale",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Lora Scale",
          "description": "Strength of the LoRAs",
          "min": 0.0,
          "max": 1.0
        },
        {
          "name": "ip_adapter_model",
          "type": {
            "type": "hf.ip_adapter"
          },
          "default": {},
          "title": "Ip Adapter Model",
          "description": "The IP adapter model to use for image processing"
        },
        {
          "name": "ip_adapter_image",
          "type": {
            "type": "image"
          },
          "default": {},
          "title": "Ip Adapter Image",
          "description": "When provided the image will be fed into the IP adapter"
        },
        {
          "name": "ip_adapter_scale",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Ip Adapter Scale",
          "description": "Strength of the IP adapter image",
          "min": 0.0,
          "max": 1.0
        },
        {
          "name": "enable_tiling",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Tiling",
          "description": "Enable tiling for the VAE. This can reduce VRAM usage."
        },
        {
          "name": "enable_cpu_offload",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Cpu Offload",
          "description": "Enable CPU offload for the pipeline. This can reduce VRAM usage."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "the_model_info": {},
      "recommended_models": [
        {
          "type": "hf.ip_adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "sdxl_models/ip-adapter_sdxl.bin"
        },
        {
          "type": "hf.ip_adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "sdxl_models/ip-adapter_sdxl_vit-h.bin"
        },
        {
          "type": "hf.ip_adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "sdxl_models/ip-adapter-plus_sdxl_vit-h.bin"
        },
        {
          "type": "hf.stable_diffusion_xl",
          "repo_id": "stabilityai/stable-diffusion-xl-base-1.0",
          "path": "sd_xl_base_1.0.safetensors"
        },
        {
          "type": "hf.stable_diffusion_xl",
          "repo_id": "stabilityai/stable-diffusion-xl-refiner-1.0",
          "path": "sd_xl_refiner_1.0.safetensors"
        },
        {
          "type": "hf.stable_diffusion_xl",
          "repo_id": "playgroundai/playground-v2.5-1024px-aesthetic",
          "path": "playground-v2.5-1024px-aesthetic.fp16.safetensors"
        },
        {
          "type": "hf.stable_diffusion_xl",
          "repo_id": "RunDiffusion/Juggernaut-XL-v9",
          "path": "Juggernaut-XL_v9_RunDiffusionPhoto_v2.safetensors"
        },
        {
          "type": "hf.stable_diffusion_xl",
          "repo_id": "dataautogpt3/ProteusV0.5",
          "path": "proteusV0.5.safetensors"
        },
        {
          "type": "hf.stable_diffusion_xl",
          "repo_id": "Lykon/dreamshaper-xl-lightning",
          "path": "DreamShaperXL_Lightning.safetensors"
        },
        {
          "type": "hf.stable_diffusion_xl",
          "repo_id": "Lykon/AAM_XL_AnimeMix",
          "path": "AAM_XL_Anime_Mix.safetensors"
        },
        {
          "type": "hf.stable_diffusion_xl",
          "repo_id": "stabilityai/sdxl-turbo",
          "path": "sd_xl_turbo_1.0_fp16.safetensors"
        },
        {
          "type": "hf.stable_diffusion_xl",
          "repo_id": "Lykon/dreamshaper-xl-v2-turbo",
          "path": "DreamShaperXL_Turbo_v2_1.safetensors"
        }
      ],
      "basic_fields": [
        "model",
        "prompt",
        "width",
        "height",
        "width",
        "height"
      ],
      "is_dynamic": false
    },
    {
      "title": "Image Classifier",
      "description": "Classifies images into predefined categories.\n    image, classification, labeling, categorization\n\n    Use cases:\n    - Content moderation by detecting inappropriate images\n    - Organizing photo libraries by automatically tagging images",
      "namespace": "huggingface.image_classification",
      "node_type": "huggingface.image_classification.ImageClassifier",
      "layout": "default",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.image_classification"
          },
          "default": {},
          "title": "Model ID on Huggingface",
          "description": "The model ID to use for the classification"
        },
        {
          "name": "image",
          "type": {
            "type": "image"
          },
          "default": {},
          "title": "Image",
          "description": "The input image to classify"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "float"
              }
            ]
          },
          "name": "output"
        }
      ],
      "the_model_info": {},
      "recommended_models": [
        {
          "type": "hf.image_classification",
          "repo_id": "google/vit-base-patch16-224",
          "allow_patterns": [
            "README.md",
            "*.safetensors",
            "*.json",
            "**/*.json"
          ]
        },
        {
          "type": "hf.image_classification",
          "repo_id": "microsoft/resnet-50",
          "allow_patterns": [
            "README.md",
            "*.safetensors",
            "*.json",
            "**/*.json"
          ]
        },
        {
          "type": "hf.image_classification",
          "repo_id": "microsoft/resnet-18",
          "allow_patterns": [
            "README.md",
            "*.safetensors",
            "*.json",
            "**/*.json"
          ]
        },
        {
          "type": "hf.image_classification",
          "repo_id": "apple/mobilevit-small",
          "allow_patterns": [
            "README.md",
            "*.bin",
            "*.json",
            "**/*.json"
          ]
        },
        {
          "type": "hf.image_classification",
          "repo_id": "apple/mobilevit-xx-small",
          "allow_patterns": [
            "README.md",
            "*.bin",
            "*.json",
            "**/*.json"
          ]
        },
        {
          "type": "hf.image_classification",
          "repo_id": "nateraw/vit-age-classifier",
          "allow_patterns": [
            "README.md",
            "*.safetensors",
            "*.json",
            "**/*.json"
          ]
        },
        {
          "type": "hf.image_classification",
          "repo_id": "Falconsai/nsfw_image_detection",
          "allow_patterns": [
            "README.md",
            "*.safetensors",
            "*.json",
            "**/*.json"
          ]
        },
        {
          "type": "hf.image_classification",
          "repo_id": "rizvandwiki/gender-classification-2",
          "allow_patterns": [
            "README.md",
            "*.safetensors",
            "*.json",
            "**/*.json"
          ]
        }
      ],
      "basic_fields": [
        "model",
        "image"
      ],
      "is_dynamic": false
    },
    {
      "title": "Zero-Shot Image Classifier",
      "description": "Classifies images into categories without the need for training data.\n    image, classification, labeling, categorization\n\n    Use cases:\n    - Quickly categorize images without training data\n    - Identify objects in images without predefined labels\n    - Automate image tagging for large datasets",
      "namespace": "huggingface.image_classification",
      "node_type": "huggingface.image_classification.ZeroShotImageClassifier",
      "layout": "default",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.zero_shot_image_classification"
          },
          "default": {},
          "title": "Model ID on Huggingface",
          "description": "The model ID to use for the classification"
        },
        {
          "name": "image",
          "type": {
            "type": "image"
          },
          "default": {},
          "title": "Image",
          "description": "The input image to classify"
        },
        {
          "name": "candidate_labels",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Candidate Labels",
          "description": "The candidate labels to classify the image against, separated by commas"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "float"
              }
            ]
          },
          "name": "output"
        }
      ],
      "the_model_info": {},
      "recommended_models": [
        {
          "type": "hf.zero_shot_image_classification",
          "repo_id": "openai/clip-vit-base-patch16",
          "allow_patterns": [
            "README.md",
            "pytorch_model.bin",
            "*.json",
            "*.txt"
          ]
        },
        {
          "type": "hf.zero_shot_image_classification",
          "repo_id": "openai/clip-vit-base-patch32",
          "allow_patterns": [
            "README.md",
            "pytorch_model.bin",
            "*.json",
            "*.txt"
          ]
        },
        {
          "type": "hf.zero_shot_image_classification",
          "repo_id": "openai/clip-vit-base-patch14",
          "allow_patterns": [
            "README.md",
            "pytorch_model.bin",
            "*.json",
            "*.txt"
          ]
        },
        {
          "type": "hf.zero_shot_image_classification",
          "repo_id": "patricjohncyh/fashion-clip",
          "allow_patterns": [
            "README.md",
            "pytorch_model.bin",
            "*.json",
            "*.txt"
          ]
        },
        {
          "type": "hf.zero_shot_image_classification",
          "repo_id": "laion/CLIP-ViT-H-14-laion2B-s32B-b79K",
          "allow_patterns": [
            "README.md",
            "pytorch_model.bin",
            "*.json",
            "*.txt"
          ]
        },
        {
          "type": "hf.zero_shot_image_classification",
          "repo_id": "laion/CLIP-ViT-g-14-laion2B-s12B-b42K",
          "allow_patterns": [
            "README.md",
            "pytorch_model.bin",
            "*.json",
            "*.txt"
          ]
        }
      ],
      "basic_fields": [
        "model",
        "image",
        "candidate_labels"
      ],
      "is_dynamic": false
    },
    {
      "title": "Hugging Face Pipeline",
      "description": "",
      "namespace": "huggingface.huggingface_pipeline",
      "node_type": "huggingface.huggingface_pipeline.HuggingFacePipeline",
      "layout": "default",
      "properties": [],
      "outputs": [
        {
          "type": {
            "type": "any"
          },
          "name": "output"
        }
      ],
      "the_model_info": {},
      "recommended_models": [],
      "basic_fields": [],
      "is_dynamic": false
    },
    {
      "title": "Fill Mask",
      "description": "Fills in a masked token in a given text.\n    text, fill-mask, natural language processing\n\n    Use cases:\n    - Text completion\n    - Sentence prediction\n    - Language understanding tasks\n    - Generating text options",
      "namespace": "huggingface.fill_mask",
      "node_type": "huggingface.fill_mask.FillMask",
      "layout": "default",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.fill_mask"
          },
          "default": {},
          "title": "Model ID",
          "description": "The model ID to use for fill-mask task"
        },
        {
          "name": "inputs",
          "type": {
            "type": "str"
          },
          "default": "The capital of France is [MASK].",
          "title": "Inputs",
          "description": "The input text with [MASK] token to be filled"
        },
        {
          "name": "top_k",
          "type": {
            "type": "int"
          },
          "default": 5,
          "title": "Top K",
          "description": "Number of top predictions to return"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "the_model_info": {},
      "recommended_models": [
        {
          "type": "hf.fill_mask",
          "repo_id": "bert-base-uncased",
          "allow_patterns": [
            "*.json",
            "*.txt",
            "*.safetensors"
          ]
        },
        {
          "type": "hf.fill_mask",
          "repo_id": "roberta-base",
          "allow_patterns": [
            "*.json",
            "*.txt",
            "*.safetensors"
          ]
        },
        {
          "type": "hf.fill_mask",
          "repo_id": "distilbert-base-uncased",
          "allow_patterns": [
            "*.json",
            "*.txt",
            "*.safetensors"
          ]
        },
        {
          "type": "hf.fill_mask",
          "repo_id": "albert-base-v2",
          "allow_patterns": [
            "*.json",
            "*.txt",
            "*.safetensors"
          ]
        }
      ],
      "basic_fields": [
        "model",
        "inputs",
        "top_k"
      ],
      "is_dynamic": false
    },
    {
      "title": "Image To Text",
      "description": "Generates textual descriptions from images.\n    image, captioning, OCR, image-to-text\n\n    Use cases:\n    - Generate captions for images\n    - Extract text from images (OCR)\n    - Describe image content for visually impaired users\n    - Build accessibility features for visual content",
      "namespace": "huggingface.image_to_text",
      "node_type": "huggingface.image_to_text.ImageToText",
      "layout": "default",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.image_to_text"
          },
          "default": {},
          "title": "Model ID on Huggingface",
          "description": "The model ID to use for image-to-text generation"
        },
        {
          "name": "image",
          "type": {
            "type": "image"
          },
          "default": {},
          "title": "Input Image",
          "description": "The image to generate text from"
        },
        {
          "name": "max_new_tokens",
          "type": {
            "type": "int"
          },
          "title": "Max New Tokens",
          "description": "The maximum number of tokens to generate (if supported by model)"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "str"
          },
          "name": "output"
        }
      ],
      "the_model_info": {},
      "recommended_models": [
        {
          "type": "hf.image_to_text",
          "repo_id": "Salesforce/blip-image-captioning-base",
          "allow_patterns": [
            "*.safetensors",
            "*.json",
            "*.txt",
            "*.model"
          ]
        },
        {
          "type": "hf.image_to_text",
          "repo_id": "Salesforce/blip2-opt-2.7b",
          "allow_patterns": [
            "*.safetensors",
            "*.json",
            "*.txt",
            "*.model"
          ]
        },
        {
          "type": "hf.image_to_text",
          "repo_id": "microsoft/git-base",
          "allow_patterns": [
            "*.safetensors",
            "*.json",
            "*.txt",
            "*.model"
          ]
        },
        {
          "type": "hf.image_to_text",
          "repo_id": "nlpconnect/vit-gpt2-image-captioning",
          "allow_patterns": [
            "*.safetensors",
            "*.json",
            "*.txt",
            "*.model"
          ]
        }
      ],
      "basic_fields": [
        "model",
        "image"
      ],
      "is_dynamic": false
    },
    {
      "title": "Image To Text",
      "description": "Generates text descriptions from images.\n    image, text, captioning, vision-language\n\n    Use cases:\n    - Automatic image captioning\n    - Assisting visually impaired users\n    - Enhancing image search capabilities\n    - Generating alt text for web images",
      "namespace": "huggingface.multimodal",
      "node_type": "huggingface.multimodal.ImageToText",
      "layout": "default",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.image_to_text"
          },
          "default": {},
          "title": "Model ID on Huggingface",
          "description": "The model ID to use for image-to-text generation"
        },
        {
          "name": "image",
          "type": {
            "type": "image"
          },
          "default": {},
          "title": "Input Image",
          "description": "The image to generate text from"
        },
        {
          "name": "max_new_tokens",
          "type": {
            "type": "int"
          },
          "default": 50,
          "title": "Max New Tokens",
          "description": "The maximum number of tokens to generate"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "str"
          },
          "name": "output"
        }
      ],
      "the_model_info": {},
      "recommended_models": [
        {
          "type": "hf.image_to_text",
          "repo_id": "Salesforce/blip-image-captioning-base",
          "allow_patterns": [
            "*.bin",
            "*.json",
            "*.txt    "
          ]
        },
        {
          "type": "hf.image_to_text",
          "repo_id": "Salesforce/blip-image-captioning-large",
          "allow_patterns": [
            "*.bin",
            "*.json",
            "*.txt"
          ]
        },
        {
          "type": "hf.image_to_text",
          "repo_id": "nlpconnect/vit-gpt2-image-captioning",
          "allow_patterns": [
            "*.bin",
            "*.json",
            "*.txt"
          ]
        },
        {
          "type": "hf.image_to_text",
          "repo_id": "microsoft/git-base-coco",
          "allow_patterns": [
            "*.bin",
            "*.json",
            "*.txt"
          ]
        }
      ],
      "basic_fields": [
        "model",
        "image",
        "max_new_tokens"
      ],
      "is_dynamic": false
    },
    {
      "title": "Visual Question Answering",
      "description": "Answers questions about images.\n    image, text, question answering, multimodal\n\n    Use cases:\n    - Image content analysis\n    - Automated image captioning\n    - Visual information retrieval\n    - Accessibility tools for visually impaired users",
      "namespace": "huggingface.multimodal",
      "node_type": "huggingface.multimodal.VisualQuestionAnswering",
      "layout": "default",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.visual_question_answering"
          },
          "default": {},
          "title": "Model ID on Huggingface",
          "description": "The model ID to use for visual question answering"
        },
        {
          "name": "image",
          "type": {
            "type": "image"
          },
          "default": {},
          "title": "Image",
          "description": "The image to analyze"
        },
        {
          "name": "question",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Question",
          "description": "The question to be answered about the image"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "str"
          },
          "name": "output"
        }
      ],
      "the_model_info": {},
      "recommended_models": [
        {
          "type": "hf.visual_question_answering",
          "repo_id": "Salesforce/blip-vqa-base",
          "allow_patterns": [
            "*.bin",
            "*.json",
            "*.txt"
          ]
        }
      ],
      "basic_fields": [
        "model",
        "image",
        "question"
      ],
      "is_dynamic": false
    },
    {
      "title": "Audio Classifier",
      "description": "Classifies audio into predefined categories.\n    audio, classification, labeling, categorization\n\n    Use cases:\n    - Classify music genres\n    - Detect speech vs. non-speech audio\n    - Identify environmental sounds\n    - Emotion recognition in speech\n\n    Recommended models\n    - MIT/ast-finetuned-audioset-10-10-0.4593\n    - ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition",
      "namespace": "huggingface.audio_classification",
      "node_type": "huggingface.audio_classification.AudioClassifier",
      "layout": "default",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.audio_classification"
          },
          "default": {},
          "title": "Model ID on Huggingface",
          "description": "The model ID to use for audio classification"
        },
        {
          "name": "audio",
          "type": {
            "type": "audio"
          },
          "default": {},
          "title": "Audio",
          "description": "The input audio to classify"
        },
        {
          "name": "top_k",
          "type": {
            "type": "int"
          },
          "default": 10,
          "title": "Top K",
          "description": "The number of top results to return"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "float"
              }
            ]
          },
          "name": "output"
        }
      ],
      "the_model_info": {},
      "recommended_models": [
        {
          "type": "hf.audio_classification",
          "repo_id": "MIT/ast-finetuned-audioset-10-10-0.4593",
          "allow_patterns": [
            "*.safetensors",
            "*.json"
          ]
        },
        {
          "type": "hf.audio_classification",
          "repo_id": "ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition",
          "allow_patterns": [
            "pytorch_model.bin",
            "*.json"
          ]
        }
      ],
      "basic_fields": [
        "model",
        "audio",
        "top_k"
      ],
      "is_dynamic": false
    },
    {
      "title": "Zero Shot Audio Classifier",
      "description": "Classifies audio into categories without the need for training data.\n    audio, classification, labeling, categorization, zero-shot\n\n    Use cases:\n    - Quickly categorize audio without training data\n    - Identify sounds or music genres without predefined labels\n    - Automate audio tagging for large datasets",
      "namespace": "huggingface.audio_classification",
      "node_type": "huggingface.audio_classification.ZeroShotAudioClassifier",
      "layout": "default",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.zero_shot_audio_classification"
          },
          "default": {},
          "title": "Model ID on Huggingface",
          "description": "The model ID to use for the classification"
        },
        {
          "name": "audio",
          "type": {
            "type": "audio"
          },
          "default": {},
          "title": "Audio",
          "description": "The input audio to classify"
        },
        {
          "name": "candidate_labels",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Candidate Labels",
          "description": "The candidate labels to classify the audio against, separated by commas"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "float"
              }
            ]
          },
          "name": "output"
        }
      ],
      "the_model_info": {},
      "recommended_models": [
        {
          "type": "hf.zero_shot_audio_classification",
          "repo_id": "laion/clap-htsat-unfused",
          "allow_patterns": [
            "model.safetensors",
            "*.json",
            "*.txt"
          ]
        }
      ],
      "basic_fields": [
        "model",
        "audio",
        "candidate_labels"
      ],
      "is_dynamic": false
    },
    {
      "title": "Base Image To Image",
      "description": "Base class for image-to-image transformation tasks.\n    image, transformation, generation, huggingface",
      "namespace": "huggingface.image_to_image",
      "node_type": "huggingface.image_to_image.BaseImageToImage",
      "layout": "default",
      "properties": [
        {
          "name": "image",
          "type": {
            "type": "image"
          },
          "default": {},
          "title": "Input Image",
          "description": "The input image to transform"
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt to guide the image transformation (if applicable)"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "the_model_info": {},
      "recommended_models": [],
      "basic_fields": [
        "image",
        "prompt"
      ],
      "is_dynamic": false
    },
    {
      "title": "Kandinsky 3 Image-to-Image",
      "description": "Transforms existing images using the Kandinsky-3 model based on text prompts.\n    image, generation, image-to-image\n\n    Use cases:\n    - Modify existing images based on text descriptions\n    - Apply specific styles or concepts to photographs or artwork\n    - Create variations of existing visual content\n    - Blend AI-generated elements with existing images",
      "namespace": "huggingface.image_to_image",
      "node_type": "huggingface.image_to_image.Kandinsky3Img2Img",
      "layout": "default",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "A photograph of the inside of a subway train. There are raccoons sitting on the seats. One of them is reading a newspaper. The window shows the city in the background.",
          "title": "Prompt",
          "description": "A text prompt describing the desired image transformation."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 25,
          "title": "Num Inference Steps",
          "description": "The number of denoising steps.",
          "min": 1.0,
          "max": 100.0
        },
        {
          "name": "strength",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Strength",
          "description": "The strength of the transformation. Use a value between 0.0 and 1.0.",
          "min": 0.0,
          "max": 1.0
        },
        {
          "name": "image",
          "type": {
            "type": "image"
          },
          "default": {},
          "title": "Input Image",
          "description": "The input image to transform"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Seed",
          "description": "Seed for the random number generator. Use -1 for a random seed.",
          "min": -1.0
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "the_model_info": {},
      "recommended_models": [
        {
          "repo_id": "kandinsky-community/kandinsky-3"
        }
      ],
      "basic_fields": [
        "prompt",
        "num_inference_steps",
        "strength",
        "image",
        "seed",
        "image",
        "prompt",
        "num_inference_steps",
        "strength"
      ],
      "is_dynamic": false
    },
    {
      "title": "Real ESRGAN",
      "description": "Performs image super-resolution using the RealESRGAN model.\n    image, super-resolution, enhancement, huggingface\n\n    Use cases:\n    - Enhance low-resolution images\n    - Improve image quality for printing or display\n    - Upscale images for better detail",
      "namespace": "huggingface.image_to_image",
      "node_type": "huggingface.image_to_image.RealESRGAN",
      "layout": "default",
      "properties": [
        {
          "name": "image",
          "type": {
            "type": "image"
          },
          "default": {},
          "title": "Input Image",
          "description": "The input image to transform"
        },
        {
          "name": "model",
          "type": {
            "type": "hf.real_esrgan"
          },
          "default": {},
          "title": "RealESRGAN Model",
          "description": "The RealESRGAN model to use for image super-resolution"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "the_model_info": {},
      "recommended_models": [
        {
          "type": "hf.real_esrgan",
          "repo_id": "ai-forever/Real-ESRGAN",
          "path": "RealESRGAN_x2.pth"
        },
        {
          "type": "hf.real_esrgan",
          "repo_id": "ai-forever/Real-ESRGAN",
          "path": "RealESRGAN_x4.pth"
        },
        {
          "type": "hf.real_esrgan",
          "repo_id": "ai-forever/Real-ESRGAN",
          "path": "RealESRGAN_x8.pth"
        },
        {
          "type": "hf.real_esrgan",
          "repo_id": "ximso/RealESRGAN_x4plus_anime_6B",
          "path": "RealESRGAN_x4plus_anime_6B.pth"
        }
      ],
      "basic_fields": [
        "image",
        "model"
      ],
      "is_dynamic": false
    },
    {
      "title": "Stable Diffusion ControlNet (Img2Img)",
      "description": "Transforms existing images using Stable Diffusion with ControlNet guidance.\n    image, generation, image-to-image, controlnet, SD\n\n    Use cases:\n    - Modify existing images with precise control over composition and structure\n    - Apply specific styles or concepts to photographs or artwork with guided transformations\n    - Create variations of existing visual content while maintaining certain features\n    - Enhance image editing capabilities with AI-guided transformations",
      "namespace": "huggingface.image_to_image",
      "node_type": "huggingface.image_to_image.StableDiffusionControlNetImg2Img",
      "layout": "default",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.stable_diffusion"
          },
          "default": {},
          "title": "Model",
          "description": "The model to use for image generation."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt for image generation."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "(blurry, low quality, deformed, mutated, bad anatomy, extra limbs, bad proportions, text, watermark, grainy, pixelated, disfigured face, missing fingers, cropped image, bad lighting",
          "title": "Negative Prompt",
          "description": "The negative prompt to guide what should not appear in the generated image."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for the random number generator. Use -1 for a random seed.",
          "min": -1.0,
          "max": 4294967295.0
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 25,
          "title": "Num Inference Steps",
          "description": "Number of denoising steps.",
          "min": 1.0,
          "max": 100.0
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 7.5,
          "title": "Guidance Scale",
          "description": "Guidance scale for generation.",
          "min": 1.0,
          "max": 20.0
        },
        {
          "name": "scheduler",
          "type": {
            "type": "enum",
            "values": [
              "DPMSolverSDEScheduler",
              "EulerDiscreteScheduler",
              "LMSDiscreteScheduler",
              "DDIMScheduler",
              "DDPMScheduler",
              "HeunDiscreteScheduler",
              "DPMSolverMultistepScheduler",
              "DEISMultistepScheduler",
              "PNDMScheduler",
              "EulerAncestralDiscreteScheduler",
              "UniPCMultistepScheduler",
              "KDPM2DiscreteScheduler",
              "DPMSolverSinglestepScheduler",
              "KDPM2AncestralDiscreteScheduler"
            ],
            "type_name": "nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionScheduler"
          },
          "default": "EulerDiscreteScheduler",
          "title": "Scheduler",
          "description": "The scheduler to use for the diffusion process."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "hf.lora_sd_config"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRA models to use for image processing"
        },
        {
          "name": "ip_adapter_model",
          "type": {
            "type": "hf.ip_adapter"
          },
          "default": {},
          "title": "Ip Adapter Model",
          "description": "The IP adapter model to use for image processing"
        },
        {
          "name": "ip_adapter_image",
          "type": {
            "type": "image"
          },
          "default": {},
          "title": "Ip Adapter Image",
          "description": "When provided the image will be fed into the IP adapter"
        },
        {
          "name": "ip_adapter_scale",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Ip Adapter Scale",
          "description": "The strength of the IP adapter",
          "min": 0.0,
          "max": 1.0
        },
        {
          "name": "detail_level",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Detail Level",
          "description": "Level of detail for the hi-res pass. 0.0 is low detail, 1.0 is high detail.",
          "min": 0.0,
          "max": 1.0
        },
        {
          "name": "enable_tiling",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Tiling",
          "description": "Enable tiling for the VAE. This can reduce VRAM usage."
        },
        {
          "name": "enable_cpu_offload",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Cpu Offload",
          "description": "Enable CPU offload for the pipeline. This can reduce VRAM usage."
        },
        {
          "name": "upscaler",
          "type": {
            "type": "enum",
            "values": [
              "None",
              "Latent",
              "Bicubic"
            ],
            "type_name": "nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionUpscaler"
          },
          "default": "None",
          "title": "Upscaler",
          "description": "The upscaler to use for 2-pass generation."
        },
        {
          "name": "image",
          "type": {
            "type": "image"
          },
          "default": {},
          "title": "Image",
          "description": "The input image to be transformed."
        },
        {
          "name": "strength",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Strength",
          "description": "Similarity to the input image",
          "min": 0.0,
          "max": 1.0
        },
        {
          "name": "controlnet",
          "type": {
            "type": "hf.controlnet"
          },
          "default": {},
          "title": "Controlnet",
          "description": "The ControlNet model to use for guidance."
        },
        {
          "name": "control_image",
          "type": {
            "type": "image"
          },
          "default": {},
          "title": "Control Image",
          "description": "The control image to guide the transformation."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "the_model_info": {},
      "recommended_models": [
        {
          "type": "hf.controlnet",
          "repo_id": "lllyasviel/control_v11p_sd15_canny",
          "path": "diffusion_pytorch_model.fp16.safetensors"
        },
        {
          "type": "hf.controlnet",
          "repo_id": "lllyasviel/control_v11p_sd15_inpaint",
          "path": "diffusion_pytorch_model.fp16.safetensors"
        },
        {
          "type": "hf.controlnet",
          "repo_id": "lllyasviel/control_v11p_sd15_mlsd",
          "path": "diffusion_pytorch_model.fp16.safetensors"
        },
        {
          "type": "hf.controlnet",
          "repo_id": "lllyasviel/control_v11p_sd15_tile",
          "path": "diffusion_pytorch_model.fp16.safetensors"
        },
        {
          "type": "hf.controlnet",
          "repo_id": "lllyasviel/control_v11p_sd15_shuffle",
          "path": "diffusion_pytorch_model.fp16.safetensors"
        },
        {
          "type": "hf.controlnet",
          "repo_id": "lllyasviel/control_v11p_sd15_ip2p",
          "path": "diffusion_pytorch_model.fp16.safetensors"
        },
        {
          "type": "hf.controlnet",
          "repo_id": "lllyasviel/control_v11p_sd15_lineart",
          "path": "diffusion_pytorch_model.fp16.safetensors"
        },
        {
          "type": "hf.controlnet",
          "repo_id": "lllyasviel/control_v11p_sd15_lineart_anime",
          "path": "diffusion_pytorch_model.fp16.safetensors"
        },
        {
          "type": "hf.controlnet",
          "repo_id": "lllyasviel/control_v11p_sd15_scribble",
          "path": "diffusion_pytorch_model.fp16.safetensors"
        },
        {
          "type": "hf.controlnet",
          "repo_id": "lllyasviel/control_v11p_sd15_openpose",
          "path": "diffusion_pytorch_model.fp16.safetensors"
        },
        {
          "type": "hf.controlnet",
          "repo_id": "lllyasviel/control_v11p_sd15_scribble",
          "path": "diffusion_pytorch_model.fp16.safetensors"
        },
        {
          "type": "hf.controlnet",
          "repo_id": "lllyasviel/control_v11p_sd15_seg",
          "path": "diffusion_pytorch_model.fp16.safetensors"
        },
        {
          "type": "hf.controlnet",
          "repo_id": "lllyasviel/control_v11p_sd15_hed",
          "path": "diffusion_pytorch_model.fp16.safetensors"
        },
        {
          "type": "hf.controlnet",
          "repo_id": "lllyasviel/control_v11p_sd15_normalbae",
          "path": "diffusion_pytorch_model.fp16.safetensors"
        },
        {
          "type": "hf.ip_adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "models/ip-adapter_sd15.bin"
        },
        {
          "type": "hf.ip_adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "models/ip-adapter_sd15_light.bin"
        },
        {
          "type": "hf.ip_adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "models/ip-adapter_sd15_vit-G.bin"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "SG161222/Realistic_Vision_V5.1_noVAE",
          "path": "Realistic_Vision_V5.1_fp16-no-ema.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "digiplay/majicMIX_realistic_v7",
          "path": "majicmixRealistic_v7.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "philz1337x/epicrealism",
          "path": "epicrealism_naturalSinRC1VAE.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "Lykon/DreamShaper",
          "path": "DreamShaper_5_beta2_noVae_half_pruned.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "Lykon/DreamShaper",
          "path": "DreamShaper_4BakedVae_fp16.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "XpucT/Deliberate",
          "path": "Deliberate_v6.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "XpucT/Deliberate",
          "path": "Deliberate_v6-inpainting.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "Lykon/AbsoluteReality",
          "path": "AbsoluteReality_1.8.1_pruned.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "Lykon/AbsoluteReality",
          "path": "AbsoluteReality_1.8.1_INPAINTING.inpainting.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "gsdf/Counterfeit-V2.5",
          "path": "Counterfeit-V2.5_fp16.safetensors"
        },
        {
          "type": "hf.ip_adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "models/ip-adapter_sd15.bin"
        },
        {
          "type": "hf.ip_adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "models/ip-adapter_sd15_light.bin"
        },
        {
          "type": "hf.ip_adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "models/ip-adapter_sd15_vit-G.bin"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "stabilityai/sd-x2-latent-upscaler",
          "allow_patterns": [
            "README.md",
            "**/*.safetensors",
            "**/*.json",
            "**/*.txt",
            "*.json"
          ]
        }
      ],
      "basic_fields": [
        "model",
        "prompt",
        "image",
        "controlnet",
        "control_image"
      ],
      "is_dynamic": false
    },
    {
      "title": "Stable Diffusion ControlNet Inpaint",
      "description": "Performs inpainting on images using Stable Diffusion with ControlNet guidance.\n    image, inpainting, controlnet, SD\n\n    Use cases:\n    - Remove unwanted objects from images with precise control\n    - Fill in missing parts of images guided by control images\n    - Modify specific areas of images while preserving the rest and maintaining structure",
      "namespace": "huggingface.image_to_image",
      "node_type": "huggingface.image_to_image.StableDiffusionControlNetInpaint",
      "layout": "default",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.stable_diffusion"
          },
          "default": {},
          "title": "Model",
          "description": "The model to use for image generation."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt for image generation."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "(blurry, low quality, deformed, mutated, bad anatomy, extra limbs, bad proportions, text, watermark, grainy, pixelated, disfigured face, missing fingers, cropped image, bad lighting",
          "title": "Negative Prompt",
          "description": "The negative prompt to guide what should not appear in the generated image."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for the random number generator. Use -1 for a random seed.",
          "min": -1.0,
          "max": 4294967295.0
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 25,
          "title": "Num Inference Steps",
          "description": "Number of denoising steps.",
          "min": 1.0,
          "max": 100.0
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 7.5,
          "title": "Guidance Scale",
          "description": "Guidance scale for generation.",
          "min": 1.0,
          "max": 20.0
        },
        {
          "name": "scheduler",
          "type": {
            "type": "enum",
            "values": [
              "DPMSolverSDEScheduler",
              "EulerDiscreteScheduler",
              "LMSDiscreteScheduler",
              "DDIMScheduler",
              "DDPMScheduler",
              "HeunDiscreteScheduler",
              "DPMSolverMultistepScheduler",
              "DEISMultistepScheduler",
              "PNDMScheduler",
              "EulerAncestralDiscreteScheduler",
              "UniPCMultistepScheduler",
              "KDPM2DiscreteScheduler",
              "DPMSolverSinglestepScheduler",
              "KDPM2AncestralDiscreteScheduler"
            ],
            "type_name": "nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionScheduler"
          },
          "default": "EulerDiscreteScheduler",
          "title": "Scheduler",
          "description": "The scheduler to use for the diffusion process."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "hf.lora_sd_config"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRA models to use for image processing"
        },
        {
          "name": "ip_adapter_model",
          "type": {
            "type": "hf.ip_adapter"
          },
          "default": {},
          "title": "Ip Adapter Model",
          "description": "The IP adapter model to use for image processing"
        },
        {
          "name": "ip_adapter_image",
          "type": {
            "type": "image"
          },
          "default": {},
          "title": "Ip Adapter Image",
          "description": "When provided the image will be fed into the IP adapter"
        },
        {
          "name": "ip_adapter_scale",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Ip Adapter Scale",
          "description": "The strength of the IP adapter",
          "min": 0.0,
          "max": 1.0
        },
        {
          "name": "detail_level",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Detail Level",
          "description": "Level of detail for the hi-res pass. 0.0 is low detail, 1.0 is high detail.",
          "min": 0.0,
          "max": 1.0
        },
        {
          "name": "enable_tiling",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Tiling",
          "description": "Enable tiling for the VAE. This can reduce VRAM usage."
        },
        {
          "name": "enable_cpu_offload",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Cpu Offload",
          "description": "Enable CPU offload for the pipeline. This can reduce VRAM usage."
        },
        {
          "name": "upscaler",
          "type": {
            "type": "enum",
            "values": [
              "None",
              "Latent",
              "Bicubic"
            ],
            "type_name": "nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionUpscaler"
          },
          "default": "None",
          "title": "Upscaler",
          "description": "The upscaler to use for 2-pass generation."
        },
        {
          "name": "controlnet",
          "type": {
            "type": "enum",
            "values": [
              "lllyasviel/control_v11p_sd15_inpaint"
            ],
            "type_name": "nodetool.nodes.huggingface.image_to_image.StableDiffusionControlNetModel"
          },
          "default": "lllyasviel/control_v11p_sd15_inpaint",
          "title": "Controlnet",
          "description": "The ControlNet model to use for guidance."
        },
        {
          "name": "init_image",
          "type": {
            "type": "image"
          },
          "default": {},
          "title": "Init Image",
          "description": "The initial image to be inpainted."
        },
        {
          "name": "mask_image",
          "type": {
            "type": "image"
          },
          "default": {},
          "title": "Mask Image",
          "description": "The mask image indicating areas to be inpainted."
        },
        {
          "name": "control_image",
          "type": {
            "type": "image"
          },
          "default": {},
          "title": "Control Image",
          "description": "The control image to guide the inpainting process."
        },
        {
          "name": "controlnet_conditioning_scale",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Controlnet Conditioning Scale",
          "description": "The scale for ControlNet conditioning.",
          "min": 0.0,
          "max": 2.0
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "the_model_info": {},
      "recommended_models": [
        {
          "type": "hf.ip_adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "models/ip-adapter_sd15.bin"
        },
        {
          "type": "hf.ip_adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "models/ip-adapter_sd15_light.bin"
        },
        {
          "type": "hf.ip_adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "models/ip-adapter_sd15_vit-G.bin"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "SG161222/Realistic_Vision_V5.1_noVAE",
          "path": "Realistic_Vision_V5.1_fp16-no-ema.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "digiplay/majicMIX_realistic_v7",
          "path": "majicmixRealistic_v7.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "philz1337x/epicrealism",
          "path": "epicrealism_naturalSinRC1VAE.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "Lykon/DreamShaper",
          "path": "DreamShaper_5_beta2_noVae_half_pruned.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "Lykon/DreamShaper",
          "path": "DreamShaper_4BakedVae_fp16.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "XpucT/Deliberate",
          "path": "Deliberate_v6.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "XpucT/Deliberate",
          "path": "Deliberate_v6-inpainting.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "Lykon/AbsoluteReality",
          "path": "AbsoluteReality_1.8.1_pruned.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "Lykon/AbsoluteReality",
          "path": "AbsoluteReality_1.8.1_INPAINTING.inpainting.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "gsdf/Counterfeit-V2.5",
          "path": "Counterfeit-V2.5_fp16.safetensors"
        },
        {
          "type": "hf.ip_adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "models/ip-adapter_sd15.bin"
        },
        {
          "type": "hf.ip_adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "models/ip-adapter_sd15_light.bin"
        },
        {
          "type": "hf.ip_adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "models/ip-adapter_sd15_vit-G.bin"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "stabilityai/sd-x2-latent-upscaler",
          "allow_patterns": [
            "README.md",
            "**/*.safetensors",
            "**/*.json",
            "**/*.txt",
            "*.json"
          ]
        }
      ],
      "basic_fields": [
        "model",
        "prompt",
        "init_image",
        "mask_image",
        "control_image",
        "controlnet_conditioning_scale"
      ],
      "is_dynamic": false
    },
    {
      "title": "Stable Diffusion ControlNet",
      "description": "Generates images using Stable Diffusion with ControlNet guidance.\n    image, generation, text-to-image, controlnet, SD\n\n    Use cases:\n    - Generate images with precise control over composition and structure\n    - Create variations of existing images while maintaining specific features\n    - Artistic image generation with guided outputs",
      "namespace": "huggingface.image_to_image",
      "node_type": "huggingface.image_to_image.StableDiffusionControlNet",
      "layout": "default",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.stable_diffusion"
          },
          "default": {},
          "title": "Model",
          "description": "The model to use for image generation."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt for image generation."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "(blurry, low quality, deformed, mutated, bad anatomy, extra limbs, bad proportions, text, watermark, grainy, pixelated, disfigured face, missing fingers, cropped image, bad lighting",
          "title": "Negative Prompt",
          "description": "The negative prompt to guide what should not appear in the generated image."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for the random number generator. Use -1 for a random seed.",
          "min": -1.0,
          "max": 4294967295.0
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 25,
          "title": "Num Inference Steps",
          "description": "Number of denoising steps.",
          "min": 1.0,
          "max": 100.0
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 7.5,
          "title": "Guidance Scale",
          "description": "Guidance scale for generation.",
          "min": 1.0,
          "max": 20.0
        },
        {
          "name": "scheduler",
          "type": {
            "type": "enum",
            "values": [
              "DPMSolverSDEScheduler",
              "EulerDiscreteScheduler",
              "LMSDiscreteScheduler",
              "DDIMScheduler",
              "DDPMScheduler",
              "HeunDiscreteScheduler",
              "DPMSolverMultistepScheduler",
              "DEISMultistepScheduler",
              "PNDMScheduler",
              "EulerAncestralDiscreteScheduler",
              "UniPCMultistepScheduler",
              "KDPM2DiscreteScheduler",
              "DPMSolverSinglestepScheduler",
              "KDPM2AncestralDiscreteScheduler"
            ],
            "type_name": "nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionScheduler"
          },
          "default": "EulerDiscreteScheduler",
          "title": "Scheduler",
          "description": "The scheduler to use for the diffusion process."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "hf.lora_sd_config"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRA models to use for image processing"
        },
        {
          "name": "ip_adapter_model",
          "type": {
            "type": "hf.ip_adapter"
          },
          "default": {},
          "title": "Ip Adapter Model",
          "description": "The IP adapter model to use for image processing"
        },
        {
          "name": "ip_adapter_image",
          "type": {
            "type": "image"
          },
          "default": {},
          "title": "Ip Adapter Image",
          "description": "When provided the image will be fed into the IP adapter"
        },
        {
          "name": "ip_adapter_scale",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Ip Adapter Scale",
          "description": "The strength of the IP adapter",
          "min": 0.0,
          "max": 1.0
        },
        {
          "name": "detail_level",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Detail Level",
          "description": "Level of detail for the hi-res pass. 0.0 is low detail, 1.0 is high detail.",
          "min": 0.0,
          "max": 1.0
        },
        {
          "name": "enable_tiling",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Tiling",
          "description": "Enable tiling for the VAE. This can reduce VRAM usage."
        },
        {
          "name": "enable_cpu_offload",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Cpu Offload",
          "description": "Enable CPU offload for the pipeline. This can reduce VRAM usage."
        },
        {
          "name": "upscaler",
          "type": {
            "type": "enum",
            "values": [
              "None",
              "Latent",
              "Bicubic"
            ],
            "type_name": "nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionUpscaler"
          },
          "default": "None",
          "title": "Upscaler",
          "description": "The upscaler to use for 2-pass generation."
        },
        {
          "name": "controlnet",
          "type": {
            "type": "hf.controlnet"
          },
          "default": {},
          "title": "Controlnet",
          "description": "The ControlNet model to use for guidance."
        },
        {
          "name": "control_image",
          "type": {
            "type": "image"
          },
          "default": {},
          "title": "Control Image",
          "description": "The control image to guide the generation process."
        },
        {
          "name": "controlnet_conditioning_scale",
          "type": {
            "type": "float"
          },
          "default": 1.0,
          "title": "Controlnet Conditioning Scale",
          "description": "The scale for ControlNet conditioning.",
          "min": 0.0,
          "max": 2.0
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "the_model_info": {},
      "recommended_models": [
        {
          "type": "hf.controlnet",
          "repo_id": "lllyasviel/control_v11p_sd15_canny",
          "path": "diffusion_pytorch_model.fp16.safetensors"
        },
        {
          "type": "hf.controlnet",
          "repo_id": "lllyasviel/control_v11p_sd15_inpaint",
          "path": "diffusion_pytorch_model.fp16.safetensors"
        },
        {
          "type": "hf.controlnet",
          "repo_id": "lllyasviel/control_v11p_sd15_mlsd",
          "path": "diffusion_pytorch_model.fp16.safetensors"
        },
        {
          "type": "hf.controlnet",
          "repo_id": "lllyasviel/control_v11p_sd15_tile",
          "path": "diffusion_pytorch_model.fp16.safetensors"
        },
        {
          "type": "hf.controlnet",
          "repo_id": "lllyasviel/control_v11p_sd15_shuffle",
          "path": "diffusion_pytorch_model.fp16.safetensors"
        },
        {
          "type": "hf.controlnet",
          "repo_id": "lllyasviel/control_v11p_sd15_ip2p",
          "path": "diffusion_pytorch_model.fp16.safetensors"
        },
        {
          "type": "hf.controlnet",
          "repo_id": "lllyasviel/control_v11p_sd15_lineart",
          "path": "diffusion_pytorch_model.fp16.safetensors"
        },
        {
          "type": "hf.controlnet",
          "repo_id": "lllyasviel/control_v11p_sd15_lineart_anime",
          "path": "diffusion_pytorch_model.fp16.safetensors"
        },
        {
          "type": "hf.controlnet",
          "repo_id": "lllyasviel/control_v11p_sd15_scribble",
          "path": "diffusion_pytorch_model.fp16.safetensors"
        },
        {
          "type": "hf.controlnet",
          "repo_id": "lllyasviel/control_v11p_sd15_openpose",
          "path": "diffusion_pytorch_model.fp16.safetensors"
        },
        {
          "type": "hf.controlnet",
          "repo_id": "lllyasviel/control_v11p_sd15_scribble",
          "path": "diffusion_pytorch_model.fp16.safetensors"
        },
        {
          "type": "hf.controlnet",
          "repo_id": "lllyasviel/control_v11p_sd15_seg",
          "path": "diffusion_pytorch_model.fp16.safetensors"
        },
        {
          "type": "hf.controlnet",
          "repo_id": "lllyasviel/control_v11p_sd15_hed",
          "path": "diffusion_pytorch_model.fp16.safetensors"
        },
        {
          "type": "hf.controlnet",
          "repo_id": "lllyasviel/control_v11p_sd15_normalbae",
          "path": "diffusion_pytorch_model.fp16.safetensors"
        },
        {
          "type": "hf.ip_adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "models/ip-adapter_sd15.bin"
        },
        {
          "type": "hf.ip_adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "models/ip-adapter_sd15_light.bin"
        },
        {
          "type": "hf.ip_adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "models/ip-adapter_sd15_vit-G.bin"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "SG161222/Realistic_Vision_V5.1_noVAE",
          "path": "Realistic_Vision_V5.1_fp16-no-ema.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "digiplay/majicMIX_realistic_v7",
          "path": "majicmixRealistic_v7.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "philz1337x/epicrealism",
          "path": "epicrealism_naturalSinRC1VAE.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "Lykon/DreamShaper",
          "path": "DreamShaper_5_beta2_noVae_half_pruned.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "Lykon/DreamShaper",
          "path": "DreamShaper_4BakedVae_fp16.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "XpucT/Deliberate",
          "path": "Deliberate_v6.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "XpucT/Deliberate",
          "path": "Deliberate_v6-inpainting.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "Lykon/AbsoluteReality",
          "path": "AbsoluteReality_1.8.1_pruned.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "Lykon/AbsoluteReality",
          "path": "AbsoluteReality_1.8.1_INPAINTING.inpainting.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "gsdf/Counterfeit-V2.5",
          "path": "Counterfeit-V2.5_fp16.safetensors"
        },
        {
          "type": "hf.ip_adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "models/ip-adapter_sd15.bin"
        },
        {
          "type": "hf.ip_adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "models/ip-adapter_sd15_light.bin"
        },
        {
          "type": "hf.ip_adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "models/ip-adapter_sd15_vit-G.bin"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "stabilityai/sd-x2-latent-upscaler",
          "allow_patterns": [
            "README.md",
            "**/*.safetensors",
            "**/*.json",
            "**/*.txt",
            "*.json"
          ]
        }
      ],
      "basic_fields": [
        "model",
        "prompt",
        "controlnet",
        "control_image",
        "controlnet_conditioning_scale"
      ],
      "is_dynamic": false
    },
    {
      "title": "Stable Diffusion (Img2Img)",
      "description": "Transforms existing images based on text prompts using Stable Diffusion.\n    image, generation, image-to-image, SD, img2img\n\n    Use cases:\n    - Modifying existing images to fit a specific style or theme\n    - Enhancing or altering photographs\n    - Creating variations of existing artwork\n    - Applying text-guided edits to images",
      "namespace": "huggingface.image_to_image",
      "node_type": "huggingface.image_to_image.StableDiffusionImg2Img",
      "layout": "default",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.stable_diffusion"
          },
          "default": {},
          "title": "Model",
          "description": "The model to use for image generation."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt for image generation."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "(blurry, low quality, deformed, mutated, bad anatomy, extra limbs, bad proportions, text, watermark, grainy, pixelated, disfigured face, missing fingers, cropped image, bad lighting",
          "title": "Negative Prompt",
          "description": "The negative prompt to guide what should not appear in the generated image."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for the random number generator. Use -1 for a random seed.",
          "min": -1.0,
          "max": 4294967295.0
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 25,
          "title": "Num Inference Steps",
          "description": "Number of denoising steps.",
          "min": 1.0,
          "max": 100.0
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 7.5,
          "title": "Guidance Scale",
          "description": "Guidance scale for generation.",
          "min": 1.0,
          "max": 20.0
        },
        {
          "name": "scheduler",
          "type": {
            "type": "enum",
            "values": [
              "DPMSolverSDEScheduler",
              "EulerDiscreteScheduler",
              "LMSDiscreteScheduler",
              "DDIMScheduler",
              "DDPMScheduler",
              "HeunDiscreteScheduler",
              "DPMSolverMultistepScheduler",
              "DEISMultistepScheduler",
              "PNDMScheduler",
              "EulerAncestralDiscreteScheduler",
              "UniPCMultistepScheduler",
              "KDPM2DiscreteScheduler",
              "DPMSolverSinglestepScheduler",
              "KDPM2AncestralDiscreteScheduler"
            ],
            "type_name": "nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionScheduler"
          },
          "default": "EulerDiscreteScheduler",
          "title": "Scheduler",
          "description": "The scheduler to use for the diffusion process."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "hf.lora_sd_config"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRA models to use for image processing"
        },
        {
          "name": "ip_adapter_model",
          "type": {
            "type": "hf.ip_adapter"
          },
          "default": {},
          "title": "Ip Adapter Model",
          "description": "The IP adapter model to use for image processing"
        },
        {
          "name": "ip_adapter_image",
          "type": {
            "type": "image"
          },
          "default": {},
          "title": "Ip Adapter Image",
          "description": "When provided the image will be fed into the IP adapter"
        },
        {
          "name": "ip_adapter_scale",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Ip Adapter Scale",
          "description": "The strength of the IP adapter",
          "min": 0.0,
          "max": 1.0
        },
        {
          "name": "detail_level",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Detail Level",
          "description": "Level of detail for the hi-res pass. 0.0 is low detail, 1.0 is high detail.",
          "min": 0.0,
          "max": 1.0
        },
        {
          "name": "enable_tiling",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Tiling",
          "description": "Enable tiling for the VAE. This can reduce VRAM usage."
        },
        {
          "name": "enable_cpu_offload",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Cpu Offload",
          "description": "Enable CPU offload for the pipeline. This can reduce VRAM usage."
        },
        {
          "name": "upscaler",
          "type": {
            "type": "enum",
            "values": [
              "None",
              "Latent",
              "Bicubic"
            ],
            "type_name": "nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionUpscaler"
          },
          "default": "None",
          "title": "Upscaler",
          "description": "The upscaler to use for 2-pass generation."
        },
        {
          "name": "init_image",
          "type": {
            "type": "image"
          },
          "default": {},
          "title": "Init Image",
          "description": "The initial image for Image-to-Image generation."
        },
        {
          "name": "strength",
          "type": {
            "type": "float"
          },
          "default": 0.8,
          "title": "Strength",
          "description": "Strength for Image-to-Image generation. Higher values allow for more deviation from the original image.",
          "min": 0.0,
          "max": 1.0
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "the_model_info": {},
      "recommended_models": [
        {
          "type": "hf.ip_adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "models/ip-adapter_sd15.bin"
        },
        {
          "type": "hf.ip_adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "models/ip-adapter_sd15_light.bin"
        },
        {
          "type": "hf.ip_adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "models/ip-adapter_sd15_vit-G.bin"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "SG161222/Realistic_Vision_V5.1_noVAE",
          "path": "Realistic_Vision_V5.1_fp16-no-ema.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "digiplay/majicMIX_realistic_v7",
          "path": "majicmixRealistic_v7.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "philz1337x/epicrealism",
          "path": "epicrealism_naturalSinRC1VAE.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "Lykon/DreamShaper",
          "path": "DreamShaper_5_beta2_noVae_half_pruned.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "Lykon/DreamShaper",
          "path": "DreamShaper_4BakedVae_fp16.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "XpucT/Deliberate",
          "path": "Deliberate_v6.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "XpucT/Deliberate",
          "path": "Deliberate_v6-inpainting.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "Lykon/AbsoluteReality",
          "path": "AbsoluteReality_1.8.1_pruned.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "Lykon/AbsoluteReality",
          "path": "AbsoluteReality_1.8.1_INPAINTING.inpainting.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "gsdf/Counterfeit-V2.5",
          "path": "Counterfeit-V2.5_fp16.safetensors"
        },
        {
          "type": "hf.ip_adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "models/ip-adapter_sd15.bin"
        },
        {
          "type": "hf.ip_adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "models/ip-adapter_sd15_light.bin"
        },
        {
          "type": "hf.ip_adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "models/ip-adapter_sd15_vit-G.bin"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "stabilityai/sd-x2-latent-upscaler",
          "allow_patterns": [
            "README.md",
            "**/*.safetensors",
            "**/*.json",
            "**/*.txt",
            "*.json"
          ]
        }
      ],
      "basic_fields": [
        "model",
        "prompt",
        "init_image",
        "strength"
      ],
      "is_dynamic": false
    },
    {
      "title": "Stable Diffusion (Inpaint)",
      "description": "Performs inpainting on images using Stable Diffusion.\n    image, inpainting, AI, SD\n\n    Use cases:\n    - Remove unwanted objects from images\n    - Fill in missing parts of images\n    - Modify specific areas of images while preserving the rest",
      "namespace": "huggingface.image_to_image",
      "node_type": "huggingface.image_to_image.StableDiffusionInpaint",
      "layout": "default",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.stable_diffusion"
          },
          "default": {},
          "title": "Model",
          "description": "The model to use for image generation."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt for image generation."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "(blurry, low quality, deformed, mutated, bad anatomy, extra limbs, bad proportions, text, watermark, grainy, pixelated, disfigured face, missing fingers, cropped image, bad lighting",
          "title": "Negative Prompt",
          "description": "The negative prompt to guide what should not appear in the generated image."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for the random number generator. Use -1 for a random seed.",
          "min": -1.0,
          "max": 4294967295.0
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 25,
          "title": "Num Inference Steps",
          "description": "Number of denoising steps.",
          "min": 1.0,
          "max": 100.0
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 7.5,
          "title": "Guidance Scale",
          "description": "Guidance scale for generation.",
          "min": 1.0,
          "max": 20.0
        },
        {
          "name": "scheduler",
          "type": {
            "type": "enum",
            "values": [
              "DPMSolverSDEScheduler",
              "EulerDiscreteScheduler",
              "LMSDiscreteScheduler",
              "DDIMScheduler",
              "DDPMScheduler",
              "HeunDiscreteScheduler",
              "DPMSolverMultistepScheduler",
              "DEISMultistepScheduler",
              "PNDMScheduler",
              "EulerAncestralDiscreteScheduler",
              "UniPCMultistepScheduler",
              "KDPM2DiscreteScheduler",
              "DPMSolverSinglestepScheduler",
              "KDPM2AncestralDiscreteScheduler"
            ],
            "type_name": "nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionScheduler"
          },
          "default": "EulerDiscreteScheduler",
          "title": "Scheduler",
          "description": "The scheduler to use for the diffusion process."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "hf.lora_sd_config"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRA models to use for image processing"
        },
        {
          "name": "ip_adapter_model",
          "type": {
            "type": "hf.ip_adapter"
          },
          "default": {},
          "title": "Ip Adapter Model",
          "description": "The IP adapter model to use for image processing"
        },
        {
          "name": "ip_adapter_image",
          "type": {
            "type": "image"
          },
          "default": {},
          "title": "Ip Adapter Image",
          "description": "When provided the image will be fed into the IP adapter"
        },
        {
          "name": "ip_adapter_scale",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Ip Adapter Scale",
          "description": "The strength of the IP adapter",
          "min": 0.0,
          "max": 1.0
        },
        {
          "name": "detail_level",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Detail Level",
          "description": "Level of detail for the hi-res pass. 0.0 is low detail, 1.0 is high detail.",
          "min": 0.0,
          "max": 1.0
        },
        {
          "name": "enable_tiling",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Tiling",
          "description": "Enable tiling for the VAE. This can reduce VRAM usage."
        },
        {
          "name": "enable_cpu_offload",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Cpu Offload",
          "description": "Enable CPU offload for the pipeline. This can reduce VRAM usage."
        },
        {
          "name": "upscaler",
          "type": {
            "type": "enum",
            "values": [
              "None",
              "Latent",
              "Bicubic"
            ],
            "type_name": "nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionUpscaler"
          },
          "default": "None",
          "title": "Upscaler",
          "description": "The upscaler to use for 2-pass generation."
        },
        {
          "name": "init_image",
          "type": {
            "type": "image"
          },
          "default": {},
          "title": "Init Image",
          "description": "The initial image to be inpainted."
        },
        {
          "name": "mask_image",
          "type": {
            "type": "image"
          },
          "default": {},
          "title": "Mask Image",
          "description": "The mask image indicating areas to be inpainted."
        },
        {
          "name": "strength",
          "type": {
            "type": "float"
          },
          "default": 0.8,
          "title": "Strength",
          "description": "Strength for inpainting. Higher values allow for more deviation from the original image.",
          "min": 0.0,
          "max": 1.0
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "the_model_info": {},
      "recommended_models": [
        {
          "type": "hf.ip_adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "models/ip-adapter_sd15.bin"
        },
        {
          "type": "hf.ip_adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "models/ip-adapter_sd15_light.bin"
        },
        {
          "type": "hf.ip_adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "models/ip-adapter_sd15_vit-G.bin"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "SG161222/Realistic_Vision_V5.1_noVAE",
          "path": "Realistic_Vision_V5.1_fp16-no-ema.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "digiplay/majicMIX_realistic_v7",
          "path": "majicmixRealistic_v7.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "philz1337x/epicrealism",
          "path": "epicrealism_naturalSinRC1VAE.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "Lykon/DreamShaper",
          "path": "DreamShaper_5_beta2_noVae_half_pruned.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "Lykon/DreamShaper",
          "path": "DreamShaper_4BakedVae_fp16.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "XpucT/Deliberate",
          "path": "Deliberate_v6.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "XpucT/Deliberate",
          "path": "Deliberate_v6-inpainting.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "Lykon/AbsoluteReality",
          "path": "AbsoluteReality_1.8.1_pruned.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "Lykon/AbsoluteReality",
          "path": "AbsoluteReality_1.8.1_INPAINTING.inpainting.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "gsdf/Counterfeit-V2.5",
          "path": "Counterfeit-V2.5_fp16.safetensors"
        },
        {
          "type": "hf.ip_adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "models/ip-adapter_sd15.bin"
        },
        {
          "type": "hf.ip_adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "models/ip-adapter_sd15_light.bin"
        },
        {
          "type": "hf.ip_adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "models/ip-adapter_sd15_vit-G.bin"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "stabilityai/sd-x2-latent-upscaler",
          "allow_patterns": [
            "README.md",
            "**/*.safetensors",
            "**/*.json",
            "**/*.txt",
            "*.json"
          ]
        }
      ],
      "basic_fields": [
        "model",
        "prompt",
        "init_image",
        "mask_image",
        "strength"
      ],
      "is_dynamic": false
    },
    {
      "title": "Stable Diffusion 4x Upscale",
      "description": "Upscales an image using Stable Diffusion 4x upscaler.\n    image, upscaling, stable-diffusion, SD\n\n    Use cases:\n    - Enhance low-resolution images\n    - Improve image quality for printing or display\n    - Create high-resolution versions of small images",
      "namespace": "huggingface.image_to_image",
      "node_type": "huggingface.image_to_image.StableDiffusionUpscale",
      "layout": "default",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt for image generation."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to guide what should not appear in the generated image."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 25,
          "title": "Num Inference Steps",
          "description": "Number of upscaling steps.",
          "min": 1.0,
          "max": 100.0
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 7.5,
          "title": "Guidance Scale",
          "description": "Guidance scale for generation.",
          "min": 1.0,
          "max": 20.0
        },
        {
          "name": "image",
          "type": {
            "type": "image"
          },
          "default": {},
          "title": "Image",
          "description": "The initial image for Image-to-Image generation."
        },
        {
          "name": "scheduler",
          "type": {
            "type": "enum",
            "values": [
              "DPMSolverSDEScheduler",
              "EulerDiscreteScheduler",
              "LMSDiscreteScheduler",
              "DDIMScheduler",
              "DDPMScheduler",
              "HeunDiscreteScheduler",
              "DPMSolverMultistepScheduler",
              "DEISMultistepScheduler",
              "PNDMScheduler",
              "EulerAncestralDiscreteScheduler",
              "UniPCMultistepScheduler",
              "KDPM2DiscreteScheduler",
              "DPMSolverSinglestepScheduler",
              "KDPM2AncestralDiscreteScheduler"
            ],
            "type_name": "nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionScheduler"
          },
          "default": "HeunDiscreteScheduler",
          "title": "Scheduler",
          "description": "The scheduler to use for the diffusion process."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for the random number generator. Use -1 for a random seed.",
          "min": -1.0,
          "max": 4294967295.0
        },
        {
          "name": "enable_tiling",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Tiling",
          "description": "Enable tiling to save VRAM"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "the_model_info": {},
      "recommended_models": [
        {
          "type": "hf.stable_diffusion_upscale",
          "repo_id": "stabilityai/stable-diffusion-x4-upscaler",
          "allow_patterns": [
            "README.md",
            "**/*.fp16.safetensors",
            "**/*.json",
            "**/*.txt",
            "*.json"
          ]
        }
      ],
      "basic_fields": [
        "prompt",
        "negative_prompt",
        "image"
      ],
      "is_dynamic": false
    },
    {
      "title": "Stable Diffusion XL ControlNet",
      "description": "Transforms existing images using Stable Diffusion XL with ControlNet guidance.\n    image, generation, image-to-image, controlnet, SDXL\n\n    Use cases:\n    - Modify existing images with precise control over composition and structure\n    - Apply specific styles or concepts to photographs or artwork with guided transformations\n    - Create variations of existing visual content while maintaining certain features\n    - Enhance image editing capabilities with AI-guided transformations",
      "namespace": "huggingface.image_to_image",
      "node_type": "huggingface.image_to_image.StableDiffusionXLControlNet",
      "layout": "default",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.stable_diffusion_xl"
          },
          "default": {},
          "title": "Model",
          "description": "The Stable Diffusion XL model to use for generation."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt for image generation."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to guide what should not appear in the generated image."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for the random number generator.",
          "min": -1.0,
          "max": 1000000.0
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 25,
          "title": "Num Inference Steps",
          "description": "Number of inference steps.",
          "min": 1.0,
          "max": 100.0
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 7.0,
          "title": "Guidance Scale",
          "description": "Guidance scale for generation.",
          "min": 0.0,
          "max": 20.0
        },
        {
          "name": "width",
          "type": {
            "type": "int"
          },
          "default": 1024,
          "title": "Width",
          "description": "Width of the generated image.",
          "min": 64.0,
          "max": 2048.0
        },
        {
          "name": "height",
          "type": {
            "type": "int"
          },
          "default": 1024,
          "title": "Height",
          "description": "Height of the generated image",
          "min": 64.0,
          "max": 2048.0
        },
        {
          "name": "scheduler",
          "type": {
            "type": "enum",
            "values": [
              "DPMSolverSDEScheduler",
              "EulerDiscreteScheduler",
              "LMSDiscreteScheduler",
              "DDIMScheduler",
              "DDPMScheduler",
              "HeunDiscreteScheduler",
              "DPMSolverMultistepScheduler",
              "DEISMultistepScheduler",
              "PNDMScheduler",
              "EulerAncestralDiscreteScheduler",
              "UniPCMultistepScheduler",
              "KDPM2DiscreteScheduler",
              "DPMSolverSinglestepScheduler",
              "KDPM2AncestralDiscreteScheduler"
            ],
            "type_name": "nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionScheduler"
          },
          "default": "EulerDiscreteScheduler",
          "title": "Scheduler",
          "description": "The scheduler to use for the diffusion process."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "hf.lora_sdxl_config"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRA models to use for image processing"
        },
        {
          "name": "lora_scale",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Lora Scale",
          "description": "Strength of the LoRAs",
          "min": 0.0,
          "max": 1.0
        },
        {
          "name": "ip_adapter_model",
          "type": {
            "type": "hf.ip_adapter"
          },
          "default": {},
          "title": "Ip Adapter Model",
          "description": "The IP adapter model to use for image processing"
        },
        {
          "name": "ip_adapter_image",
          "type": {
            "type": "image"
          },
          "default": {},
          "title": "Ip Adapter Image",
          "description": "When provided the image will be fed into the IP adapter"
        },
        {
          "name": "ip_adapter_scale",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Ip Adapter Scale",
          "description": "Strength of the IP adapter image",
          "min": 0.0,
          "max": 1.0
        },
        {
          "name": "enable_tiling",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Tiling",
          "description": "Enable tiling for the VAE. This can reduce VRAM usage."
        },
        {
          "name": "enable_cpu_offload",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Cpu Offload",
          "description": "Enable CPU offload for the pipeline. This can reduce VRAM usage."
        },
        {
          "name": "init_image",
          "type": {
            "type": "image"
          },
          "default": {},
          "title": "Init Image",
          "description": "The initial image for Image-to-Image generation."
        },
        {
          "name": "strength",
          "type": {
            "type": "float"
          },
          "default": 0.8,
          "title": "Strength",
          "description": "Strength for Image-to-Image generation. Higher values allow for more deviation from the original image.",
          "min": 0.0,
          "max": 1.0
        },
        {
          "name": "controlnet",
          "type": {
            "type": "hf.controlnet"
          },
          "default": {},
          "title": "Controlnet",
          "description": "The ControlNet model to use for guidance."
        },
        {
          "name": "control_image",
          "type": {
            "type": "image"
          },
          "default": {},
          "title": "Control Image",
          "description": "The control image to guide the transformation."
        },
        {
          "name": "controlnet_conditioning_scale",
          "type": {
            "type": "float"
          },
          "default": 1.0,
          "title": "Controlnet Conditioning Scale",
          "description": "The scale for ControlNet conditioning.",
          "min": 0.0,
          "max": 2.0
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "the_model_info": {},
      "recommended_models": [
        {
          "type": "hf.ip_adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "sdxl_models/ip-adapter_sdxl.bin"
        },
        {
          "type": "hf.ip_adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "sdxl_models/ip-adapter_sdxl_vit-h.bin"
        },
        {
          "type": "hf.ip_adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "sdxl_models/ip-adapter-plus_sdxl_vit-h.bin"
        },
        {
          "type": "hf.stable_diffusion_xl",
          "repo_id": "stabilityai/stable-diffusion-xl-base-1.0",
          "path": "sd_xl_base_1.0.safetensors"
        },
        {
          "type": "hf.stable_diffusion_xl",
          "repo_id": "stabilityai/stable-diffusion-xl-refiner-1.0",
          "path": "sd_xl_refiner_1.0.safetensors"
        },
        {
          "type": "hf.stable_diffusion_xl",
          "repo_id": "playgroundai/playground-v2.5-1024px-aesthetic",
          "path": "playground-v2.5-1024px-aesthetic.fp16.safetensors"
        },
        {
          "type": "hf.stable_diffusion_xl",
          "repo_id": "RunDiffusion/Juggernaut-XL-v9",
          "path": "Juggernaut-XL_v9_RunDiffusionPhoto_v2.safetensors"
        },
        {
          "type": "hf.stable_diffusion_xl",
          "repo_id": "dataautogpt3/ProteusV0.5",
          "path": "proteusV0.5.safetensors"
        },
        {
          "type": "hf.stable_diffusion_xl",
          "repo_id": "Lykon/dreamshaper-xl-lightning",
          "path": "DreamShaperXL_Lightning.safetensors"
        },
        {
          "type": "hf.stable_diffusion_xl",
          "repo_id": "Lykon/AAM_XL_AnimeMix",
          "path": "AAM_XL_Anime_Mix.safetensors"
        },
        {
          "type": "hf.stable_diffusion_xl",
          "repo_id": "stabilityai/sdxl-turbo",
          "path": "sd_xl_turbo_1.0_fp16.safetensors"
        },
        {
          "type": "hf.stable_diffusion_xl",
          "repo_id": "Lykon/dreamshaper-xl-v2-turbo",
          "path": "DreamShaperXL_Turbo_v2_1.safetensors"
        }
      ],
      "basic_fields": [
        "model",
        "prompt",
        "width",
        "height",
        "init_image",
        "strength",
        "controlnet",
        "control_image",
        "controlnet_conditioning_scale"
      ],
      "is_dynamic": false
    },
    {
      "title": "Stable Diffusion XL (Img2Img)",
      "description": "Transforms existing images based on text prompts using Stable Diffusion XL.\n    image, generation, image-to-image, SDXL\n\n    Use cases:\n    - Modifying existing images to fit a specific style or theme\n    - Enhancing or altering photographs\n    - Creating variations of existing artwork\n    - Applying text-guided edits to images",
      "namespace": "huggingface.image_to_image",
      "node_type": "huggingface.image_to_image.StableDiffusionXLImg2Img",
      "layout": "default",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.stable_diffusion_xl"
          },
          "default": {},
          "title": "Model",
          "description": "The Stable Diffusion XL model to use for generation."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt for image generation."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to guide what should not appear in the generated image."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for the random number generator.",
          "min": -1.0,
          "max": 1000000.0
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 25,
          "title": "Num Inference Steps",
          "description": "Number of inference steps.",
          "min": 1.0,
          "max": 100.0
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 7.0,
          "title": "Guidance Scale",
          "description": "Guidance scale for generation.",
          "min": 0.0,
          "max": 20.0
        },
        {
          "name": "width",
          "type": {
            "type": "int"
          },
          "default": 1024,
          "title": "Width",
          "description": "Width of the generated image.",
          "min": 64.0,
          "max": 2048.0
        },
        {
          "name": "height",
          "type": {
            "type": "int"
          },
          "default": 1024,
          "title": "Height",
          "description": "Height of the generated image",
          "min": 64.0,
          "max": 2048.0
        },
        {
          "name": "scheduler",
          "type": {
            "type": "enum",
            "values": [
              "DPMSolverSDEScheduler",
              "EulerDiscreteScheduler",
              "LMSDiscreteScheduler",
              "DDIMScheduler",
              "DDPMScheduler",
              "HeunDiscreteScheduler",
              "DPMSolverMultistepScheduler",
              "DEISMultistepScheduler",
              "PNDMScheduler",
              "EulerAncestralDiscreteScheduler",
              "UniPCMultistepScheduler",
              "KDPM2DiscreteScheduler",
              "DPMSolverSinglestepScheduler",
              "KDPM2AncestralDiscreteScheduler"
            ],
            "type_name": "nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionScheduler"
          },
          "default": "EulerDiscreteScheduler",
          "title": "Scheduler",
          "description": "The scheduler to use for the diffusion process."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "hf.lora_sdxl_config"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRA models to use for image processing"
        },
        {
          "name": "lora_scale",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Lora Scale",
          "description": "Strength of the LoRAs",
          "min": 0.0,
          "max": 1.0
        },
        {
          "name": "ip_adapter_model",
          "type": {
            "type": "hf.ip_adapter"
          },
          "default": {},
          "title": "Ip Adapter Model",
          "description": "The IP adapter model to use for image processing"
        },
        {
          "name": "ip_adapter_image",
          "type": {
            "type": "image"
          },
          "default": {},
          "title": "Ip Adapter Image",
          "description": "When provided the image will be fed into the IP adapter"
        },
        {
          "name": "ip_adapter_scale",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Ip Adapter Scale",
          "description": "Strength of the IP adapter image",
          "min": 0.0,
          "max": 1.0
        },
        {
          "name": "enable_tiling",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Tiling",
          "description": "Enable tiling for the VAE. This can reduce VRAM usage."
        },
        {
          "name": "enable_cpu_offload",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Cpu Offload",
          "description": "Enable CPU offload for the pipeline. This can reduce VRAM usage."
        },
        {
          "name": "init_image",
          "type": {
            "type": "image"
          },
          "default": {},
          "title": "Init Image",
          "description": "The initial image for Image-to-Image generation."
        },
        {
          "name": "strength",
          "type": {
            "type": "float"
          },
          "default": 0.8,
          "title": "Strength",
          "description": "Strength for Image-to-Image generation. Higher values allow for more deviation from the original image.",
          "min": 0.0,
          "max": 1.0
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "the_model_info": {},
      "recommended_models": [
        {
          "type": "hf.ip_adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "sdxl_models/ip-adapter_sdxl.bin"
        },
        {
          "type": "hf.ip_adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "sdxl_models/ip-adapter_sdxl_vit-h.bin"
        },
        {
          "type": "hf.ip_adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "sdxl_models/ip-adapter-plus_sdxl_vit-h.bin"
        },
        {
          "type": "hf.stable_diffusion_xl",
          "repo_id": "stabilityai/stable-diffusion-xl-base-1.0",
          "path": "sd_xl_base_1.0.safetensors"
        },
        {
          "type": "hf.stable_diffusion_xl",
          "repo_id": "stabilityai/stable-diffusion-xl-refiner-1.0",
          "path": "sd_xl_refiner_1.0.safetensors"
        },
        {
          "type": "hf.stable_diffusion_xl",
          "repo_id": "playgroundai/playground-v2.5-1024px-aesthetic",
          "path": "playground-v2.5-1024px-aesthetic.fp16.safetensors"
        },
        {
          "type": "hf.stable_diffusion_xl",
          "repo_id": "RunDiffusion/Juggernaut-XL-v9",
          "path": "Juggernaut-XL_v9_RunDiffusionPhoto_v2.safetensors"
        },
        {
          "type": "hf.stable_diffusion_xl",
          "repo_id": "dataautogpt3/ProteusV0.5",
          "path": "proteusV0.5.safetensors"
        },
        {
          "type": "hf.stable_diffusion_xl",
          "repo_id": "Lykon/dreamshaper-xl-lightning",
          "path": "DreamShaperXL_Lightning.safetensors"
        },
        {
          "type": "hf.stable_diffusion_xl",
          "repo_id": "Lykon/AAM_XL_AnimeMix",
          "path": "AAM_XL_Anime_Mix.safetensors"
        },
        {
          "type": "hf.stable_diffusion_xl",
          "repo_id": "stabilityai/sdxl-turbo",
          "path": "sd_xl_turbo_1.0_fp16.safetensors"
        },
        {
          "type": "hf.stable_diffusion_xl",
          "repo_id": "Lykon/dreamshaper-xl-v2-turbo",
          "path": "DreamShaperXL_Turbo_v2_1.safetensors"
        }
      ],
      "basic_fields": [
        "model",
        "prompt",
        "width",
        "height",
        "init_image",
        "strength"
      ],
      "is_dynamic": false
    },
    {
      "title": "Stable Diffusion XL (Inpaint)",
      "description": "Performs inpainting on images using Stable Diffusion XL.\n    image, inpainting, SDXL\n\n    Use cases:\n    - Remove unwanted objects from images\n    - Fill in missing parts of images\n    - Modify specific areas of images while preserving the rest",
      "namespace": "huggingface.image_to_image",
      "node_type": "huggingface.image_to_image.StableDiffusionXLInpainting",
      "layout": "default",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.stable_diffusion_xl"
          },
          "default": {},
          "title": "Model",
          "description": "The Stable Diffusion XL model to use for generation."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt for image generation."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to guide what should not appear in the generated image."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for the random number generator.",
          "min": -1.0,
          "max": 1000000.0
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 25,
          "title": "Num Inference Steps",
          "description": "Number of inference steps.",
          "min": 1.0,
          "max": 100.0
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 7.0,
          "title": "Guidance Scale",
          "description": "Guidance scale for generation.",
          "min": 0.0,
          "max": 20.0
        },
        {
          "name": "width",
          "type": {
            "type": "int"
          },
          "default": 1024,
          "title": "Width",
          "description": "Width of the generated image.",
          "min": 64.0,
          "max": 2048.0
        },
        {
          "name": "height",
          "type": {
            "type": "int"
          },
          "default": 1024,
          "title": "Height",
          "description": "Height of the generated image",
          "min": 64.0,
          "max": 2048.0
        },
        {
          "name": "scheduler",
          "type": {
            "type": "enum",
            "values": [
              "DPMSolverSDEScheduler",
              "EulerDiscreteScheduler",
              "LMSDiscreteScheduler",
              "DDIMScheduler",
              "DDPMScheduler",
              "HeunDiscreteScheduler",
              "DPMSolverMultistepScheduler",
              "DEISMultistepScheduler",
              "PNDMScheduler",
              "EulerAncestralDiscreteScheduler",
              "UniPCMultistepScheduler",
              "KDPM2DiscreteScheduler",
              "DPMSolverSinglestepScheduler",
              "KDPM2AncestralDiscreteScheduler"
            ],
            "type_name": "nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionScheduler"
          },
          "default": "EulerDiscreteScheduler",
          "title": "Scheduler",
          "description": "The scheduler to use for the diffusion process."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "hf.lora_sdxl_config"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRA models to use for image processing"
        },
        {
          "name": "lora_scale",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Lora Scale",
          "description": "Strength of the LoRAs",
          "min": 0.0,
          "max": 1.0
        },
        {
          "name": "ip_adapter_model",
          "type": {
            "type": "hf.ip_adapter"
          },
          "default": {},
          "title": "Ip Adapter Model",
          "description": "The IP adapter model to use for image processing"
        },
        {
          "name": "ip_adapter_image",
          "type": {
            "type": "image"
          },
          "default": {},
          "title": "Ip Adapter Image",
          "description": "When provided the image will be fed into the IP adapter"
        },
        {
          "name": "ip_adapter_scale",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Ip Adapter Scale",
          "description": "Strength of the IP adapter image",
          "min": 0.0,
          "max": 1.0
        },
        {
          "name": "enable_tiling",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Tiling",
          "description": "Enable tiling for the VAE. This can reduce VRAM usage."
        },
        {
          "name": "enable_cpu_offload",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Cpu Offload",
          "description": "Enable CPU offload for the pipeline. This can reduce VRAM usage."
        },
        {
          "name": "image",
          "type": {
            "type": "image"
          },
          "default": {},
          "title": "Image",
          "description": "The initial image to be inpainted."
        },
        {
          "name": "mask_image",
          "type": {
            "type": "image"
          },
          "default": {},
          "title": "Mask Image",
          "description": "The mask image indicating areas to be inpainted."
        },
        {
          "name": "strength",
          "type": {
            "type": "float"
          },
          "default": 0.8,
          "title": "Strength",
          "description": "Strength for inpainting. Higher values allow for more deviation from the original image.",
          "min": 0.0,
          "max": 1.0
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "the_model_info": {},
      "recommended_models": [
        {
          "type": "hf.ip_adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "sdxl_models/ip-adapter_sdxl.bin"
        },
        {
          "type": "hf.ip_adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "sdxl_models/ip-adapter_sdxl_vit-h.bin"
        },
        {
          "type": "hf.ip_adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "sdxl_models/ip-adapter-plus_sdxl_vit-h.bin"
        },
        {
          "type": "hf.stable_diffusion_xl",
          "repo_id": "stabilityai/stable-diffusion-xl-base-1.0",
          "path": "sd_xl_base_1.0.safetensors"
        },
        {
          "type": "hf.stable_diffusion_xl",
          "repo_id": "stabilityai/stable-diffusion-xl-refiner-1.0",
          "path": "sd_xl_refiner_1.0.safetensors"
        },
        {
          "type": "hf.stable_diffusion_xl",
          "repo_id": "playgroundai/playground-v2.5-1024px-aesthetic",
          "path": "playground-v2.5-1024px-aesthetic.fp16.safetensors"
        },
        {
          "type": "hf.stable_diffusion_xl",
          "repo_id": "RunDiffusion/Juggernaut-XL-v9",
          "path": "Juggernaut-XL_v9_RunDiffusionPhoto_v2.safetensors"
        },
        {
          "type": "hf.stable_diffusion_xl",
          "repo_id": "dataautogpt3/ProteusV0.5",
          "path": "proteusV0.5.safetensors"
        },
        {
          "type": "hf.stable_diffusion_xl",
          "repo_id": "Lykon/dreamshaper-xl-lightning",
          "path": "DreamShaperXL_Lightning.safetensors"
        },
        {
          "type": "hf.stable_diffusion_xl",
          "repo_id": "Lykon/AAM_XL_AnimeMix",
          "path": "AAM_XL_Anime_Mix.safetensors"
        },
        {
          "type": "hf.stable_diffusion_xl",
          "repo_id": "stabilityai/sdxl-turbo",
          "path": "sd_xl_turbo_1.0_fp16.safetensors"
        },
        {
          "type": "hf.stable_diffusion_xl",
          "repo_id": "Lykon/dreamshaper-xl-v2-turbo",
          "path": "DreamShaperXL_Turbo_v2_1.safetensors"
        }
      ],
      "basic_fields": [
        "model",
        "prompt",
        "width",
        "height",
        "image",
        "mask_image",
        "strength"
      ],
      "is_dynamic": false
    },
    {
      "title": "Swin2SR",
      "description": "Performs image super-resolution using the Swin2SR model.\n    image, super-resolution, enhancement, huggingface\n\n    Use cases:\n    - Enhance low-resolution images\n    - Improve image quality for printing or display\n    - Upscale images for better detail",
      "namespace": "huggingface.image_to_image",
      "node_type": "huggingface.image_to_image.Swin2SR",
      "layout": "default",
      "properties": [
        {
          "name": "image",
          "type": {
            "type": "image"
          },
          "default": {},
          "title": "Input Image",
          "description": "The input image to transform"
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt to guide the image transformation (if applicable)"
        },
        {
          "name": "model",
          "type": {
            "type": "hf.image_to_image"
          },
          "default": {},
          "title": "Model ID on Huggingface",
          "description": "The model ID to use for image super-resolution"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "the_model_info": {},
      "recommended_models": [
        {
          "type": "hf.image_to_image",
          "repo_id": "caidas/swin2SR-classical-sr-x2-64",
          "allow_patterns": [
            "README.md",
            "*.safetensors",
            "*.json",
            "**/*.json"
          ]
        },
        {
          "type": "hf.image_to_image",
          "repo_id": "caidas/swin2SR-classical-sr-x4-48",
          "allow_patterns": [
            "README.md",
            "*.safetensors",
            "*.json",
            "**/*.json"
          ]
        },
        {
          "type": "hf.image_to_image",
          "repo_id": "caidas/swin2SR-lightweight-sr-x2-64",
          "allow_patterns": [
            "README.md",
            "*.bin",
            "*.json",
            "**/*.json"
          ]
        },
        {
          "type": "hf.image_to_image",
          "repo_id": "caidas/swin2SR-realworld-sr-x4-64-bsrgan-psnr",
          "allow_patterns": [
            "README.md",
            "*.bin",
            "*.json",
            "**/*.json"
          ]
        }
      ],
      "basic_fields": [
        "image",
        "prompt",
        "model"
      ],
      "is_dynamic": false
    },
    {
      "title": "Sentence Similarity",
      "description": "Compares the similarity between two sentences.\n    text, sentence similarity, embeddings, natural language processing\n\n    Use cases:\n    - Duplicate detection in text data\n    - Semantic search\n    - Sentiment analysis",
      "namespace": "huggingface.sentence_similarity",
      "node_type": "huggingface.sentence_similarity.SentenceSimilarity",
      "layout": "default",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.sentence_similarity"
          },
          "default": {},
          "title": "Model ID on Huggingface",
          "description": "The model ID to use for sentence similarity"
        },
        {
          "name": "inputs",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Input Text",
          "description": "The text to compare"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "np_array"
          },
          "name": "output"
        }
      ],
      "the_model_info": {},
      "recommended_models": [
        {
          "type": "hf.sentence_similarity",
          "repo_id": "sentence-transformers/all-mpnet-base-v2",
          "allow_patterns": [
            "*.safetensors",
            "*.txt",
            "*,json"
          ]
        },
        {
          "type": "hf.sentence_similarity",
          "repo_id": "sentence-transformers/all-MiniLM-L6-v2",
          "allow_patterns": [
            "*.safetensors",
            "*.txt",
            "*,json"
          ]
        },
        {
          "type": "hf.sentence_similarity",
          "repo_id": "BAAI/bge-m3",
          "allow_patterns": [
            "*.safetensors",
            "*.txt",
            "*,json"
          ]
        },
        {
          "type": "hf.sentence_similarity",
          "repo_id": "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
          "allow_patterns": [
            "*.safetensors",
            "*.txt",
            "*,json"
          ]
        }
      ],
      "basic_fields": [
        "model",
        "inputs"
      ],
      "is_dynamic": false
    },
    {
      "title": "Text To Text",
      "description": "Performs text-to-text generation tasks.\n    text, generation, translation, question-answering, summarization, nlp, natural-language-processing\n\n    Use cases:\n    - Text translation\n    - Text summarization\n    - Paraphrasing\n    - Text style transfer\n\n    Usage:\n    Start with a command like Translate, Summarize, or Q (for question)\n    Follow with the text you want to translate, summarize, or answer a question about.\n    Examples:\n    - Translate to German: Hello\n    - Summarize: The quick brown fox jumps over the lazy dog.\n    - Q: Who ate the cookie? followed by the text of the cookie monster.",
      "namespace": "huggingface.text_to_text",
      "node_type": "huggingface.text_to_text.TextToText",
      "layout": "default",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.text2text_generation"
          },
          "default": {},
          "title": "Model ID on Huggingface",
          "description": "The model ID to use for the text-to-text generation"
        },
        {
          "name": "text",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Input Text",
          "description": "The input text for the text-to-text task"
        },
        {
          "name": "max_length",
          "type": {
            "type": "int"
          },
          "default": 50,
          "title": "Max Length",
          "description": "The maximum length of the generated text"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "str"
          },
          "name": "output"
        }
      ],
      "the_model_info": {},
      "recommended_models": [
        {
          "type": "hf.text2text_generation",
          "repo_id": "google/flan-t5-small",
          "allow_patterns": [
            "*.json",
            "*.txt",
            "*.safetensors"
          ]
        },
        {
          "type": "hf.text2text_generation",
          "repo_id": "google/flan-t5-base",
          "allow_patterns": [
            "*.json",
            "*.txt",
            "*.safetensors"
          ]
        },
        {
          "type": "hf.text2text_generation",
          "repo_id": "google/flan-t5-large",
          "allow_patterns": [
            "*.json",
            "*.txt",
            "*.safetensors"
          ]
        },
        {
          "type": "hf.text2text_generation",
          "repo_id": "gokaygokay/Flux-Prompt-Enhance",
          "allow_patterns": [
            "*.json",
            "*.txt",
            "*.safetensors"
          ]
        }
      ],
      "basic_fields": [
        "model",
        "text"
      ],
      "is_dynamic": false
    },
    {
      "title": "Feature Extraction",
      "description": "Extracts features from text using pre-trained models.\n    text, feature extraction, embeddings, natural language processing\n\n    Use cases:\n    - Text similarity comparison\n    - Clustering text documents\n    - Input for machine learning models\n    - Semantic search applications",
      "namespace": "huggingface.feature_extraction",
      "node_type": "huggingface.feature_extraction.FeatureExtraction",
      "layout": "default",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.feature_extraction"
          },
          "default": {},
          "title": "Model ID on Huggingface",
          "description": "The model ID to use for feature extraction"
        },
        {
          "name": "inputs",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Input Text",
          "description": "The text to extract features from"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "np_array"
          },
          "name": "output"
        }
      ],
      "the_model_info": {},
      "recommended_models": [
        {
          "type": "hf.feature_extraction",
          "repo_id": "mixedbread-ai/mxbai-embed-large-v1",
          "allow_patterns": [
            "*.safetensors",
            "*.txt",
            "*,json"
          ]
        },
        {
          "type": "hf.feature_extraction",
          "repo_id": "BAAI/bge-base-en-v1.5",
          "allow_patterns": [
            "*.safetensors",
            "*.txt",
            "*,json"
          ]
        },
        {
          "type": "hf.feature_extraction",
          "repo_id": "BAAI/bge-large-en-v1.5",
          "allow_patterns": [
            "*.safetensors",
            "*.txt",
            "*,json"
          ]
        }
      ],
      "basic_fields": [
        "model",
        "inputs"
      ],
      "is_dynamic": false
    },
    {
      "title": "Depth Estimation",
      "description": "Estimates depth from a single image.\n    image, depth estimation, 3D, huggingface\n\n    Use cases:\n    - Generate depth maps for 3D modeling\n    - Assist in augmented reality applications\n    - Enhance computer vision systems for robotics\n    - Improve scene understanding in autonomous vehicles",
      "namespace": "huggingface.depth_estimation",
      "node_type": "huggingface.depth_estimation.DepthEstimation",
      "layout": "default",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.depth_estimation"
          },
          "default": {
            "repo_id": "LiheYoung/depth-anything-base-hf"
          },
          "title": "Model ID on Huggingface",
          "description": "The model ID to use for depth estimation"
        },
        {
          "name": "image",
          "type": {
            "type": "image"
          },
          "default": {},
          "title": "Image",
          "description": "The input image for depth estimation"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "the_model_info": {},
      "recommended_models": [
        {
          "type": "hf.depth_estimation",
          "repo_id": "depth-anything/Depth-Anything-V2-Small-hf"
        },
        {
          "type": "hf.depth_estimation",
          "repo_id": "depth-anything/Depth-Anything-V2-Base-hf"
        },
        {
          "type": "hf.depth_estimation",
          "repo_id": "depth-anything/Depth-Anything-V2-Large-hf"
        },
        {
          "type": "hf.depth_estimation",
          "repo_id": "Intel/dpt-large",
          "allow_patterns": [
            "README.md",
            "*.safetensors",
            "*.json",
            "**/*.json",
            "txt"
          ]
        }
      ],
      "basic_fields": [
        "model",
        "image"
      ],
      "is_dynamic": false
    },
    {
      "title": "Bark",
      "description": "Bark is a text-to-audio model created by Suno. Bark can generate highly realistic, multilingual speech as well as other audio - including music, background noise and simple sound effects. The model can also produce nonverbal communications like laughing, sighing and crying.\n    tts, audio, speech, huggingface\n\n    Use cases:\n    - Create voice content for apps and websites\n    - Develop voice assistants with natural-sounding speech\n    - Generate automated announcements for public spaces",
      "namespace": "huggingface.text_to_speech",
      "node_type": "huggingface.text_to_speech.Bark",
      "layout": "default",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.text_to_speech"
          },
          "default": {},
          "title": "Model ID on Huggingface",
          "description": "The model ID to use for the image generation"
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Inputs",
          "description": "The input text to the model"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "the_model_info": {},
      "recommended_models": [
        {
          "type": "hf.text_to_speech",
          "repo_id": "suno/bark",
          "allow_patterns": [
            "*.bin",
            "*.json",
            "*.txt"
          ]
        },
        {
          "type": "hf.text_to_speech",
          "repo_id": "suno/bark-small",
          "allow_patterns": [
            "*.bin",
            "*.json",
            "*.txt"
          ]
        }
      ],
      "basic_fields": [
        "model",
        "prompt"
      ],
      "is_dynamic": false
    },
    {
      "title": "Text To Speech",
      "description": "A generic Text-to-Speech node that can work with various Hugging Face TTS models.\n    tts, audio, speech, huggingface, speak, voice\n\n    Use cases:\n    - Generate speech from text for various applications\n    - Create voice content for apps, websites, or virtual assistants\n    - Produce audio narrations for videos, presentations, or e-learning content",
      "namespace": "huggingface.text_to_speech",
      "node_type": "huggingface.text_to_speech.TextToSpeech",
      "layout": "default",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.text_to_speech"
          },
          "default": {},
          "title": "Model ID on Huggingface",
          "description": "The model ID to use for text-to-speech generation"
        },
        {
          "name": "text",
          "type": {
            "type": "str"
          },
          "default": "Hello, this is a test of the text-to-speech system.",
          "title": "Input Text",
          "description": "The text to convert to speech"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "the_model_info": {},
      "recommended_models": [
        {
          "type": "hf.text_to_speech",
          "repo_id": "facebook/mms-tts-eng",
          "allow_patterns": [
            "*.bin",
            "*.json",
            "*.txt"
          ]
        },
        {
          "type": "hf.text_to_speech",
          "repo_id": "facebook/mms-tts-kor",
          "allow_patterns": [
            "*.bin",
            "*.json",
            "*.txt"
          ]
        },
        {
          "type": "hf.text_to_speech",
          "repo_id": "facebook/mms-tts-fra",
          "allow_patterns": [
            "*.bin",
            "*.json",
            "*.txt"
          ]
        },
        {
          "type": "hf.text_to_speech",
          "repo_id": "facebook/mms-tts-deu",
          "allow_patterns": [
            "*.bin",
            "*.json",
            "*.txt"
          ]
        }
      ],
      "basic_fields": [
        "model",
        "text"
      ],
      "is_dynamic": false
    },
    {
      "title": "Text Classifier",
      "description": "Classifies text into predefined categories using a Hugging Face model.\n    text, classification, zero-shot, natural language processing",
      "namespace": "huggingface.text_classification",
      "node_type": "huggingface.text_classification.TextClassifier",
      "layout": "default",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.text_classification"
          },
          "default": {},
          "title": "Model ID on Huggingface",
          "description": "The model ID to use for the classification"
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Inputs",
          "description": "The input text to the model"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "float"
              }
            ]
          },
          "name": "output"
        }
      ],
      "the_model_info": {},
      "recommended_models": [
        {
          "type": "hf.text_classification",
          "repo_id": "cardiffnlp/twitter-roberta-base-sentiment-latest",
          "allow_patterns": [
            "*.json",
            "*.txt",
            "*.bin"
          ]
        },
        {
          "type": "hf.text_classification",
          "repo_id": "michellejieli/emotion_text_classifier",
          "allow_patterns": [
            "*.json",
            "*.txt",
            "*.bin"
          ]
        }
      ],
      "basic_fields": [
        "model",
        "prompt"
      ],
      "is_dynamic": false
    },
    {
      "title": "Zero Shot Text Classifier",
      "description": "Performs zero-shot classification on text.\n    text, classification, zero-shot, natural language processing\n\n    Use cases:\n    - Classify text into custom categories without training\n    - Topic detection in documents\n    - Sentiment analysis with custom sentiment labels\n    - Intent classification in conversational AI",
      "namespace": "huggingface.text_classification",
      "node_type": "huggingface.text_classification.ZeroShotTextClassifier",
      "layout": "default",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.zero_shot_classification"
          },
          "default": {},
          "title": "Model ID on Huggingface",
          "description": "The model ID to use for zero-shot classification"
        },
        {
          "name": "inputs",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Input Text",
          "description": "The text to classify"
        },
        {
          "name": "candidate_labels",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Candidate Labels",
          "description": "Comma-separated list of candidate labels for classification"
        },
        {
          "name": "multi_label",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Multi-label Classification",
          "description": "Whether to perform multi-label classification"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "float"
              }
            ]
          },
          "name": "output"
        }
      ],
      "the_model_info": {},
      "recommended_models": [
        {
          "type": "hf.zero_shot_classification",
          "repo_id": "facebook/bart-large-mnli",
          "allow_patterns": [
            "*.json",
            "*.txt",
            "*.safetensors"
          ]
        },
        {
          "type": "hf.zero_shot_classification",
          "repo_id": "MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli",
          "allow_patterns": [
            "*.json",
            "*.txt",
            "*.safetensors"
          ]
        },
        {
          "type": "hf.zero_shot_classification",
          "repo_id": "MoritzLaurer/mDeBERTa-v3-base-mnli-xnli",
          "allow_patterns": [
            "*.json",
            "*.txt",
            "*.safetensors"
          ]
        },
        {
          "type": "hf.zero_shot_classification",
          "repo_id": "tasksource/ModernBERT-base-nli",
          "allow_patterns": [
            "*.json",
            "*.txt",
            "*.safetensors"
          ]
        },
        {
          "type": "hf.zero_shot_classification",
          "repo_id": "cross-encoder/nli-deberta-v3-base",
          "allow_patterns": [
            "*.json",
            "*.txt",
            "*.safetensors"
          ]
        },
        {
          "type": "hf.zero_shot_classification",
          "repo_id": "microsoft/deberta-v2-xlarge-mnli",
          "allow_patterns": [
            "*.json",
            "*.txt",
            "*.safetensors"
          ]
        },
        {
          "type": "hf.zero_shot_classification",
          "repo_id": "roberta-large-mnli",
          "allow_patterns": [
            "*.json",
            "*.txt",
            "*.safetensors"
          ]
        }
      ],
      "basic_fields": [
        "model",
        "inputs",
        "candidate_labels",
        "multi_label"
      ],
      "is_dynamic": false
    },
    {
      "title": "Stable Diffusion Base",
      "description": "",
      "namespace": "huggingface.stable_diffusion_base",
      "node_type": "huggingface.stable_diffusion_base.StableDiffusionBase",
      "layout": "default",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.stable_diffusion"
          },
          "default": {},
          "title": "Model",
          "description": "The model to use for image generation."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt for image generation."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "(blurry, low quality, deformed, mutated, bad anatomy, extra limbs, bad proportions, text, watermark, grainy, pixelated, disfigured face, missing fingers, cropped image, bad lighting",
          "title": "Negative Prompt",
          "description": "The negative prompt to guide what should not appear in the generated image."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for the random number generator. Use -1 for a random seed.",
          "min": -1.0,
          "max": 4294967295.0
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 25,
          "title": "Num Inference Steps",
          "description": "Number of denoising steps.",
          "min": 1.0,
          "max": 100.0
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 7.5,
          "title": "Guidance Scale",
          "description": "Guidance scale for generation.",
          "min": 1.0,
          "max": 20.0
        },
        {
          "name": "scheduler",
          "type": {
            "type": "enum",
            "values": [
              "DPMSolverSDEScheduler",
              "EulerDiscreteScheduler",
              "LMSDiscreteScheduler",
              "DDIMScheduler",
              "DDPMScheduler",
              "HeunDiscreteScheduler",
              "DPMSolverMultistepScheduler",
              "DEISMultistepScheduler",
              "PNDMScheduler",
              "EulerAncestralDiscreteScheduler",
              "UniPCMultistepScheduler",
              "KDPM2DiscreteScheduler",
              "DPMSolverSinglestepScheduler",
              "KDPM2AncestralDiscreteScheduler"
            ],
            "type_name": "nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionScheduler"
          },
          "default": "EulerDiscreteScheduler",
          "title": "Scheduler",
          "description": "The scheduler to use for the diffusion process."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "hf.lora_sd_config"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRA models to use for image processing"
        },
        {
          "name": "ip_adapter_model",
          "type": {
            "type": "hf.ip_adapter"
          },
          "default": {},
          "title": "Ip Adapter Model",
          "description": "The IP adapter model to use for image processing"
        },
        {
          "name": "ip_adapter_image",
          "type": {
            "type": "image"
          },
          "default": {},
          "title": "Ip Adapter Image",
          "description": "When provided the image will be fed into the IP adapter"
        },
        {
          "name": "ip_adapter_scale",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Ip Adapter Scale",
          "description": "The strength of the IP adapter",
          "min": 0.0,
          "max": 1.0
        },
        {
          "name": "detail_level",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Detail Level",
          "description": "Level of detail for the hi-res pass. 0.0 is low detail, 1.0 is high detail.",
          "min": 0.0,
          "max": 1.0
        },
        {
          "name": "enable_tiling",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Tiling",
          "description": "Enable tiling for the VAE. This can reduce VRAM usage."
        },
        {
          "name": "enable_cpu_offload",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Cpu Offload",
          "description": "Enable CPU offload for the pipeline. This can reduce VRAM usage."
        },
        {
          "name": "upscaler",
          "type": {
            "type": "enum",
            "values": [
              "None",
              "Latent",
              "Bicubic"
            ],
            "type_name": "nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionUpscaler"
          },
          "default": "None",
          "title": "Upscaler",
          "description": "The upscaler to use for 2-pass generation."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "the_model_info": {},
      "recommended_models": [
        {
          "type": "hf.ip_adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "models/ip-adapter_sd15.bin"
        },
        {
          "type": "hf.ip_adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "models/ip-adapter_sd15_light.bin"
        },
        {
          "type": "hf.ip_adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "models/ip-adapter_sd15_vit-G.bin"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "SG161222/Realistic_Vision_V5.1_noVAE",
          "path": "Realistic_Vision_V5.1_fp16-no-ema.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "digiplay/majicMIX_realistic_v7",
          "path": "majicmixRealistic_v7.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "philz1337x/epicrealism",
          "path": "epicrealism_naturalSinRC1VAE.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "Lykon/DreamShaper",
          "path": "DreamShaper_5_beta2_noVae_half_pruned.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "Lykon/DreamShaper",
          "path": "DreamShaper_4BakedVae_fp16.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "XpucT/Deliberate",
          "path": "Deliberate_v6.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "XpucT/Deliberate",
          "path": "Deliberate_v6-inpainting.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "Lykon/AbsoluteReality",
          "path": "AbsoluteReality_1.8.1_pruned.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "Lykon/AbsoluteReality",
          "path": "AbsoluteReality_1.8.1_INPAINTING.inpainting.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "gsdf/Counterfeit-V2.5",
          "path": "Counterfeit-V2.5_fp16.safetensors"
        },
        {
          "type": "hf.ip_adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "models/ip-adapter_sd15.bin"
        },
        {
          "type": "hf.ip_adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "models/ip-adapter_sd15_light.bin"
        },
        {
          "type": "hf.ip_adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "models/ip-adapter_sd15_vit-G.bin"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "stabilityai/sd-x2-latent-upscaler",
          "allow_patterns": [
            "README.md",
            "**/*.safetensors",
            "**/*.json",
            "**/*.txt",
            "*.json"
          ]
        }
      ],
      "basic_fields": [
        "model",
        "prompt"
      ],
      "is_dynamic": false
    },
    {
      "title": "Stable Diffusion XLBase",
      "description": "",
      "namespace": "huggingface.stable_diffusion_base",
      "node_type": "huggingface.stable_diffusion_base.StableDiffusionXLBase",
      "layout": "default",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.stable_diffusion_xl"
          },
          "default": {},
          "title": "Model",
          "description": "The Stable Diffusion XL model to use for generation."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt for image generation."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to guide what should not appear in the generated image."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for the random number generator.",
          "min": -1.0,
          "max": 1000000.0
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 25,
          "title": "Num Inference Steps",
          "description": "Number of inference steps.",
          "min": 1.0,
          "max": 100.0
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 7.0,
          "title": "Guidance Scale",
          "description": "Guidance scale for generation.",
          "min": 0.0,
          "max": 20.0
        },
        {
          "name": "width",
          "type": {
            "type": "int"
          },
          "default": 1024,
          "title": "Width",
          "description": "Width of the generated image.",
          "min": 64.0,
          "max": 2048.0
        },
        {
          "name": "height",
          "type": {
            "type": "int"
          },
          "default": 1024,
          "title": "Height",
          "description": "Height of the generated image",
          "min": 64.0,
          "max": 2048.0
        },
        {
          "name": "scheduler",
          "type": {
            "type": "enum",
            "values": [
              "DPMSolverSDEScheduler",
              "EulerDiscreteScheduler",
              "LMSDiscreteScheduler",
              "DDIMScheduler",
              "DDPMScheduler",
              "HeunDiscreteScheduler",
              "DPMSolverMultistepScheduler",
              "DEISMultistepScheduler",
              "PNDMScheduler",
              "EulerAncestralDiscreteScheduler",
              "UniPCMultistepScheduler",
              "KDPM2DiscreteScheduler",
              "DPMSolverSinglestepScheduler",
              "KDPM2AncestralDiscreteScheduler"
            ],
            "type_name": "nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionScheduler"
          },
          "default": "EulerDiscreteScheduler",
          "title": "Scheduler",
          "description": "The scheduler to use for the diffusion process."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "hf.lora_sdxl_config"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRA models to use for image processing"
        },
        {
          "name": "lora_scale",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Lora Scale",
          "description": "Strength of the LoRAs",
          "min": 0.0,
          "max": 1.0
        },
        {
          "name": "ip_adapter_model",
          "type": {
            "type": "hf.ip_adapter"
          },
          "default": {},
          "title": "Ip Adapter Model",
          "description": "The IP adapter model to use for image processing"
        },
        {
          "name": "ip_adapter_image",
          "type": {
            "type": "image"
          },
          "default": {},
          "title": "Ip Adapter Image",
          "description": "When provided the image will be fed into the IP adapter"
        },
        {
          "name": "ip_adapter_scale",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Ip Adapter Scale",
          "description": "Strength of the IP adapter image",
          "min": 0.0,
          "max": 1.0
        },
        {
          "name": "enable_tiling",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Tiling",
          "description": "Enable tiling for the VAE. This can reduce VRAM usage."
        },
        {
          "name": "enable_cpu_offload",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Cpu Offload",
          "description": "Enable CPU offload for the pipeline. This can reduce VRAM usage."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "the_model_info": {},
      "recommended_models": [
        {
          "type": "hf.ip_adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "sdxl_models/ip-adapter_sdxl.bin"
        },
        {
          "type": "hf.ip_adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "sdxl_models/ip-adapter_sdxl_vit-h.bin"
        },
        {
          "type": "hf.ip_adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "sdxl_models/ip-adapter-plus_sdxl_vit-h.bin"
        },
        {
          "type": "hf.stable_diffusion_xl",
          "repo_id": "stabilityai/stable-diffusion-xl-base-1.0",
          "path": "sd_xl_base_1.0.safetensors"
        },
        {
          "type": "hf.stable_diffusion_xl",
          "repo_id": "stabilityai/stable-diffusion-xl-refiner-1.0",
          "path": "sd_xl_refiner_1.0.safetensors"
        },
        {
          "type": "hf.stable_diffusion_xl",
          "repo_id": "playgroundai/playground-v2.5-1024px-aesthetic",
          "path": "playground-v2.5-1024px-aesthetic.fp16.safetensors"
        },
        {
          "type": "hf.stable_diffusion_xl",
          "repo_id": "RunDiffusion/Juggernaut-XL-v9",
          "path": "Juggernaut-XL_v9_RunDiffusionPhoto_v2.safetensors"
        },
        {
          "type": "hf.stable_diffusion_xl",
          "repo_id": "dataautogpt3/ProteusV0.5",
          "path": "proteusV0.5.safetensors"
        },
        {
          "type": "hf.stable_diffusion_xl",
          "repo_id": "Lykon/dreamshaper-xl-lightning",
          "path": "DreamShaperXL_Lightning.safetensors"
        },
        {
          "type": "hf.stable_diffusion_xl",
          "repo_id": "Lykon/AAM_XL_AnimeMix",
          "path": "AAM_XL_Anime_Mix.safetensors"
        },
        {
          "type": "hf.stable_diffusion_xl",
          "repo_id": "stabilityai/sdxl-turbo",
          "path": "sd_xl_turbo_1.0_fp16.safetensors"
        },
        {
          "type": "hf.stable_diffusion_xl",
          "repo_id": "Lykon/dreamshaper-xl-v2-turbo",
          "path": "DreamShaperXL_Turbo_v2_1.safetensors"
        }
      ],
      "basic_fields": [
        "model",
        "prompt",
        "width",
        "height"
      ],
      "is_dynamic": false
    },
    {
      "title": "Translation",
      "description": "Translates text from one language to another.\n    text, translation, natural language processing\n\n    Use cases:\n    - Multilingual content creation\n    - Cross-language communication\n    - Localization of applications and websites\n\n    Note: some models support more languages than others.",
      "namespace": "huggingface.translation",
      "node_type": "huggingface.translation.Translation",
      "layout": "default",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.translation"
          },
          "default": {},
          "title": "Model ID on HuggingFace",
          "description": "The model ID to use for translation"
        },
        {
          "name": "inputs",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Input Text",
          "description": "The text to translate"
        },
        {
          "name": "source_lang",
          "type": {
            "type": "enum",
            "values": [
              "ar",
              "bn",
              "bs",
              "zh",
              "hr",
              "cs",
              "da",
              "nl",
              "en",
              "fil",
              "fi",
              "fr",
              "de",
              "el",
              "he",
              "hi",
              "id",
              "it",
              "ja",
              "ko",
              "ms",
              "me",
              "no",
              "pl",
              "pt",
              "pa",
              "ru",
              "ro",
              "sr",
              "sk",
              "sl",
              "es",
              "sv",
              "th",
              "tr",
              "vi"
            ],
            "type_name": "nodetool.nodes.huggingface.translation.LanguageCode"
          },
          "default": "en",
          "title": "Source Language",
          "description": "The source language code (e.g., 'en' for English)"
        },
        {
          "name": "target_lang",
          "type": {
            "type": "enum",
            "values": [
              "ar",
              "bn",
              "bs",
              "zh",
              "hr",
              "cs",
              "da",
              "nl",
              "en",
              "fil",
              "fi",
              "fr",
              "de",
              "el",
              "he",
              "hi",
              "id",
              "it",
              "ja",
              "ko",
              "ms",
              "me",
              "no",
              "pl",
              "pt",
              "pa",
              "ru",
              "ro",
              "sr",
              "sk",
              "sl",
              "es",
              "sv",
              "th",
              "tr",
              "vi"
            ],
            "type_name": "nodetool.nodes.huggingface.translation.LanguageCode"
          },
          "default": "fr",
          "title": "Target Language",
          "description": "The target language code (e.g., 'fr' for French)"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "str"
          },
          "name": "output"
        }
      ],
      "the_model_info": {},
      "recommended_models": [
        {
          "type": "hf.translation",
          "repo_id": "google-t5/t5-base",
          "allow_patterns": [
            "*.json",
            "*.txt",
            "*.safetensors"
          ]
        },
        {
          "type": "hf.translation",
          "repo_id": "google-t5/t5-large",
          "allow_patterns": [
            "*.json",
            "*.txt",
            "*.safetensors"
          ]
        },
        {
          "type": "hf.translation",
          "repo_id": "google-t5/t5-small",
          "allow_patterns": [
            "*.json",
            "*.txt",
            "*.safetensors"
          ]
        }
      ],
      "basic_fields": [
        "model",
        "inputs",
        "source_lang",
        "target_lang"
      ],
      "is_dynamic": false
    },
    {
      "title": "Reranker",
      "description": "Reranks pairs of text based on their semantic similarity.\n    text, ranking, reranking, natural language processing\n\n    Use cases:\n    - Improve search results ranking\n    - Question-answer pair scoring\n    - Document relevance ranking",
      "namespace": "huggingface.ranking",
      "node_type": "huggingface.ranking.Reranker",
      "layout": "default",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.reranker"
          },
          "default": {},
          "title": "Model ID on Huggingface",
          "description": "The model ID to use for reranking"
        },
        {
          "name": "query",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Query Text",
          "description": "The query text to compare against candidates"
        },
        {
          "name": "candidates",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Candidate Texts",
          "description": "List of candidate texts to rank"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "float"
              }
            ]
          },
          "name": "output"
        }
      ],
      "the_model_info": {},
      "recommended_models": [
        {
          "type": "hf.reranker",
          "repo_id": "BAAI/bge-reranker-v2-m3",
          "allow_patterns": [
            "*.safetensors",
            "*.txt",
            "*.json"
          ]
        },
        {
          "type": "hf.reranker",
          "repo_id": "BAAI/bge-reranker-base",
          "allow_patterns": [
            "*.safetensors",
            "*.txt",
            "*.json"
          ]
        },
        {
          "type": "hf.reranker",
          "repo_id": "BAAI/bge-reranker-large",
          "allow_patterns": [
            "*.safetensors",
            "*.txt",
            "*.json"
          ]
        }
      ],
      "basic_fields": [
        "model",
        "query",
        "candidates"
      ],
      "is_dynamic": false
    },
    {
      "title": "Text Generation",
      "description": "Generates text based on a given prompt.\n    text, generation, natural language processing\n\n    Use cases:\n    - Creative writing assistance\n    - Automated content generation\n    - Chatbots and conversational AI\n    - Code generation and completion",
      "namespace": "huggingface.text_generation",
      "node_type": "huggingface.text_generation.TextGeneration",
      "layout": "default",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.text_generation"
          },
          "default": {},
          "title": "Model ID on Huggingface",
          "description": "The model ID to use for the text generation"
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The input text prompt for generation"
        },
        {
          "name": "max_new_tokens",
          "type": {
            "type": "int"
          },
          "default": 50,
          "title": "Max New Tokens",
          "description": "The maximum number of new tokens to generate"
        },
        {
          "name": "temperature",
          "type": {
            "type": "float"
          },
          "default": 1.0,
          "title": "Temperature",
          "description": "Controls randomness in generation. Lower values make it more deterministic.",
          "min": 0.0,
          "max": 2.0
        },
        {
          "name": "top_p",
          "type": {
            "type": "float"
          },
          "default": 1.0,
          "title": "Top P",
          "description": "Controls diversity of generated text. Lower values make it more focused.",
          "min": 0.0,
          "max": 1.0
        },
        {
          "name": "do_sample",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Do Sample",
          "description": "Whether to use sampling or greedy decoding"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "str"
          },
          "name": "output"
        }
      ],
      "the_model_info": {},
      "recommended_models": [
        {
          "type": "hf.text_generation",
          "repo_id": "gpt2",
          "allow_patterns": [
            "*.json",
            "*.txt",
            "*.safetensors"
          ]
        },
        {
          "type": "hf.text_generation",
          "repo_id": "distilgpt2",
          "allow_patterns": [
            "*.json",
            "*.txt",
            "*.safetensors"
          ]
        },
        {
          "type": "hf.text_generation",
          "repo_id": "Qwen/Qwen2-0.5B-Instruct",
          "allow_patterns": [
            "*.json",
            "*.txt",
            "*.safetensors"
          ]
        },
        {
          "type": "hf.text_generation",
          "repo_id": "bigcode/starcoder",
          "allow_patterns": [
            "*.json",
            "*.txt",
            "*.safetensors"
          ]
        }
      ],
      "basic_fields": [
        "model",
        "prompt"
      ],
      "is_dynamic": false
    },
    {
      "title": "Chunks To SRT",
      "description": "Convert audio chunks to SRT (SubRip Subtitle) format\n    subtitle, srt, whisper, transcription\n\n    **Use Cases:**\n    - Generate subtitles for videos\n    - Create closed captions from audio transcriptions\n    - Convert speech-to-text output to a standardized subtitle format\n\n    **Features:**\n    - Converts Whisper audio chunks to SRT format\n    - Supports customizable time offset\n    - Generates properly formatted SRT file content",
      "namespace": "huggingface.automatic_speech_recognition",
      "node_type": "huggingface.automatic_speech_recognition.ChunksToSRT",
      "layout": "default",
      "properties": [
        {
          "name": "chunks",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "audio_chunk"
              }
            ]
          },
          "default": [],
          "title": "Audio Chunks",
          "description": "List of audio chunks from Whisper transcription"
        },
        {
          "name": "time_offset",
          "type": {
            "type": "float"
          },
          "default": 0.0,
          "title": "Time Offset",
          "description": "Time offset in seconds to apply to all timestamps"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "str"
          },
          "name": "output"
        }
      ],
      "the_model_info": {},
      "recommended_models": [],
      "basic_fields": [
        "chunks",
        "time_offset"
      ],
      "is_dynamic": false
    },
    {
      "title": "Whisper",
      "description": "Convert speech to text\n    asr, automatic-speech-recognition, speech-to-text, translate, transcribe, audio, huggingface\n\n    **Use Cases:**\n    - Voice input for a chatbot\n    - Transcribe or translate audio files\n    - Create subtitles for videos\n\n    **Features:**\n    - Multilingual speech recognition\n    - Speech translation\n    - Language identification\n\n    **Note**\n    - Language selection is sorted by word error rate in the FLEURS benchmark\n    - There are many variants of Whisper that are optimized for different use cases.\n\n    **Links:**\n    - https://github.com/openai/whisper\n    - https://platform.openai.com/docs/guides/speech-to-text/supported-languages",
      "namespace": "huggingface.automatic_speech_recognition",
      "node_type": "huggingface.automatic_speech_recognition.Whisper",
      "layout": "default",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.automatic_speech_recognition"
          },
          "default": {},
          "title": "Model ID on Huggingface",
          "description": "The model ID to use for the speech recognition."
        },
        {
          "name": "audio",
          "type": {
            "type": "audio"
          },
          "default": {},
          "title": "Audio Input",
          "description": "The input audio to transcribe."
        },
        {
          "name": "task",
          "type": {
            "type": "enum",
            "values": [
              "transcribe",
              "translate"
            ],
            "type_name": "nodetool.nodes.huggingface.automatic_speech_recognition.Task"
          },
          "default": "transcribe",
          "title": "Task",
          "description": "The task to perform: 'transcribe' for speech-to-text or 'translate' for speech translation."
        },
        {
          "name": "language",
          "type": {
            "type": "enum",
            "values": [
              "auto_detect",
              "spanish",
              "italian",
              "korean",
              "portuguese",
              "english",
              "japanese",
              "german",
              "russian",
              "dutch",
              "polish",
              "catalan",
              "french",
              "indonesian",
              "ukrainian",
              "turkish",
              "malay",
              "swedish",
              "mandarin",
              "finnish",
              "norwegian",
              "romanian",
              "thai",
              "vietnamese",
              "slovak",
              "arabic",
              "czech",
              "croatian",
              "greek",
              "serbian",
              "danish",
              "bulgarian",
              "hungarian",
              "filipino",
              "bosnian",
              "galician",
              "macedonian",
              "hindi",
              "estonian",
              "slovenian",
              "tamil",
              "latvian",
              "azerbaijani",
              "urdu",
              "lithuanian",
              "hebrew",
              "welsh",
              "persian",
              "icelandic",
              "kazakh",
              "afrikaans",
              "kannada",
              "marathi",
              "swahili",
              "telugu",
              "maori",
              "nepali",
              "armenian",
              "belarusian",
              "gujarati",
              "punjabi",
              "bengali"
            ],
            "type_name": "nodetool.nodes.huggingface.automatic_speech_recognition.WhisperLanguage"
          },
          "default": "auto_detect",
          "title": "Language",
          "description": "The language of the input audio. If not specified, the model will attempt to detect it automatically."
        },
        {
          "name": "timestamps",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "word",
              "sentence"
            ],
            "type_name": "nodetool.nodes.huggingface.automatic_speech_recognition.Timestamps"
          },
          "default": "none",
          "title": "Timestamps",
          "description": "The type of timestamps to return for the generated text."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "str"
          },
          "name": "text"
        },
        {
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "audio_chunk"
              }
            ]
          },
          "name": "chunks"
        }
      ],
      "the_model_info": {},
      "recommended_models": [
        {
          "type": "hf.automatic_speech_recognition",
          "repo_id": "openai/whisper-large-v3",
          "allow_patterns": [
            "model.safetensors",
            "*.json",
            "*.txt"
          ]
        },
        {
          "type": "hf.automatic_speech_recognition",
          "repo_id": "openai/whisper-large-v3-turbo",
          "allow_patterns": [
            "model.safetensors",
            "*.json",
            "*.txt"
          ]
        },
        {
          "type": "hf.automatic_speech_recognition",
          "repo_id": "openai/whisper-large-v2",
          "allow_patterns": [
            "model.safetensors",
            "*.json",
            "*.txt"
          ]
        },
        {
          "type": "hf.automatic_speech_recognition",
          "repo_id": "openai/whisper-medium",
          "allow_patterns": [
            "model.safetensors",
            "*.json",
            "*.txt"
          ]
        },
        {
          "type": "hf.automatic_speech_recognition",
          "repo_id": "openai/whisper-small",
          "allow_patterns": [
            "model.safetensors",
            "*.json",
            "*.txt"
          ]
        },
        {
          "type": "hf.automatic_speech_recognition",
          "repo_id": "Systran/faster-whisper-large-v3",
          "allow_patterns": [
            "model.bin",
            "*.json",
            "*.txt"
          ]
        }
      ],
      "basic_fields": [
        "model",
        "audio",
        "task"
      ],
      "is_dynamic": false
    },
    {
      "title": "Summarize",
      "description": "Summarizes text using a Hugging Face model.\n    text, summarization, AI, LLM",
      "namespace": "huggingface.summarization",
      "node_type": "huggingface.summarization.Summarize",
      "layout": "default",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.text_generation"
          },
          "default": {},
          "title": "Model ID on Huggingface",
          "description": "The model ID to use for the text generation"
        },
        {
          "name": "inputs",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Inputs",
          "description": "The input text to summarize"
        },
        {
          "name": "max_length",
          "type": {
            "type": "int"
          },
          "default": 100,
          "title": "Max Length",
          "description": "The maximum length of the generated text"
        },
        {
          "name": "do_sample",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Do Sample",
          "description": "Whether to sample from the model"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "str"
          },
          "name": "output"
        }
      ],
      "the_model_info": {},
      "recommended_models": [
        {
          "type": "hf.text_generation",
          "repo_id": "Falconsai/text_summarization",
          "allow_patterns": [
            "*.json",
            "*.txt",
            "*.safetensors"
          ]
        },
        {
          "type": "hf.text_generation",
          "repo_id": "Falconsai/medical_summarization",
          "allow_patterns": [
            "*.json",
            "*.txt",
            "*.safetensors"
          ]
        },
        {
          "type": "hf.text_generation",
          "repo_id": "imvladikon/het5_summarization",
          "allow_patterns": [
            "*.json",
            "*.txt",
            "*.safetensors"
          ]
        }
      ],
      "basic_fields": [
        "model",
        "inputs"
      ],
      "is_dynamic": false
    },
    {
      "title": "LoRA Selector",
      "description": "Selects up to 5 LoRA models to apply to a Stable Diffusion model.\n    lora, model customization, fine-tuning, SD\n\n    Use cases:\n    - Combining multiple LoRA models for unique image styles\n    - Fine-tuning Stable Diffusion models with specific attributes\n    - Experimenting with different LoRA combinations",
      "namespace": "huggingface.lora",
      "node_type": "huggingface.lora.LoRASelector",
      "layout": "default",
      "properties": [
        {
          "name": "lora1",
          "type": {
            "type": "hf.lora_sd"
          },
          "default": {},
          "title": "Lora1",
          "description": "First LoRA model"
        },
        {
          "name": "strength1",
          "type": {
            "type": "float"
          },
          "default": 1.0,
          "title": "Strength1",
          "description": "Strength for first LoRA",
          "min": 0.0,
          "max": 2.0
        },
        {
          "name": "lora2",
          "type": {
            "type": "hf.lora_sd"
          },
          "default": {},
          "title": "Lora2",
          "description": "Second LoRA model"
        },
        {
          "name": "strength2",
          "type": {
            "type": "float"
          },
          "default": 1.0,
          "title": "Strength2",
          "description": "Strength for second LoRA",
          "min": 0.0,
          "max": 2.0
        },
        {
          "name": "lora3",
          "type": {
            "type": "hf.lora_sd"
          },
          "default": {},
          "title": "Lora3",
          "description": "Third LoRA model"
        },
        {
          "name": "strength3",
          "type": {
            "type": "float"
          },
          "default": 1.0,
          "title": "Strength3",
          "description": "Strength for third LoRA",
          "min": 0.0,
          "max": 2.0
        },
        {
          "name": "lora4",
          "type": {
            "type": "hf.lora_sd"
          },
          "default": {},
          "title": "Lora4",
          "description": "Fourth LoRA model"
        },
        {
          "name": "strength4",
          "type": {
            "type": "float"
          },
          "default": 1.0,
          "title": "Strength4",
          "description": "Strength for fourth LoRA",
          "min": 0.0,
          "max": 2.0
        },
        {
          "name": "lora5",
          "type": {
            "type": "hf.lora_sd"
          },
          "default": {},
          "title": "Lora5",
          "description": "Fifth LoRA model"
        },
        {
          "name": "strength5",
          "type": {
            "type": "float"
          },
          "default": 1.0,
          "title": "Strength5",
          "description": "Strength for fifth LoRA",
          "min": 0.0,
          "max": 2.0
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "hf.lora_sd_config"
              }
            ]
          },
          "name": "output"
        }
      ],
      "the_model_info": {},
      "recommended_models": [
        {
          "type": "hf.lora_sd",
          "repo_id": "danbrown/loras",
          "path": "2d_sprite.safetensors"
        },
        {
          "type": "hf.lora_sd",
          "repo_id": "danbrown/loras",
          "path": "ghibli_scenery.safetensors"
        },
        {
          "type": "hf.lora_sd",
          "repo_id": "danbrown/loras",
          "path": "add_detail.safetensors"
        },
        {
          "type": "hf.lora_sd",
          "repo_id": "danbrown/loras",
          "path": "colorwater.safetensors"
        },
        {
          "type": "hf.lora_sd",
          "repo_id": "danbrown/loras",
          "path": "sxz_game_assets.safetensors"
        },
        {
          "type": "hf.lora_sd",
          "repo_id": "danbrown/loras",
          "path": "3Danaglyph.safetensors"
        },
        {
          "type": "hf.lora_sd",
          "repo_id": "danbrown/loras",
          "path": "akiratoriyama_style.safetensors"
        },
        {
          "type": "hf.lora_sd",
          "repo_id": "danbrown/loras",
          "path": "animeoutlineV4.safetensors"
        },
        {
          "type": "hf.lora_sd",
          "repo_id": "danbrown/loras",
          "path": "aqua_konosuba.safetensors"
        },
        {
          "type": "hf.lora_sd",
          "repo_id": "danbrown/loras",
          "path": "arakihirohiko_style.safetensors"
        },
        {
          "type": "hf.lora_sd",
          "repo_id": "danbrown/loras",
          "path": "arcane_style.safetensors"
        },
        {
          "type": "hf.lora_sd",
          "repo_id": "danbrown/loras",
          "path": "canetaazul.safetensors"
        },
        {
          "type": "hf.lora_sd",
          "repo_id": "danbrown/loras",
          "path": "cyberpunk_tarot.safetensors"
        },
        {
          "type": "hf.lora_sd",
          "repo_id": "danbrown/loras",
          "path": "discoelysium_style.safetensors"
        },
        {
          "type": "hf.lora_sd",
          "repo_id": "danbrown/loras",
          "path": "esdeath_akamegakill.safetensors"
        },
        {
          "type": "hf.lora_sd",
          "repo_id": "danbrown/loras",
          "path": "fire_vfx.safetensors"
        },
        {
          "type": "hf.lora_sd",
          "repo_id": "danbrown/loras",
          "path": "flamingeye.safetensors"
        },
        {
          "type": "hf.lora_sd",
          "repo_id": "danbrown/loras",
          "path": "funnycreatures.safetensors"
        },
        {
          "type": "hf.lora_sd",
          "repo_id": "danbrown/loras",
          "path": "gacha_splash.safetensors"
        },
        {
          "type": "hf.lora_sd",
          "repo_id": "danbrown/loras",
          "path": "gigachad.safetensors"
        },
        {
          "type": "hf.lora_sd",
          "repo_id": "danbrown/loras",
          "path": "gyokai_style.safetensors"
        },
        {
          "type": "hf.lora_sd",
          "repo_id": "danbrown/loras",
          "path": "harold.safetensors"
        },
        {
          "type": "hf.lora_sd",
          "repo_id": "danbrown/loras",
          "path": "hiderohoribes_style.safetensors"
        },
        {
          "type": "hf.lora_sd",
          "repo_id": "danbrown/loras",
          "path": "ilyakuvshinov_style.safetensors"
        },
        {
          "type": "hf.lora_sd",
          "repo_id": "danbrown/loras",
          "path": "jacksparrow.safetensors"
        },
        {
          "type": "hf.lora_sd",
          "repo_id": "danbrown/loras",
          "path": "jimlee_style.safetensors"
        },
        {
          "type": "hf.lora_sd",
          "repo_id": "danbrown/loras",
          "path": "komowataharuka_chibiart.safetensors"
        },
        {
          "type": "hf.lora_sd",
          "repo_id": "danbrown/loras",
          "path": "lightning_vfx.safetensors"
        },
        {
          "type": "hf.lora_sd",
          "repo_id": "danbrown/loras",
          "path": "lucy_cyberpunk.safetensors"
        },
        {
          "type": "hf.lora_sd",
          "repo_id": "danbrown/loras",
          "path": "luisap_pixelart.safetensors"
        },
        {
          "type": "hf.lora_sd",
          "repo_id": "danbrown/loras",
          "path": "mumei_kabaneri.safetensors"
        },
        {
          "type": "hf.lora_sd",
          "repo_id": "danbrown/loras",
          "path": "myheroacademia_style.safetensors"
        },
        {
          "type": "hf.lora_sd",
          "repo_id": "danbrown/loras",
          "path": "neoartcore.safetensors"
        },
        {
          "type": "hf.lora_sd",
          "repo_id": "danbrown/loras",
          "path": "ochakouraraka.safetensors"
        },
        {
          "type": "hf.lora_sd",
          "repo_id": "danbrown/loras",
          "path": "onepiece_style.safetensors"
        },
        {
          "type": "hf.lora_sd",
          "repo_id": "danbrown/loras",
          "path": "paimon_genshinimpact.safetensors"
        },
        {
          "type": "hf.lora_sd",
          "repo_id": "danbrown/loras",
          "path": "peanutscomics_style.safetensors"
        },
        {
          "type": "hf.lora_sd",
          "repo_id": "danbrown/loras",
          "path": "pepefrog.safetensors"
        },
        {
          "type": "hf.lora_sd",
          "repo_id": "danbrown/loras",
          "path": "persona5_portraits.safetensors"
        },
        {
          "type": "hf.lora_sd",
          "repo_id": "danbrown/loras",
          "path": "persona5_style.safetensors"
        },
        {
          "type": "hf.lora_sd",
          "repo_id": "danbrown/loras",
          "path": "pixhell.safetensors"
        },
        {
          "type": "hf.lora_sd",
          "repo_id": "danbrown/loras",
          "path": "princesszelda.safetensors"
        },
        {
          "type": "hf.lora_sd",
          "repo_id": "danbrown/loras",
          "path": "satoshiuruchihara_style.safetensors"
        },
        {
          "type": "hf.lora_sd",
          "repo_id": "danbrown/loras",
          "path": "shinobu_demonslayer.safetensors"
        },
        {
          "type": "hf.lora_sd",
          "repo_id": "danbrown/loras",
          "path": "sokolov_style.safetensors"
        },
        {
          "type": "hf.lora_sd",
          "repo_id": "danbrown/loras",
          "path": "standingbackgroundv1.safetensors"
        },
        {
          "type": "hf.lora_sd",
          "repo_id": "danbrown/loras",
          "path": "sun_shadow_style.safetensors"
        },
        {
          "type": "hf.lora_sd",
          "repo_id": "danbrown/loras",
          "path": "thickeranimelines.safetensors"
        },
        {
          "type": "hf.lora_sd",
          "repo_id": "danbrown/loras",
          "path": "threesidedview.safetensors"
        },
        {
          "type": "hf.lora_sd",
          "repo_id": "danbrown/loras",
          "path": "twitch_emotes.safetensors"
        },
        {
          "type": "hf.lora_sd",
          "repo_id": "danbrown/loras",
          "path": "water_vfx.safetensors"
        },
        {
          "type": "hf.lora_sd",
          "repo_id": "danbrown/loras",
          "path": "wlop_style.safetensors"
        },
        {
          "type": "hf.lora_sd",
          "repo_id": "danbrown/loras",
          "path": "zerotwo_darling.safetensors"
        }
      ],
      "basic_fields": [
        "lora1",
        "strength1",
        "lora2",
        "strength2",
        "lora3",
        "strength3",
        "lora4",
        "strength4",
        "lora5",
        "strength5"
      ],
      "is_dynamic": false
    },
    {
      "title": "LoRA XL Selector",
      "description": "Selects up to 5 LoRA models to apply to a Stable Diffusion XL model.\n    lora, model customization, fine-tuning, SDXL\n\n    Use cases:\n    - Combining multiple LoRA models for unique image styles\n    - Fine-tuning Stable Diffusion XL models with specific attributes\n    - Experimenting with different LoRA combinations",
      "namespace": "huggingface.lora",
      "node_type": "huggingface.lora.LoRASelectorXL",
      "layout": "default",
      "properties": [
        {
          "name": "lora1",
          "type": {
            "type": "hf.lora_sdxl"
          },
          "default": {},
          "title": "Lora1",
          "description": "First LoRA model"
        },
        {
          "name": "strength1",
          "type": {
            "type": "float"
          },
          "default": 1.0,
          "title": "Strength1",
          "description": "Strength for first LoRA",
          "min": 0.0,
          "max": 2.0
        },
        {
          "name": "lora2",
          "type": {
            "type": "hf.lora_sdxl"
          },
          "default": {},
          "title": "Lora2",
          "description": "Second LoRA model"
        },
        {
          "name": "strength2",
          "type": {
            "type": "float"
          },
          "default": 1.0,
          "title": "Strength2",
          "description": "Strength for second LoRA",
          "min": 0.0,
          "max": 2.0
        },
        {
          "name": "lora3",
          "type": {
            "type": "hf.lora_sdxl"
          },
          "default": {},
          "title": "Lora3",
          "description": "Third LoRA model"
        },
        {
          "name": "strength3",
          "type": {
            "type": "float"
          },
          "default": 1.0,
          "title": "Strength3",
          "description": "Strength for third LoRA",
          "min": 0.0,
          "max": 2.0
        },
        {
          "name": "lora4",
          "type": {
            "type": "hf.lora_sdxl"
          },
          "default": {},
          "title": "Lora4",
          "description": "Fourth LoRA model"
        },
        {
          "name": "strength4",
          "type": {
            "type": "float"
          },
          "default": 1.0,
          "title": "Strength4",
          "description": "Strength for fourth LoRA",
          "min": 0.0,
          "max": 2.0
        },
        {
          "name": "lora5",
          "type": {
            "type": "hf.lora_sdxl"
          },
          "default": {},
          "title": "Lora5",
          "description": "Fifth LoRA model"
        },
        {
          "name": "strength5",
          "type": {
            "type": "float"
          },
          "default": 1.0,
          "title": "Strength5",
          "description": "Strength for fifth LoRA",
          "min": 0.0,
          "max": 2.0
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "hf.lora_sdxl_config"
              }
            ]
          },
          "name": "output"
        }
      ],
      "the_model_info": {},
      "recommended_models": [
        {
          "type": "hf.lora_sdxl",
          "repo_id": "CiroN2022/toy-face",
          "path": "toy_face_sdxl.safetensors"
        },
        {
          "type": "hf.lora_sdxl",
          "repo_id": "nerijs/pixel-art-xl",
          "path": "pixel-art-xl.safetensors"
        },
        {
          "type": "hf.lora_sdxl",
          "repo_id": "goofyai/3d_render_style_xl",
          "path": "3d_render_style_xl.safetensors"
        },
        {
          "type": "hf.lora_sdxl",
          "repo_id": "artificialguybr/CuteCartoonRedmond-V2",
          "path": "CuteCartoonRedmond-CuteCartoon-CuteCartoonAF.safetensors"
        },
        {
          "type": "hf.lora_sdxl",
          "repo_id": "blink7630/graphic-novel-illustration",
          "path": "Graphic_Novel_Illustration-000007.safetensors"
        },
        {
          "type": "hf.lora_sdxl",
          "repo_id": "robert123231/coloringbookgenerator",
          "path": "ColoringBookRedmond-ColoringBook-ColoringBookAF.safetensors"
        },
        {
          "type": "hf.lora_sdxl",
          "repo_id": "Linaqruf/anime-detailer-xl-lora",
          "path": "anime-detailer-xl-lora.safetensors"
        }
      ],
      "basic_fields": [
        "lora1",
        "strength1",
        "lora2",
        "strength2",
        "lora3",
        "strength3",
        "lora4",
        "strength4",
        "lora5",
        "strength5"
      ],
      "is_dynamic": false
    },
    {
      "title": "Huggingface",
      "description": "",
      "namespace": "huggingface.huggingface_node",
      "node_type": "huggingface.huggingface_node.Huggingface",
      "layout": "default",
      "properties": [],
      "outputs": [
        {
          "type": {
            "type": "any"
          },
          "name": "output"
        }
      ],
      "the_model_info": {},
      "recommended_models": [],
      "basic_fields": [],
      "is_dynamic": false
    },
    {
      "title": "Object Detection",
      "description": "Detects and localizes objects in images.\n    image, object detection, bounding boxes, huggingface\n\n    Use cases:\n    - Identify and count objects in images\n    - Locate specific items in complex scenes\n    - Assist in autonomous vehicle vision systems\n    - Enhance security camera footage analysis",
      "namespace": "huggingface.object_detection",
      "node_type": "huggingface.object_detection.ObjectDetection",
      "layout": "default",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.object_detection"
          },
          "default": {
            "repo_id": "facebook/detr-resnet-50"
          },
          "title": "Model ID on Huggingface",
          "description": "The model ID to use for object detection"
        },
        {
          "name": "image",
          "type": {
            "type": "image"
          },
          "default": {},
          "title": "Inputs",
          "description": "The input image for object detection"
        },
        {
          "name": "threshold",
          "type": {
            "type": "float"
          },
          "default": 0.9,
          "title": "Confidence Threshold",
          "description": "Minimum confidence score for detected objects"
        },
        {
          "name": "top_k",
          "type": {
            "type": "int"
          },
          "default": 5,
          "title": "Top K",
          "description": "The number of top predictions to return"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "object_detection_result"
              }
            ]
          },
          "name": "output"
        }
      ],
      "the_model_info": {},
      "recommended_models": [
        {
          "type": "hf.object_detection",
          "repo_id": "facebook/detr-resnet-50",
          "allow_patterns": [
            "README.md",
            "*.bin",
            "*.json",
            "**/*.json"
          ]
        },
        {
          "type": "hf.object_detection",
          "repo_id": "facebook/detr-resnet-101",
          "allow_patterns": [
            "README.md",
            "*.safetensors",
            "*.json",
            "**/*.json"
          ]
        },
        {
          "type": "hf.object_detection",
          "repo_id": "hustvl/yolos-tiny",
          "allow_patterns": [
            "README.md",
            "*.safetensors",
            "*.json",
            "**/*.json"
          ]
        },
        {
          "type": "hf.object_detection",
          "repo_id": "hustvl/yolos-small",
          "allow_patterns": [
            "README.md",
            "*.safetensors",
            "*.json",
            "**/*.json"
          ]
        },
        {
          "type": "hf.object_detection",
          "repo_id": "microsoft/table-transformer-detection",
          "allow_patterns": [
            "README.md",
            "*.bin",
            "*.json",
            "**/*.json"
          ]
        },
        {
          "type": "hf.object_detection",
          "repo_id": "microsoft/table-transformer-structure-recognition-v1.1-all",
          "allow_patterns": [
            "README.md",
            "*.bin",
            "*.json",
            "**/*.json"
          ]
        },
        {
          "type": "hf.object_detection",
          "repo_id": "valentinafeve/yolos-fashionpedia",
          "allow_patterns": [
            "README.md",
            "*.bin",
            "*.json",
            "**/*.json"
          ]
        }
      ],
      "basic_fields": [
        "model",
        "image"
      ],
      "is_dynamic": false
    },
    {
      "title": "Visualize Object Detection",
      "description": "Visualizes object detection results on images.\n    image, object detection, bounding boxes, visualization, mask",
      "namespace": "huggingface.object_detection",
      "node_type": "huggingface.object_detection.VisualizeObjectDetection",
      "layout": "default",
      "properties": [
        {
          "name": "image",
          "type": {
            "type": "image"
          },
          "default": {},
          "title": "Image",
          "description": "The input image to visualize"
        },
        {
          "name": "objects",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "object_detection_result"
              }
            ]
          },
          "default": {},
          "title": "Detected Objects",
          "description": "The detected objects to visualize"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "the_model_info": {},
      "recommended_models": [],
      "basic_fields": [
        "image",
        "objects"
      ],
      "is_dynamic": false
    },
    {
      "title": "Zero-Shot Object Detection",
      "description": "Detects objects in images without the need for training data.\n    image, object detection, bounding boxes, zero-shot, mask\n\n    Use cases:\n    - Quickly detect objects in images without training data\n    - Identify objects in images without predefined labels\n    - Automate object detection for large datasets",
      "namespace": "huggingface.object_detection",
      "node_type": "huggingface.object_detection.ZeroShotObjectDetection",
      "layout": "default",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.zero_shot_object_detection"
          },
          "default": {
            "repo_id": "google/owlv2-base-patch16"
          },
          "title": "Model ID on Huggingface",
          "description": "The model ID to use for object detection"
        },
        {
          "name": "image",
          "type": {
            "type": "image"
          },
          "default": {},
          "title": "Inputs",
          "description": "The input image for object detection"
        },
        {
          "name": "threshold",
          "type": {
            "type": "float"
          },
          "default": 0.1,
          "title": "Confidence Threshold",
          "description": "Minimum confidence score for detected objects"
        },
        {
          "name": "top_k",
          "type": {
            "type": "int"
          },
          "default": 5,
          "title": "Top K",
          "description": "The number of top predictions to return"
        },
        {
          "name": "candidate_labels",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Candidate Labels",
          "description": "The candidate labels to detect in the image, separated by commas"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "object_detection_result"
              }
            ]
          },
          "name": "output"
        }
      ],
      "the_model_info": {},
      "recommended_models": [
        {
          "type": "hf.zero_shot_object_detection",
          "repo_id": "google/owlvit-base-patch32",
          "allow_patterns": [
            "README.md",
            "*.bin",
            "*.json",
            "**/*.json",
            "txt"
          ]
        },
        {
          "type": "hf.zero_shot_object_detection",
          "repo_id": "google/owlvit-large-patch14",
          "allow_patterns": [
            "README.md",
            "*.bin",
            "*.json",
            "**/*.json",
            "txt"
          ]
        },
        {
          "type": "hf.zero_shot_object_detection",
          "repo_id": "google/owlvit-base-patch16",
          "allow_patterns": [
            "README.md",
            "*.bin",
            "*.json",
            "**/*.json",
            "txt"
          ]
        },
        {
          "type": "hf.zero_shot_object_detection",
          "repo_id": "google/owlv2-base-patch16",
          "allow_patterns": [
            "README.md",
            "*.bin",
            "*.json",
            "**/*.json",
            "txt"
          ]
        },
        {
          "type": "hf.zero_shot_object_detection",
          "repo_id": "google/owlv2-base-patch16-ensemble",
          "allow_patterns": [
            "README.md",
            "*.bin",
            "*.json",
            "**/*.json",
            "txt"
          ]
        },
        {
          "type": "hf.zero_shot_object_detection",
          "repo_id": "IDEA-Research/grounding-dino-tiny",
          "allow_patterns": [
            "README.md",
            "*.bin",
            "*.json",
            "**/*.json",
            "txt"
          ]
        }
      ],
      "basic_fields": [
        "model",
        "image",
        "threshold",
        "top_k",
        "candidate_labels"
      ],
      "is_dynamic": false
    },
    {
      "title": "Animate Diff",
      "description": "Generates animated GIFs using the AnimateDiff pipeline.\n    image, animation, generation, AI\n\n    Use cases:\n    - Create animated visual content from text descriptions\n    - Generate dynamic visual effects for creative projects\n    - Produce animated illustrations for digital media",
      "namespace": "huggingface.video",
      "node_type": "huggingface.video.AnimateDiff",
      "layout": "default",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.stable_diffusion"
          },
          "default": {},
          "title": "Model",
          "description": "The model to use for image generation."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "masterpiece, bestquality, highlydetailed, ultradetailed, sunset, orange sky, warm lighting, fishing boats, ocean waves seagulls, rippling water, wharf, silhouette, serene atmosphere, dusk, evening glow, golden hour, coastal landscape, seaside scenery",
          "title": "Prompt",
          "description": "A text prompt describing the desired animation."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "bad quality, worse quality",
          "title": "Negative Prompt",
          "description": "A text prompt describing what you don't want in the animation."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 16,
          "title": "Num Frames",
          "description": "The number of frames in the animation.",
          "min": 1.0,
          "max": 60.0
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 7.5,
          "title": "Guidance Scale",
          "description": "Scale for classifier-free guidance.",
          "min": 1.0,
          "max": 20.0
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 25,
          "title": "Num Inference Steps",
          "description": "The number of denoising steps.",
          "min": 1.0,
          "max": 100.0
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": 42,
          "title": "Seed",
          "description": "Seed for the random number generator.",
          "min": 0.0
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "the_model_info": {},
      "recommended_models": [
        {
          "type": "hf.text_to_video",
          "repo_id": "guoyww/animatediff-motion-adapter-v1-5-2",
          "allow_patterns": [
            "*.fp16.safetensors",
            "*.json",
            "*.txt"
          ]
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "Lykon/dreamshaper-8",
          "allow_patterns": [
            "**/*.fp16.safetensors",
            "**/*.json",
            "**/*.txt",
            "*.json"
          ]
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "Yntec/Deliberate2",
          "allow_patterns": [
            "**/*.fp16.safetensors",
            "**/*.json",
            "**/*.txt",
            "*.json"
          ]
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "imagepipeline/epiC-PhotoGasm",
          "allow_patterns": [
            "**/*.fp16.safetensors",
            "**/*.json",
            "**/*.txt",
            "*.json"
          ]
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "526christian/526mix-v1.5",
          "allow_patterns": [
            "**/*.fp16.safetensors",
            "**/*.json",
            "**/*.txt",
            "*.json"
          ]
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "stablediffusionapi/realistic-vision-v51",
          "allow_patterns": [
            "**/*.fp16.safetensors",
            "**/*.json",
            "**/*.txt",
            "*.json"
          ]
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "stablediffusionapi/anything-v5",
          "allow_patterns": [
            "**/*.fp16.safetensors",
            "**/*.json",
            "**/*.txt",
            "*.json"
          ]
        }
      ],
      "basic_fields": [
        "model",
        "prompt"
      ],
      "is_dynamic": false
    },
    {
      "title": "Stable Video Diffusion",
      "description": "Generates a video from a single image using the Stable Video Diffusion model.\n    video, generation, AI, image-to-video, stable-diffusion, SD\n\n    Use cases:\n    - Create short animations from static images\n    - Generate dynamic content for presentations or social media\n    - Prototype video ideas from still concept art",
      "namespace": "huggingface.video",
      "node_type": "huggingface.video.StableVideoDiffusion",
      "layout": "default",
      "properties": [
        {
          "name": "input_image",
          "type": {
            "type": "image"
          },
          "default": {},
          "title": "Input Image",
          "description": "The input image to generate the video from, resized to 1024x576."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 14,
          "title": "Num Frames",
          "description": "Number of frames to generate.",
          "min": 1.0,
          "max": 50.0
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 25,
          "title": "Num Inference Steps",
          "description": "Number of steps per generated frame",
          "min": 1.0,
          "max": 100.0
        },
        {
          "name": "fps",
          "type": {
            "type": "int"
          },
          "default": 7,
          "title": "Fps",
          "description": "Frames per second for the output video.",
          "min": 1.0,
          "max": 30.0
        },
        {
          "name": "decode_chunk_size",
          "type": {
            "type": "int"
          },
          "default": 8,
          "title": "Decode Chunk Size",
          "description": "Number of frames to decode at once.",
          "min": 1.0,
          "max": 16.0
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": 42,
          "title": "Seed",
          "description": "Random seed for generation.",
          "min": 0.0,
          "max": 4294967295.0
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "the_model_info": {},
      "recommended_models": [
        {
          "type": "hf.stable_diffusion",
          "repo_id": "stabilityai/stable-video-diffusion-img2vid-xt",
          "allow_patterns": [
            "**/*.fp16.safetensors",
            "**/*.json",
            "**/*.txt",
            "*.json"
          ]
        }
      ],
      "basic_fields": [
        "input_image",
        "num_frames",
        "num_inference_steps",
        "fps",
        "decode_chunk_size",
        "seed"
      ],
      "is_dynamic": false
    },
    {
      "title": "Token Classification",
      "description": "Performs token classification tasks such as Named Entity Recognition (NER).\n    text, token classification, named entity recognition, natural language processing\n\n    Use cases:\n    - Named Entity Recognition in text\n    - Part-of-speech tagging\n    - Chunking and shallow parsing\n    - Information extraction from unstructured text",
      "namespace": "huggingface.token_classification",
      "node_type": "huggingface.token_classification.TokenClassification",
      "layout": "default",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.token_classification"
          },
          "default": {},
          "title": "Model ID on Huggingface",
          "description": "The model ID to use for token classification"
        },
        {
          "name": "inputs",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Input Text",
          "description": "The input text for token classification"
        },
        {
          "name": "aggregation_strategy",
          "type": {
            "type": "enum",
            "values": [
              "simple",
              "first",
              "average",
              "max"
            ],
            "type_name": "nodetool.nodes.huggingface.token_classification.AggregationStrategy"
          },
          "default": "simple",
          "title": "Aggregation Strategy",
          "description": "Strategy to aggregate tokens into entities"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dataframe"
          },
          "name": "output"
        }
      ],
      "the_model_info": {},
      "recommended_models": [],
      "basic_fields": [
        "model",
        "inputs",
        "aggregation_strategy"
      ],
      "is_dynamic": false
    }
  ],
  "assets": [
    {
      "package_name": "nodetool-huggingface",
      "name": "Summarize Audio.jpg",
      "path": ""
    },
    {
      "package_name": "nodetool-huggingface",
      "name": "Upscaling.jpg",
      "path": ""
    },
    {
      "package_name": "nodetool-huggingface",
      "name": "Object Detection.jpg",
      "path": ""
    },
    {
      "package_name": "nodetool-huggingface",
      "name": "Image Enhance.jpg",
      "path": ""
    },
    {
      "package_name": "nodetool-huggingface",
      "name": "Stable Diffusion.jpg",
      "path": ""
    },
    {
      "package_name": "nodetool-huggingface",
      "name": "Piano Track.jpg",
      "path": ""
    },
    {
      "package_name": "nodetool-huggingface",
      "name": "Segmentation.jpg",
      "path": ""
    },
    {
      "package_name": "nodetool-huggingface",
      "name": "Audio To Image.jpg",
      "path": ""
    },
    {
      "package_name": "nodetool-huggingface",
      "name": "Style Transfer.jpg",
      "path": ""
    },
    {
      "package_name": "nodetool-huggingface",
      "name": "Transcribe Audio.jpg",
      "path": ""
    },
    {
      "package_name": "nodetool-huggingface",
      "name": "Movie Posters.jpg",
      "path": ""
    },
    {
      "package_name": "nodetool-huggingface",
      "name": "Audio To Spectrogram.jpg",
      "path": ""
    },
    {
      "package_name": "nodetool-huggingface",
      "name": "Depth Estimation.jpg",
      "path": ""
    },
    {
      "package_name": "nodetool-huggingface",
      "name": "Add Subtitles To Video.jpg",
      "path": ""
    },
    {
      "package_name": "nodetool-huggingface",
      "name": "stable_diffusion_xl.jpg",
      "path": ""
    },
    {
      "package_name": "nodetool-huggingface",
      "name": "Pokemon Maker.jpg",
      "path": ""
    }
  ],
  "examples": [
    {
      "id": "add_subtitles_to_video",
      "name": "Add Subtitles To Video",
      "description": "This workflow automatically transcribes speech in videos and adds subtitles. It extracts audio from the input video, uses OpenAI's Whisper model to generate word-level timestamps and transcriptions, and then renders the subtitles back onto the original video. Perfect for creating accessible content, adding captions to social media videos, or transcribing presentations.",
      "tags": [
        "start",
        "video",
        "huggingface"
      ]
    },
    {
      "id": "style_transfer",
      "name": "Style Transfer",
      "description": "Transform your images by applying artistic styles from reference images. This workflow uses IP-Adapter to transfer visual styles while ControlNet preserves the original structure. Perfect for creating artistic variations of portraits or other images.",
      "tags": [
        "huggingface",
        "image",
        "start"
      ]
    },
    {
      "id": "object_detection",
      "name": "Object Detection",
      "description": "Detect objects in an image and visualize the detections",
      "tags": [
        "huggingface",
        "start"
      ]
    },
    {
      "id": "segmentation",
      "name": "Segmentation",
      "description": "Segment images and visualize the segments",
      "tags": [
        "huggingface",
        "image"
      ]
    },
    {
      "id": "6e96807232a211f0a8870000194fbf00",
      "name": "Movie Posters",
      "description": "Create cinematic movie posters using AI image generation",
      "tags": [
        "start",
        "image",
        "huggingface"
      ]
    },
    {
      "id": "transcribe_audio",
      "name": "Transcribe Audio",
      "description": "Convert speech to text using Whisper model with word-level timestamps",
      "tags": [
        "start",
        "audio",
        "huggingface"
      ]
    },
    {
      "id": "8675bdaa388a11f0951800006f96a7c6",
      "name": "Pokemon Maker",
      "description": "",
      "tags": []
    },
    {
      "id": "01ddcf16e35711ef80af000038478aae",
      "name": "Audio To Image",
      "description": "Transform spoken descriptions into images with this workflow. Record or upload audio, which is transcribed by Whisper and then visualized by Stable Diffusion. Perfect for quickly generating images from verbal ideas without typing.",
      "tags": [
        "huggingface",
        "multimodal",
        "start"
      ]
    },
    {
      "id": "3dc7a22e12f311f0a84600004c6eb2d5",
      "name": "Audio To Spectrogram",
      "description": "Create a spectrogram from an audio file and use creative upscaling to transform it into wall-worthy art.",
      "tags": [
        "audio",
        "multimodal",
        "start",
        "huggingface"
      ]
    },
    {
      "id": "depth_estimation",
      "name": "Depth Estimation",
      "description": "Estimate the depth of an image",
      "tags": [
        "image",
        "huggingface"
      ]
    },
    {
      "id": "f1d42e6a12fb11f0901100001aeb0d2f",
      "name": "Summarize Audio",
      "description": "Transcribe an audio file and summarize the text.",
      "tags": [
        "audio",
        "start",
        "huggingface"
      ]
    },
    {
      "id": "dfff77a8f38911ef919400004a056799",
      "name": "Upscaling",
      "description": "Upscale low-resolution images to higher quality using RealESRGAN, a powerful AI model that enhances details and clarity without artifacts.",
      "tags": [
        "image",
        "start",
        "huggingface"
      ]
    }
  ]
}