{
  "name": "nodetool-huggingface",
  "description": "HuggingFace nodes for Nodetool",
  "version": "0.6.0",
  "authors": [
    "Matthias Georgi <matti.georgi@gmail.com>"
  ],
  "repo_id": "nodetool-ai/nodetool-huggingface",
  "nodes": [
    {
      "title": "LoRA Selector",
      "description": "Selects up to 5 LoRA models to apply to a Stable Diffusion model.\n    lora, model customization, fine-tuning, SD\n\n    Use cases:\n    - Combining multiple LoRA models for unique image styles\n    - Fine-tuning Stable Diffusion models with specific attributes\n    - Experimenting with different LoRA combinations",
      "namespace": "huggingface.lora",
      "node_type": "huggingface.lora.LoRASelector",
      "properties": [
        {
          "name": "lora1",
          "type": {
            "type": "hf.lora_sd"
          },
          "default": {},
          "title": "Lora1",
          "description": "First LoRA model"
        },
        {
          "name": "strength1",
          "type": {
            "type": "float"
          },
          "default": 1.0,
          "title": "Strength1",
          "description": "Strength for first LoRA",
          "min": 0.0,
          "max": 2.0
        },
        {
          "name": "lora2",
          "type": {
            "type": "hf.lora_sd"
          },
          "default": {},
          "title": "Lora2",
          "description": "Second LoRA model"
        },
        {
          "name": "strength2",
          "type": {
            "type": "float"
          },
          "default": 1.0,
          "title": "Strength2",
          "description": "Strength for second LoRA",
          "min": 0.0,
          "max": 2.0
        },
        {
          "name": "lora3",
          "type": {
            "type": "hf.lora_sd"
          },
          "default": {},
          "title": "Lora3",
          "description": "Third LoRA model"
        },
        {
          "name": "strength3",
          "type": {
            "type": "float"
          },
          "default": 1.0,
          "title": "Strength3",
          "description": "Strength for third LoRA",
          "min": 0.0,
          "max": 2.0
        },
        {
          "name": "lora4",
          "type": {
            "type": "hf.lora_sd"
          },
          "default": {},
          "title": "Lora4",
          "description": "Fourth LoRA model"
        },
        {
          "name": "strength4",
          "type": {
            "type": "float"
          },
          "default": 1.0,
          "title": "Strength4",
          "description": "Strength for fourth LoRA",
          "min": 0.0,
          "max": 2.0
        },
        {
          "name": "lora5",
          "type": {
            "type": "hf.lora_sd"
          },
          "default": {},
          "title": "Lora5",
          "description": "Fifth LoRA model"
        },
        {
          "name": "strength5",
          "type": {
            "type": "float"
          },
          "default": 1.0,
          "title": "Strength5",
          "description": "Strength for fifth LoRA",
          "min": 0.0,
          "max": 2.0
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "hf.lora_sd_config"
              }
            ]
          },
          "name": "output"
        }
      ],
      "recommended_models": [
        {
          "type": "hf.lora_sd",
          "repo_id": "danbrown/loras",
          "path": "2d_sprite.safetensors"
        },
        {
          "type": "hf.lora_sd",
          "repo_id": "danbrown/loras",
          "path": "ghibli_scenery.safetensors"
        },
        {
          "type": "hf.lora_sd",
          "repo_id": "danbrown/loras",
          "path": "add_detail.safetensors"
        },
        {
          "type": "hf.lora_sd",
          "repo_id": "danbrown/loras",
          "path": "colorwater.safetensors"
        },
        {
          "type": "hf.lora_sd",
          "repo_id": "danbrown/loras",
          "path": "sxz_game_assets.safetensors"
        },
        {
          "type": "hf.lora_sd",
          "repo_id": "danbrown/loras",
          "path": "3Danaglyph.safetensors"
        },
        {
          "type": "hf.lora_sd",
          "repo_id": "danbrown/loras",
          "path": "akiratoriyama_style.safetensors"
        },
        {
          "type": "hf.lora_sd",
          "repo_id": "danbrown/loras",
          "path": "animeoutlineV4.safetensors"
        },
        {
          "type": "hf.lora_sd",
          "repo_id": "danbrown/loras",
          "path": "aqua_konosuba.safetensors"
        },
        {
          "type": "hf.lora_sd",
          "repo_id": "danbrown/loras",
          "path": "arakihirohiko_style.safetensors"
        },
        {
          "type": "hf.lora_sd",
          "repo_id": "danbrown/loras",
          "path": "arcane_style.safetensors"
        },
        {
          "type": "hf.lora_sd",
          "repo_id": "danbrown/loras",
          "path": "canetaazul.safetensors"
        },
        {
          "type": "hf.lora_sd",
          "repo_id": "danbrown/loras",
          "path": "cyberpunk_tarot.safetensors"
        },
        {
          "type": "hf.lora_sd",
          "repo_id": "danbrown/loras",
          "path": "discoelysium_style.safetensors"
        },
        {
          "type": "hf.lora_sd",
          "repo_id": "danbrown/loras",
          "path": "esdeath_akamegakill.safetensors"
        },
        {
          "type": "hf.lora_sd",
          "repo_id": "danbrown/loras",
          "path": "fire_vfx.safetensors"
        },
        {
          "type": "hf.lora_sd",
          "repo_id": "danbrown/loras",
          "path": "flamingeye.safetensors"
        },
        {
          "type": "hf.lora_sd",
          "repo_id": "danbrown/loras",
          "path": "funnycreatures.safetensors"
        },
        {
          "type": "hf.lora_sd",
          "repo_id": "danbrown/loras",
          "path": "gacha_splash.safetensors"
        },
        {
          "type": "hf.lora_sd",
          "repo_id": "danbrown/loras",
          "path": "gigachad.safetensors"
        },
        {
          "type": "hf.lora_sd",
          "repo_id": "danbrown/loras",
          "path": "gyokai_style.safetensors"
        },
        {
          "type": "hf.lora_sd",
          "repo_id": "danbrown/loras",
          "path": "harold.safetensors"
        },
        {
          "type": "hf.lora_sd",
          "repo_id": "danbrown/loras",
          "path": "hiderohoribes_style.safetensors"
        },
        {
          "type": "hf.lora_sd",
          "repo_id": "danbrown/loras",
          "path": "ilyakuvshinov_style.safetensors"
        },
        {
          "type": "hf.lora_sd",
          "repo_id": "danbrown/loras",
          "path": "jacksparrow.safetensors"
        },
        {
          "type": "hf.lora_sd",
          "repo_id": "danbrown/loras",
          "path": "jimlee_style.safetensors"
        },
        {
          "type": "hf.lora_sd",
          "repo_id": "danbrown/loras",
          "path": "komowataharuka_chibiart.safetensors"
        },
        {
          "type": "hf.lora_sd",
          "repo_id": "danbrown/loras",
          "path": "lightning_vfx.safetensors"
        },
        {
          "type": "hf.lora_sd",
          "repo_id": "danbrown/loras",
          "path": "lucy_cyberpunk.safetensors"
        },
        {
          "type": "hf.lora_sd",
          "repo_id": "danbrown/loras",
          "path": "luisap_pixelart.safetensors"
        },
        {
          "type": "hf.lora_sd",
          "repo_id": "danbrown/loras",
          "path": "mumei_kabaneri.safetensors"
        },
        {
          "type": "hf.lora_sd",
          "repo_id": "danbrown/loras",
          "path": "myheroacademia_style.safetensors"
        },
        {
          "type": "hf.lora_sd",
          "repo_id": "danbrown/loras",
          "path": "neoartcore.safetensors"
        },
        {
          "type": "hf.lora_sd",
          "repo_id": "danbrown/loras",
          "path": "ochakouraraka.safetensors"
        },
        {
          "type": "hf.lora_sd",
          "repo_id": "danbrown/loras",
          "path": "onepiece_style.safetensors"
        },
        {
          "type": "hf.lora_sd",
          "repo_id": "danbrown/loras",
          "path": "paimon_genshinimpact.safetensors"
        },
        {
          "type": "hf.lora_sd",
          "repo_id": "danbrown/loras",
          "path": "peanutscomics_style.safetensors"
        },
        {
          "type": "hf.lora_sd",
          "repo_id": "danbrown/loras",
          "path": "pepefrog.safetensors"
        },
        {
          "type": "hf.lora_sd",
          "repo_id": "danbrown/loras",
          "path": "persona5_portraits.safetensors"
        },
        {
          "type": "hf.lora_sd",
          "repo_id": "danbrown/loras",
          "path": "persona5_style.safetensors"
        },
        {
          "type": "hf.lora_sd",
          "repo_id": "danbrown/loras",
          "path": "pixhell.safetensors"
        },
        {
          "type": "hf.lora_sd",
          "repo_id": "danbrown/loras",
          "path": "princesszelda.safetensors"
        },
        {
          "type": "hf.lora_sd",
          "repo_id": "danbrown/loras",
          "path": "satoshiuruchihara_style.safetensors"
        },
        {
          "type": "hf.lora_sd",
          "repo_id": "danbrown/loras",
          "path": "shinobu_demonslayer.safetensors"
        },
        {
          "type": "hf.lora_sd",
          "repo_id": "danbrown/loras",
          "path": "sokolov_style.safetensors"
        },
        {
          "type": "hf.lora_sd",
          "repo_id": "danbrown/loras",
          "path": "standingbackgroundv1.safetensors"
        },
        {
          "type": "hf.lora_sd",
          "repo_id": "danbrown/loras",
          "path": "sun_shadow_style.safetensors"
        },
        {
          "type": "hf.lora_sd",
          "repo_id": "danbrown/loras",
          "path": "thickeranimelines.safetensors"
        },
        {
          "type": "hf.lora_sd",
          "repo_id": "danbrown/loras",
          "path": "threesidedview.safetensors"
        },
        {
          "type": "hf.lora_sd",
          "repo_id": "danbrown/loras",
          "path": "twitch_emotes.safetensors"
        },
        {
          "type": "hf.lora_sd",
          "repo_id": "danbrown/loras",
          "path": "water_vfx.safetensors"
        },
        {
          "type": "hf.lora_sd",
          "repo_id": "danbrown/loras",
          "path": "wlop_style.safetensors"
        },
        {
          "type": "hf.lora_sd",
          "repo_id": "danbrown/loras",
          "path": "zerotwo_darling.safetensors"
        }
      ],
      "basic_fields": [
        "lora1",
        "strength1",
        "lora2",
        "strength2",
        "lora3",
        "strength3",
        "lora4",
        "strength4",
        "lora5",
        "strength5"
      ]
    },
    {
      "title": "LoRA XL Selector",
      "description": "Selects up to 5 LoRA models to apply to a Stable Diffusion XL model.\n    lora, model customization, fine-tuning, SDXL\n\n    Use cases:\n    - Combining multiple LoRA models for unique image styles\n    - Fine-tuning Stable Diffusion XL models with specific attributes\n    - Experimenting with different LoRA combinations",
      "namespace": "huggingface.lora",
      "node_type": "huggingface.lora.LoRASelectorXL",
      "properties": [
        {
          "name": "lora1",
          "type": {
            "type": "hf.lora_sdxl"
          },
          "default": {},
          "title": "Lora1",
          "description": "First LoRA model"
        },
        {
          "name": "strength1",
          "type": {
            "type": "float"
          },
          "default": 1.0,
          "title": "Strength1",
          "description": "Strength for first LoRA",
          "min": 0.0,
          "max": 2.0
        },
        {
          "name": "lora2",
          "type": {
            "type": "hf.lora_sdxl"
          },
          "default": {},
          "title": "Lora2",
          "description": "Second LoRA model"
        },
        {
          "name": "strength2",
          "type": {
            "type": "float"
          },
          "default": 1.0,
          "title": "Strength2",
          "description": "Strength for second LoRA",
          "min": 0.0,
          "max": 2.0
        },
        {
          "name": "lora3",
          "type": {
            "type": "hf.lora_sdxl"
          },
          "default": {},
          "title": "Lora3",
          "description": "Third LoRA model"
        },
        {
          "name": "strength3",
          "type": {
            "type": "float"
          },
          "default": 1.0,
          "title": "Strength3",
          "description": "Strength for third LoRA",
          "min": 0.0,
          "max": 2.0
        },
        {
          "name": "lora4",
          "type": {
            "type": "hf.lora_sdxl"
          },
          "default": {},
          "title": "Lora4",
          "description": "Fourth LoRA model"
        },
        {
          "name": "strength4",
          "type": {
            "type": "float"
          },
          "default": 1.0,
          "title": "Strength4",
          "description": "Strength for fourth LoRA",
          "min": 0.0,
          "max": 2.0
        },
        {
          "name": "lora5",
          "type": {
            "type": "hf.lora_sdxl"
          },
          "default": {},
          "title": "Lora5",
          "description": "Fifth LoRA model"
        },
        {
          "name": "strength5",
          "type": {
            "type": "float"
          },
          "default": 1.0,
          "title": "Strength5",
          "description": "Strength for fifth LoRA",
          "min": 0.0,
          "max": 2.0
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "hf.lora_sdxl_config"
              }
            ]
          },
          "name": "output"
        }
      ],
      "recommended_models": [
        {
          "type": "hf.lora_sdxl",
          "repo_id": "CiroN2022/toy-face",
          "path": "toy_face_sdxl.safetensors"
        },
        {
          "type": "hf.lora_sdxl",
          "repo_id": "nerijs/pixel-art-xl",
          "path": "pixel-art-xl.safetensors"
        },
        {
          "type": "hf.lora_sdxl",
          "repo_id": "goofyai/3d_render_style_xl",
          "path": "3d_render_style_xl.safetensors"
        },
        {
          "type": "hf.lora_sdxl",
          "repo_id": "artificialguybr/CuteCartoonRedmond-V2",
          "path": "CuteCartoonRedmond-CuteCartoon-CuteCartoonAF.safetensors"
        },
        {
          "type": "hf.lora_sdxl",
          "repo_id": "blink7630/graphic-novel-illustration",
          "path": "Graphic_Novel_Illustration-000007.safetensors"
        },
        {
          "type": "hf.lora_sdxl",
          "repo_id": "robert123231/coloringbookgenerator",
          "path": "ColoringBookRedmond-ColoringBook-ColoringBookAF.safetensors"
        },
        {
          "type": "hf.lora_sdxl",
          "repo_id": "Linaqruf/anime-detailer-xl-lora",
          "path": "anime-detailer-xl-lora.safetensors"
        }
      ],
      "basic_fields": [
        "lora1",
        "strength1",
        "lora2",
        "strength2",
        "lora3",
        "strength3",
        "lora4",
        "strength4",
        "lora5",
        "strength5"
      ]
    },
    {
      "title": "Question Answering",
      "description": "Answers questions based on a given context.\n    text, question answering, natural language processing\n\n    Use cases:\n    - Automated customer support\n    - Information retrieval from documents\n    - Reading comprehension tasks\n    - Enhancing search functionality",
      "namespace": "huggingface.question_answering",
      "node_type": "huggingface.question_answering.QuestionAnswering",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.question_answering"
          },
          "default": {},
          "title": "Model ID on Huggingface",
          "description": "The model ID to use for question answering"
        },
        {
          "name": "context",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Context",
          "description": "The context or passage to answer questions from"
        },
        {
          "name": "question",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Question",
          "description": "The question to be answered based on the context"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "recommended_models": [
        {
          "type": "hf.question_answering",
          "repo_id": "distilbert-base-cased-distilled-squad",
          "allow_patterns": [
            "*.json",
            "*.txt",
            "*.safetensors"
          ]
        },
        {
          "type": "hf.question_answering",
          "repo_id": "bert-large-uncased-whole-word-masking-finetuned-squad",
          "allow_patterns": [
            "*.json",
            "*.txt",
            "*.safetensors"
          ]
        },
        {
          "type": "hf.question_answering",
          "repo_id": "deepset/roberta-base-squad2",
          "allow_patterns": [
            "*.json",
            "*.txt",
            "*.safetensors"
          ]
        },
        {
          "type": "hf.question_answering",
          "repo_id": "distilbert-base-uncased-distilled-squad",
          "allow_patterns": [
            "*.json",
            "*.txt",
            "*.safetensors"
          ]
        }
      ],
      "basic_fields": [
        "model",
        "context",
        "question"
      ]
    },
    {
      "title": "Table Question Answering",
      "description": "Answers questions based on tabular data.\n    table, question answering, natural language processing\n\n    Use cases:\n    - Querying databases using natural language\n    - Analyzing spreadsheet data with questions\n    - Extracting insights from tabular reports\n    - Automated data exploration",
      "namespace": "huggingface.question_answering",
      "node_type": "huggingface.question_answering.TableQuestionAnswering",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.table_question_answering"
          },
          "default": {},
          "title": "Model ID on Huggingface",
          "description": "The model ID to use for table question answering"
        },
        {
          "name": "dataframe",
          "type": {
            "type": "dataframe"
          },
          "default": {},
          "title": "Table",
          "description": "The input table to query"
        },
        {
          "name": "question",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Question",
          "description": "The question to be answered based on the table"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "str"
          },
          "name": "answer"
        },
        {
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "tuple",
                "type_args": [
                  {
                    "type": "int"
                  },
                  {
                    "type": "int"
                  }
                ]
              }
            ]
          },
          "name": "coordinates"
        },
        {
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "name": "cells"
        },
        {
          "type": {
            "type": "str"
          },
          "name": "aggregator"
        }
      ],
      "recommended_models": [
        {
          "type": "hf.table_question_answering",
          "repo_id": "google/tapas-base-finetuned-wtq",
          "allow_patterns": [
            "*.json",
            "*.txt",
            "*.safetensors"
          ]
        },
        {
          "type": "hf.table_question_answering",
          "repo_id": "google/tapas-large-finetuned-wtq",
          "allow_patterns": [
            "*.json",
            "*.txt",
            "*.safetensors"
          ]
        },
        {
          "type": "hf.table_question_answering",
          "repo_id": "microsoft/tapex-large-finetuned-tabfact",
          "allow_patterns": [
            "*.json",
            "*.txt",
            "*.safetensors"
          ]
        },
        {
          "type": "hf.table_question_answering",
          "repo_id": "google/tapas-large-finetuned-squad",
          "allow_patterns": [
            "*.json",
            "*.txt",
            "*.safetensors"
          ]
        }
      ],
      "basic_fields": [
        "model",
        "dataframe",
        "question"
      ]
    },
    {
      "title": "Fill Mask",
      "description": "Fills in a masked token in a given text.\n    text, fill-mask, natural language processing\n\n    Use cases:\n    - Text completion\n    - Sentence prediction\n    - Language understanding tasks\n    - Generating text options",
      "namespace": "huggingface.fill_mask",
      "node_type": "huggingface.fill_mask.FillMask",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.fill_mask"
          },
          "default": {},
          "title": "Model ID",
          "description": "The model ID to use for fill-mask task"
        },
        {
          "name": "inputs",
          "type": {
            "type": "str"
          },
          "default": "The capital of France is [MASK].",
          "title": "Inputs",
          "description": "The input text with [MASK] token to be filled"
        },
        {
          "name": "top_k",
          "type": {
            "type": "int"
          },
          "default": 5,
          "title": "Top K",
          "description": "Number of top predictions to return"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "recommended_models": [
        {
          "type": "hf.fill_mask",
          "repo_id": "bert-base-uncased",
          "allow_patterns": [
            "*.json",
            "*.txt",
            "*.safetensors"
          ]
        },
        {
          "type": "hf.fill_mask",
          "repo_id": "roberta-base",
          "allow_patterns": [
            "*.json",
            "*.txt",
            "*.safetensors"
          ]
        },
        {
          "type": "hf.fill_mask",
          "repo_id": "distilbert-base-uncased",
          "allow_patterns": [
            "*.json",
            "*.txt",
            "*.safetensors"
          ]
        },
        {
          "type": "hf.fill_mask",
          "repo_id": "albert-base-v2",
          "allow_patterns": [
            "*.json",
            "*.txt",
            "*.safetensors"
          ]
        }
      ],
      "basic_fields": [
        "model",
        "inputs",
        "top_k"
      ]
    },
    {
      "title": "Translation",
      "description": "Translates text from one language to another.\n    text, translation, natural language processing\n\n    Use cases:\n    - Multilingual content creation\n    - Cross-language communication\n    - Localization of applications and websites\n\n    Note: some models support more languages than others.",
      "namespace": "huggingface.translation",
      "node_type": "huggingface.translation.Translation",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.translation"
          },
          "default": {},
          "title": "Model ID on HuggingFace",
          "description": "The model ID to use for translation"
        },
        {
          "name": "inputs",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Input Text",
          "description": "The text to translate"
        },
        {
          "name": "source_lang",
          "type": {
            "type": "enum",
            "values": [
              "ar",
              "bn",
              "bs",
              "zh",
              "hr",
              "cs",
              "da",
              "nl",
              "en",
              "fil",
              "fi",
              "fr",
              "de",
              "el",
              "he",
              "hi",
              "id",
              "it",
              "ja",
              "ko",
              "ms",
              "me",
              "no",
              "pl",
              "pt",
              "pa",
              "ru",
              "ro",
              "sr",
              "sk",
              "sl",
              "es",
              "sv",
              "th",
              "tr",
              "vi"
            ],
            "type_name": "nodetool.nodes.huggingface.translation.LanguageCode"
          },
          "default": "en",
          "title": "Source Language",
          "description": "The source language code (e.g., 'en' for English)"
        },
        {
          "name": "target_lang",
          "type": {
            "type": "enum",
            "values": [
              "ar",
              "bn",
              "bs",
              "zh",
              "hr",
              "cs",
              "da",
              "nl",
              "en",
              "fil",
              "fi",
              "fr",
              "de",
              "el",
              "he",
              "hi",
              "id",
              "it",
              "ja",
              "ko",
              "ms",
              "me",
              "no",
              "pl",
              "pt",
              "pa",
              "ru",
              "ro",
              "sr",
              "sk",
              "sl",
              "es",
              "sv",
              "th",
              "tr",
              "vi"
            ],
            "type_name": "nodetool.nodes.huggingface.translation.LanguageCode"
          },
          "default": "fr",
          "title": "Target Language",
          "description": "The target language code (e.g., 'fr' for French)"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "str"
          },
          "name": "output"
        }
      ],
      "recommended_models": [
        {
          "type": "hf.translation",
          "repo_id": "google-t5/t5-base",
          "allow_patterns": [
            "*.json",
            "*.txt",
            "*.safetensors"
          ]
        },
        {
          "type": "hf.translation",
          "repo_id": "google-t5/t5-large",
          "allow_patterns": [
            "*.json",
            "*.txt",
            "*.safetensors"
          ]
        },
        {
          "type": "hf.translation",
          "repo_id": "google-t5/t5-small",
          "allow_patterns": [
            "*.json",
            "*.txt",
            "*.safetensors"
          ]
        }
      ],
      "basic_fields": [
        "model",
        "inputs",
        "source_lang",
        "target_lang"
      ]
    },
    {
      "title": "Image To Text",
      "description": "Generates textual descriptions from images.\n    image, captioning, OCR, image-to-text\n\n    Use cases:\n    - Generate captions for images\n    - Extract text from images (OCR)\n    - Describe image content for visually impaired users\n    - Build accessibility features for visual content",
      "namespace": "huggingface.image_to_text",
      "node_type": "huggingface.image_to_text.ImageToText",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.image_to_text"
          },
          "default": {},
          "title": "Model ID on Huggingface",
          "description": "The model ID to use for image-to-text generation"
        },
        {
          "name": "image",
          "type": {
            "type": "image"
          },
          "default": {},
          "title": "Input Image",
          "description": "The image to generate text from"
        },
        {
          "name": "max_new_tokens",
          "type": {
            "type": "int"
          },
          "default": 1024,
          "title": "Max New Tokens",
          "description": "The maximum number of tokens to generate (if supported by model)"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "str"
          },
          "name": "output"
        }
      ],
      "recommended_models": [
        {
          "type": "hf.image_to_text",
          "repo_id": "Salesforce/blip-image-captioning-base",
          "allow_patterns": [
            "*.safetensors",
            "*.json",
            "*.txt",
            "*.model"
          ]
        },
        {
          "type": "hf.image_to_text",
          "repo_id": "Salesforce/blip2-opt-2.7b",
          "allow_patterns": [
            "*.safetensors",
            "*.json",
            "*.txt",
            "*.model"
          ]
        },
        {
          "type": "hf.image_to_text",
          "repo_id": "microsoft/git-base",
          "allow_patterns": [
            "*.safetensors",
            "*.json",
            "*.txt",
            "*.model"
          ]
        },
        {
          "type": "hf.image_to_text",
          "repo_id": "nlpconnect/vit-gpt2-image-captioning",
          "allow_patterns": [
            "*.safetensors",
            "*.json",
            "*.txt",
            "*.model"
          ]
        }
      ],
      "basic_fields": [
        "model",
        "image"
      ]
    },
    {
      "title": "Load Image To Text Model",
      "description": "",
      "namespace": "huggingface.image_to_text",
      "node_type": "huggingface.image_to_text.LoadImageToTextModel",
      "properties": [
        {
          "name": "repo_id",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Model ID on Huggingface",
          "description": "The model ID to use for image-to-text generation"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "hf.image_to_text"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "repo_id"
      ]
    },
    {
      "title": "Image To Text",
      "description": "Generates text descriptions from images.\n    image, text, captioning, vision-language\n\n    Use cases:\n    - Automatic image captioning\n    - Assisting visually impaired users\n    - Enhancing image search capabilities\n    - Generating alt text for web images",
      "namespace": "huggingface.multimodal",
      "node_type": "huggingface.multimodal.ImageToText",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.image_to_text"
          },
          "default": {},
          "title": "Model ID on Huggingface",
          "description": "The model ID to use for image-to-text generation"
        },
        {
          "name": "image",
          "type": {
            "type": "image"
          },
          "default": {},
          "title": "Input Image",
          "description": "The image to generate text from"
        },
        {
          "name": "max_new_tokens",
          "type": {
            "type": "int"
          },
          "default": 50,
          "title": "Max New Tokens",
          "description": "The maximum number of tokens to generate"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "str"
          },
          "name": "output"
        }
      ],
      "recommended_models": [
        {
          "type": "hf.image_to_text",
          "repo_id": "Salesforce/blip-image-captioning-base",
          "allow_patterns": [
            "*.bin",
            "*.json",
            "*.txt    "
          ]
        },
        {
          "type": "hf.image_to_text",
          "repo_id": "Salesforce/blip-image-captioning-large",
          "allow_patterns": [
            "*.bin",
            "*.json",
            "*.txt"
          ]
        },
        {
          "type": "hf.image_to_text",
          "repo_id": "nlpconnect/vit-gpt2-image-captioning",
          "allow_patterns": [
            "*.bin",
            "*.json",
            "*.txt"
          ]
        },
        {
          "type": "hf.image_to_text",
          "repo_id": "microsoft/git-base-coco",
          "allow_patterns": [
            "*.bin",
            "*.json",
            "*.txt"
          ]
        }
      ],
      "basic_fields": [
        "model",
        "image",
        "max_new_tokens"
      ]
    },
    {
      "title": "Visual Question Answering",
      "description": "Answers questions about images.\n    image, text, question answering, multimodal\n\n    Use cases:\n    - Image content analysis\n    - Automated image captioning\n    - Visual information retrieval\n    - Accessibility tools for visually impaired users",
      "namespace": "huggingface.multimodal",
      "node_type": "huggingface.multimodal.VisualQuestionAnswering",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.visual_question_answering"
          },
          "default": {},
          "title": "Model ID on Huggingface",
          "description": "The model ID to use for visual question answering"
        },
        {
          "name": "image",
          "type": {
            "type": "image"
          },
          "default": {},
          "title": "Image",
          "description": "The image to analyze"
        },
        {
          "name": "question",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Question",
          "description": "The question to be answered about the image"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "str"
          },
          "name": "output"
        }
      ],
      "recommended_models": [
        {
          "type": "hf.visual_question_answering",
          "repo_id": "Salesforce/blip-vqa-base",
          "allow_patterns": [
            "*.bin",
            "*.json",
            "*.txt"
          ]
        }
      ],
      "basic_fields": [
        "model",
        "image",
        "question"
      ]
    },
    {
      "title": "Animate Diff",
      "description": "Generates animated GIFs using the AnimateDiff pipeline.\n    image, animation, generation, AI\n\n    Use cases:\n    - Create animated visual content from text descriptions\n    - Generate dynamic visual effects for creative projects\n    - Produce animated illustrations for digital media",
      "namespace": "huggingface.text_to_video",
      "node_type": "huggingface.text_to_video.AnimateDiff",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.stable_diffusion"
          },
          "default": {},
          "title": "Model",
          "description": "The model to use for image generation."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "masterpiece, bestquality, highlydetailed, ultradetailed, sunset, orange sky, warm lighting, fishing boats, ocean waves seagulls, rippling water, wharf, silhouette, serene atmosphere, dusk, evening glow, golden hour, coastal landscape, seaside scenery",
          "title": "Prompt",
          "description": "A text prompt describing the desired animation."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "bad quality, worse quality",
          "title": "Negative Prompt",
          "description": "A text prompt describing what you don't want in the animation."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 16,
          "title": "Num Frames",
          "description": "The number of frames in the animation.",
          "min": 1.0,
          "max": 60.0
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 7.5,
          "title": "Guidance Scale",
          "description": "Scale for classifier-free guidance.",
          "min": 1.0,
          "max": 20.0
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 25,
          "title": "Num Inference Steps",
          "description": "The number of denoising steps.",
          "min": 1.0,
          "max": 100.0
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": 42,
          "title": "Seed",
          "description": "Seed for the random number generator.",
          "min": 0.0
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "recommended_models": [
        {
          "type": "hf.text_to_video",
          "repo_id": "guoyww/animatediff-motion-adapter-v1-5-2",
          "allow_patterns": [
            "*.fp16.safetensors",
            "*.json",
            "*.txt"
          ]
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "Lykon/dreamshaper-8",
          "allow_patterns": [
            "**/*.fp16.safetensors",
            "**/*.json",
            "**/*.txt",
            "*.json"
          ]
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "Yntec/Deliberate2",
          "allow_patterns": [
            "**/*.fp16.safetensors",
            "**/*.json",
            "**/*.txt",
            "*.json"
          ]
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "imagepipeline/epiC-PhotoGasm",
          "allow_patterns": [
            "**/*.fp16.safetensors",
            "**/*.json",
            "**/*.txt",
            "*.json"
          ]
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "526christian/526mix-v1.5",
          "allow_patterns": [
            "**/*.fp16.safetensors",
            "**/*.json",
            "**/*.txt",
            "*.json"
          ]
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "stablediffusionapi/realistic-vision-v51",
          "allow_patterns": [
            "**/*.fp16.safetensors",
            "**/*.json",
            "**/*.txt",
            "*.json"
          ]
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "stablediffusionapi/anything-v5",
          "allow_patterns": [
            "**/*.fp16.safetensors",
            "**/*.json",
            "**/*.txt",
            "*.json"
          ]
        }
      ],
      "basic_fields": [
        "model",
        "prompt"
      ]
    },
    {
      "title": "CogVideoX",
      "description": "Generates videos from text prompts using CogVideoX, a large diffusion transformer model.\n    video, generation, AI, text-to-video, transformer, diffusion\n\n    Use cases:\n    - Create high-quality videos from text descriptions\n    - Generate longer and more consistent videos\n    - Produce cinematic content for creative projects\n    - Create animated scenes for storytelling\n    - Generate video content for marketing and media",
      "namespace": "huggingface.text_to_video",
      "node_type": "huggingface.text_to_video.CogVideoX",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "A detailed wooden toy ship with intricately carved masts and sails is seen gliding smoothly over a plush, blue carpet that mimics the waves of the sea. The ship's hull is painted a rich brown, with tiny windows. The carpet, soft and textured, provides a perfect backdrop, resembling an oceanic expanse. Surrounding the ship are various other toys and children's items, hinting at a playful environment. The scene captures the innocence and imagination of childhood, with the toy ship's journey symbolizing endless adventures in a whimsical, indoor setting.",
          "title": "Prompt",
          "description": "A text prompt describing the desired video."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "A text prompt describing what to avoid in the video."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 49,
          "title": "Num Frames",
          "description": "The number of frames in the video. Must be divisible by 8 + 1 (e.g., 49, 81, 113).",
          "min": 49.0,
          "max": 113.0
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 6.0,
          "title": "Guidance Scale",
          "description": "The scale for classifier-free guidance.",
          "min": 1.0,
          "max": 20.0
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 50,
          "title": "Num Inference Steps",
          "description": "The number of denoising steps.",
          "min": 1.0,
          "max": 100.0
        },
        {
          "name": "height",
          "type": {
            "type": "int"
          },
          "default": 480,
          "title": "Height",
          "description": "The height of the generated video in pixels.",
          "min": 256.0,
          "max": 1024.0
        },
        {
          "name": "width",
          "type": {
            "type": "int"
          },
          "default": 720,
          "title": "Width",
          "description": "The width of the generated video in pixels.",
          "min": 256.0,
          "max": 1024.0
        },
        {
          "name": "fps",
          "type": {
            "type": "int"
          },
          "default": 8,
          "title": "Fps",
          "description": "Frames per second for the output video.",
          "min": 1.0,
          "max": 30.0
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for the random number generator. Use -1 for a random seed.",
          "min": -1.0
        },
        {
          "name": "max_sequence_length",
          "type": {
            "type": "int"
          },
          "default": 226,
          "title": "Max Sequence Length",
          "description": "Maximum sequence length in encoded prompt.",
          "min": 1.0,
          "max": 512.0
        },
        {
          "name": "enable_cpu_offload",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Cpu Offload",
          "description": "Enable CPU offload to reduce VRAM usage."
        },
        {
          "name": "enable_vae_slicing",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Vae Slicing",
          "description": "Enable VAE slicing to reduce VRAM usage."
        },
        {
          "name": "enable_vae_tiling",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Vae Tiling",
          "description": "Enable VAE tiling to reduce VRAM usage for large videos."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "recommended_models": [
        {
          "type": "hf.text_to_video",
          "repo_id": "THUDM/CogVideoX-2b",
          "allow_patterns": [
            "**/*.safetensors",
            "**/*.json",
            "**/*.txt",
            "*.json"
          ]
        },
        {
          "type": "hf.text_to_video",
          "repo_id": "THUDM/CogVideoX-5b",
          "allow_patterns": [
            "**/*.safetensors",
            "**/*.json",
            "**/*.txt",
            "*.json"
          ]
        }
      ],
      "basic_fields": [
        "prompt",
        "num_frames",
        "height",
        "width"
      ]
    },
    {
      "title": "Stable Video Diffusion",
      "description": "Generates a video from a single image using the Stable Video Diffusion model.\n    video, generation, AI, image-to-video, stable-diffusion, SD\n\n    Use cases:\n    - Create short animations from static images\n    - Generate dynamic content for presentations or social media\n    - Prototype video ideas from still concept art",
      "namespace": "huggingface.text_to_video",
      "node_type": "huggingface.text_to_video.StableVideoDiffusion",
      "properties": [
        {
          "name": "input_image",
          "type": {
            "type": "image"
          },
          "default": {},
          "title": "Input Image",
          "description": "The input image to generate the video from, resized to 1024x576."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 14,
          "title": "Num Frames",
          "description": "Number of frames to generate.",
          "min": 1.0,
          "max": 50.0
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 25,
          "title": "Num Inference Steps",
          "description": "Number of steps per generated frame",
          "min": 1.0,
          "max": 100.0
        },
        {
          "name": "fps",
          "type": {
            "type": "int"
          },
          "default": 7,
          "title": "Fps",
          "description": "Frames per second for the output video.",
          "min": 1.0,
          "max": 30.0
        },
        {
          "name": "decode_chunk_size",
          "type": {
            "type": "int"
          },
          "default": 8,
          "title": "Decode Chunk Size",
          "description": "Number of frames to decode at once.",
          "min": 1.0,
          "max": 16.0
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": 42,
          "title": "Seed",
          "description": "Random seed for generation.",
          "min": 0.0,
          "max": 4294967295.0
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "recommended_models": [
        {
          "type": "hf.stable_diffusion",
          "repo_id": "stabilityai/stable-video-diffusion-img2vid-xt",
          "allow_patterns": [
            "**/*.fp16.safetensors",
            "**/*.json",
            "**/*.txt",
            "*.json"
          ]
        }
      ],
      "basic_fields": [
        "input_image",
        "num_frames",
        "num_inference_steps",
        "fps",
        "decode_chunk_size",
        "seed"
      ]
    },
    {
      "title": "Wan (Text-to-Video)",
      "description": "Generates videos from text prompts using Wan text-to-video pipeline.\n    video, generation, AI, text-to-video, diffusion, Wan\n\n    Use cases:\n    - Create high-quality videos from text descriptions\n    - Efficient 1.3B model for consumer GPUs or 14B for maximum quality",
      "namespace": "huggingface.text_to_video",
      "node_type": "huggingface.text_to_video.Wan_T2V",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "A robot standing on a mountain top at sunset, cinematic lighting, high detail",
          "title": "Prompt",
          "description": "A text prompt describing the desired video."
        },
        {
          "name": "model_variant",
          "type": {
            "type": "enum",
            "values": [
              "Wan-AI/Wan2.2-T2V-A14B-Diffusers",
              "Wan-AI/Wan2.1-T2V-14B-Diffusers",
              "Wan-AI/Wan2.2-TI2V-5B-Diffusers"
            ],
            "type_name": "nodetool.nodes.huggingface.text_to_video.WanModel"
          },
          "default": "Wan-AI/Wan2.2-T2V-A14B-Diffusers",
          "title": "Model Variant",
          "description": "Select the Wan model to use."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "A text prompt describing what to avoid in the video."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 49,
          "title": "Num Frames",
          "description": "The number of frames in the video.",
          "min": 16.0,
          "max": 129.0
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 5.0,
          "title": "Guidance Scale",
          "description": "The scale for classifier-free guidance.",
          "min": 1.0,
          "max": 20.0
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Num Inference Steps",
          "description": "The number of denoising steps.",
          "min": 1.0,
          "max": 100.0
        },
        {
          "name": "height",
          "type": {
            "type": "int"
          },
          "default": 480,
          "title": "Height",
          "description": "The height of the generated video in pixels.",
          "min": 256.0,
          "max": 1080.0
        },
        {
          "name": "width",
          "type": {
            "type": "int"
          },
          "default": 720,
          "title": "Width",
          "description": "The width of the generated video in pixels.",
          "min": 256.0,
          "max": 1920.0
        },
        {
          "name": "fps",
          "type": {
            "type": "int"
          },
          "default": 16,
          "title": "Fps",
          "description": "Frames per second for the output video.",
          "min": 1.0,
          "max": 60.0
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for the random number generator. Use -1 for a random seed.",
          "min": -1.0
        },
        {
          "name": "max_sequence_length",
          "type": {
            "type": "int"
          },
          "default": 512,
          "title": "Max Sequence Length",
          "description": "Maximum sequence length in encoded prompt.",
          "min": 64.0,
          "max": 1024.0
        },
        {
          "name": "enable_cpu_offload",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Cpu Offload",
          "description": "Enable CPU offload to reduce VRAM usage."
        },
        {
          "name": "enable_vae_slicing",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Vae Slicing",
          "description": "Enable VAE slicing to reduce VRAM usage."
        },
        {
          "name": "enable_vae_tiling",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Vae Tiling",
          "description": "Enable VAE tiling to reduce VRAM usage for large videos."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "recommended_models": [
        {
          "type": "hf.text_to_video",
          "repo_id": "Wan-AI/Wan2.2-T2V-A14B-Diffusers",
          "allow_patterns": [
            "**/*.safetensors",
            "**/*.json",
            "**/*.txt",
            "*.json"
          ]
        },
        {
          "type": "hf.text_to_video",
          "repo_id": "Wan-AI/Wan2.1-T2V-14B-Diffusers",
          "allow_patterns": [
            "**/*.safetensors",
            "**/*.json",
            "**/*.txt",
            "*.json"
          ]
        },
        {
          "type": "hf.text_to_video",
          "repo_id": "Wan-AI/Wan2.2-TI2V-5B-Diffusers",
          "allow_patterns": [
            "**/*.safetensors",
            "**/*.json",
            "**/*.txt",
            "*.json"
          ]
        }
      ],
      "basic_fields": [
        "prompt",
        "num_frames",
        "height",
        "width",
        "model_variant"
      ]
    },
    {
      "title": "Summarize",
      "description": "Summarizes text using a Hugging Face model.\n    text, summarization, AI, LLM",
      "namespace": "huggingface.summarization",
      "node_type": "huggingface.summarization.Summarize",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.text_generation"
          },
          "default": {},
          "title": "Model ID on Huggingface",
          "description": "The model ID to use for the text generation"
        },
        {
          "name": "inputs",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Inputs",
          "description": "The input text to summarize"
        },
        {
          "name": "max_length",
          "type": {
            "type": "int"
          },
          "default": 100,
          "title": "Max Length",
          "description": "The maximum length of the generated text"
        },
        {
          "name": "do_sample",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Do Sample",
          "description": "Whether to sample from the model"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "str"
          },
          "name": "output"
        }
      ],
      "recommended_models": [
        {
          "type": "hf.text_generation",
          "repo_id": "Falconsai/text_summarization",
          "allow_patterns": [
            "*.json",
            "*.txt",
            "*.safetensors"
          ]
        },
        {
          "type": "hf.text_generation",
          "repo_id": "Falconsai/medical_summarization",
          "allow_patterns": [
            "*.json",
            "*.txt",
            "*.safetensors"
          ]
        },
        {
          "type": "hf.text_generation",
          "repo_id": "imvladikon/het5_summarization",
          "allow_patterns": [
            "*.json",
            "*.txt",
            "*.safetensors"
          ]
        }
      ],
      "basic_fields": [
        "model",
        "inputs"
      ]
    },
    {
      "title": "Huggingface",
      "description": "",
      "namespace": "huggingface.huggingface_node",
      "node_type": "huggingface.huggingface_node.Huggingface",
      "outputs": [
        {
          "type": {
            "type": "any"
          },
          "name": "output"
        }
      ]
    },
    {
      "title": "Depth Estimation",
      "description": "Estimates depth from a single image.\n    image, depth estimation, 3D, huggingface\n\n    Use cases:\n    - Generate depth maps for 3D modeling\n    - Assist in augmented reality applications\n    - Enhance computer vision systems for robotics\n    - Improve scene understanding in autonomous vehicles",
      "namespace": "huggingface.depth_estimation",
      "node_type": "huggingface.depth_estimation.DepthEstimation",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.depth_estimation"
          },
          "default": {
            "repo_id": "LiheYoung/depth-anything-base-hf"
          },
          "title": "Model ID on Huggingface",
          "description": "The model ID to use for depth estimation"
        },
        {
          "name": "image",
          "type": {
            "type": "image"
          },
          "default": {},
          "title": "Image",
          "description": "The input image for depth estimation"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "recommended_models": [
        {
          "type": "hf.depth_estimation",
          "repo_id": "depth-anything/Depth-Anything-V2-Small-hf"
        },
        {
          "type": "hf.depth_estimation",
          "repo_id": "depth-anything/Depth-Anything-V2-Base-hf"
        },
        {
          "type": "hf.depth_estimation",
          "repo_id": "depth-anything/Depth-Anything-V2-Large-hf"
        },
        {
          "type": "hf.depth_estimation",
          "repo_id": "Intel/dpt-large",
          "allow_patterns": [
            "README.md",
            "*.safetensors",
            "*.json",
            "**/*.json",
            "txt"
          ]
        }
      ],
      "basic_fields": [
        "model",
        "image"
      ]
    },
    {
      "title": "Token Classification",
      "description": "Performs token classification tasks such as Named Entity Recognition (NER).\n    text, token classification, named entity recognition, natural language processing\n\n    Use cases:\n    - Named Entity Recognition in text\n    - Part-of-speech tagging\n    - Chunking and shallow parsing\n    - Information extraction from unstructured text",
      "namespace": "huggingface.token_classification",
      "node_type": "huggingface.token_classification.TokenClassification",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.token_classification"
          },
          "default": {},
          "title": "Model ID on Huggingface",
          "description": "The model ID to use for token classification"
        },
        {
          "name": "inputs",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Input Text",
          "description": "The input text for token classification"
        },
        {
          "name": "aggregation_strategy",
          "type": {
            "type": "enum",
            "values": [
              "simple",
              "first",
              "average",
              "max"
            ],
            "type_name": "nodetool.nodes.huggingface.token_classification.AggregationStrategy"
          },
          "default": "simple",
          "title": "Aggregation Strategy",
          "description": "Strategy to aggregate tokens into entities"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dataframe"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "model",
        "inputs",
        "aggregation_strategy"
      ]
    },
    {
      "title": "Find Segment",
      "description": "Extracts a specific segment from a list of segmentation masks.\n    image, segmentation, object detection, mask",
      "namespace": "huggingface.image_segmentation",
      "node_type": "huggingface.image_segmentation.FindSegment",
      "properties": [
        {
          "name": "segments",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "image_segmentation_result"
              }
            ]
          },
          "default": {},
          "title": "Segmentation Masks",
          "description": "The segmentation masks to search"
        },
        {
          "name": "segment_label",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Label",
          "description": "The label of the segment to extract"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "segments",
        "segment_label"
      ]
    },
    {
      "title": "SAM2 Segmentation",
      "description": "Performs semantic segmentation on images using SAM2 (Segment Anything Model 2).\n    image, segmentation, object detection, scene parsing, mask\n\n    Use cases:\n    - Automatic segmentation of objects in images\n    - Instance segmentation for computer vision tasks\n    - Interactive segmentation with point prompts\n    - Scene understanding and object detection",
      "namespace": "huggingface.image_segmentation",
      "node_type": "huggingface.image_segmentation.SAM2Segmentation",
      "properties": [
        {
          "name": "image",
          "type": {
            "type": "image"
          },
          "default": {},
          "title": "Input Image",
          "description": "The input image to segment"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "image"
              }
            ]
          },
          "name": "output"
        }
      ],
      "recommended_models": [
        {
          "repo_id": "facebook/sam2-hiera-large"
        }
      ],
      "basic_fields": [
        "image"
      ]
    },
    {
      "title": "Image Segmentation",
      "description": "Performs semantic segmentation on images, identifying and labeling different regions.\n    image, segmentation, object detection, scene parsing\n\n    Use cases:\n    - Segmenting objects in images\n    - Segmenting facial features in images",
      "namespace": "huggingface.image_segmentation",
      "node_type": "huggingface.image_segmentation.Segmentation",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.image_segmentation"
          },
          "default": {
            "repo_id": "nvidia/segformer-b3-finetuned-ade-512-512"
          },
          "title": "Model ID on Huggingface",
          "description": "The model ID to use for the segmentation"
        },
        {
          "name": "image",
          "type": {
            "type": "image"
          },
          "default": {},
          "title": "Image",
          "description": "The input image to segment"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "image_segmentation_result"
              }
            ]
          },
          "name": "output"
        }
      ],
      "recommended_models": [
        {
          "type": "hf.image_segmentation",
          "repo_id": "nvidia/segformer-b3-finetuned-ade-512-512",
          "allow_patterns": [
            "README.md",
            "*.bin",
            "*.json",
            "**/*.json"
          ]
        },
        {
          "type": "hf.image_segmentation",
          "repo_id": "mattmdjaga/segformer_b2_clothes",
          "allow_patterns": [
            "README.md",
            "*.bin",
            "*.json",
            "**/*.json"
          ]
        }
      ],
      "basic_fields": [
        "model",
        "image"
      ]
    },
    {
      "title": "Visualize Segmentation",
      "description": "Visualizes segmentation masks on images with labels.\n    image, segmentation, visualization, mask\n\n    Use cases:\n    - Visualize results of image segmentation models\n    - Analyze and compare different segmentation techniques\n    - Create labeled images for presentations or reports",
      "namespace": "huggingface.image_segmentation",
      "node_type": "huggingface.image_segmentation.VisualizeSegmentation",
      "properties": [
        {
          "name": "image",
          "type": {
            "type": "image"
          },
          "default": {},
          "title": "Image",
          "description": "The input image to visualize"
        },
        {
          "name": "segments",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "image_segmentation_result"
              }
            ]
          },
          "default": [],
          "title": "Segmentation Masks",
          "description": "The segmentation masks to visualize"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "segments"
      ]
    },
    {
      "title": "Image Classifier",
      "description": "Classifies images into predefined categories.\n    image, classification, labeling, categorization\n\n    Use cases:\n    - Content moderation by detecting inappropriate images\n    - Organizing photo libraries by automatically tagging images",
      "namespace": "huggingface.image_classification",
      "node_type": "huggingface.image_classification.ImageClassifier",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.image_classification"
          },
          "default": {},
          "title": "Model ID on Huggingface",
          "description": "The model ID to use for the classification"
        },
        {
          "name": "image",
          "type": {
            "type": "image"
          },
          "default": {},
          "title": "Image",
          "description": "The input image to classify"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "float"
              }
            ]
          },
          "name": "output"
        }
      ],
      "recommended_models": [
        {
          "type": "hf.image_classification",
          "repo_id": "google/vit-base-patch16-224",
          "allow_patterns": [
            "README.md",
            "*.safetensors",
            "*.json",
            "**/*.json"
          ]
        },
        {
          "type": "hf.image_classification",
          "repo_id": "microsoft/resnet-50",
          "allow_patterns": [
            "README.md",
            "*.safetensors",
            "*.json",
            "**/*.json"
          ]
        },
        {
          "type": "hf.image_classification",
          "repo_id": "microsoft/resnet-18",
          "allow_patterns": [
            "README.md",
            "*.safetensors",
            "*.json",
            "**/*.json"
          ]
        },
        {
          "type": "hf.image_classification",
          "repo_id": "apple/mobilevit-small",
          "allow_patterns": [
            "README.md",
            "*.bin",
            "*.json",
            "**/*.json"
          ]
        },
        {
          "type": "hf.image_classification",
          "repo_id": "apple/mobilevit-xx-small",
          "allow_patterns": [
            "README.md",
            "*.bin",
            "*.json",
            "**/*.json"
          ]
        },
        {
          "type": "hf.image_classification",
          "repo_id": "nateraw/vit-age-classifier",
          "allow_patterns": [
            "README.md",
            "*.safetensors",
            "*.json",
            "**/*.json"
          ]
        },
        {
          "type": "hf.image_classification",
          "repo_id": "Falconsai/nsfw_image_detection",
          "allow_patterns": [
            "README.md",
            "*.safetensors",
            "*.json",
            "**/*.json"
          ]
        },
        {
          "type": "hf.image_classification",
          "repo_id": "rizvandwiki/gender-classification-2",
          "allow_patterns": [
            "README.md",
            "*.safetensors",
            "*.json",
            "**/*.json"
          ]
        }
      ],
      "basic_fields": [
        "model",
        "image"
      ]
    },
    {
      "title": "Zero-Shot Image Classifier",
      "description": "Classifies images into categories without the need for training data.\n    image, classification, labeling, categorization\n\n    Use cases:\n    - Quickly categorize images without training data\n    - Identify objects in images without predefined labels\n    - Automate image tagging for large datasets",
      "namespace": "huggingface.image_classification",
      "node_type": "huggingface.image_classification.ZeroShotImageClassifier",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.zero_shot_image_classification"
          },
          "default": {},
          "title": "Model ID on Huggingface",
          "description": "The model ID to use for the classification"
        },
        {
          "name": "image",
          "type": {
            "type": "image"
          },
          "default": {},
          "title": "Image",
          "description": "The input image to classify"
        },
        {
          "name": "candidate_labels",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Candidate Labels",
          "description": "The candidate labels to classify the image against, separated by commas"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "float"
              }
            ]
          },
          "name": "output"
        }
      ],
      "recommended_models": [
        {
          "type": "hf.zero_shot_image_classification",
          "repo_id": "openai/clip-vit-base-patch16",
          "allow_patterns": [
            "README.md",
            "pytorch_model.bin",
            "*.json",
            "*.txt"
          ]
        },
        {
          "type": "hf.zero_shot_image_classification",
          "repo_id": "openai/clip-vit-base-patch32",
          "allow_patterns": [
            "README.md",
            "pytorch_model.bin",
            "*.json",
            "*.txt"
          ]
        },
        {
          "type": "hf.zero_shot_image_classification",
          "repo_id": "openai/clip-vit-base-patch14",
          "allow_patterns": [
            "README.md",
            "pytorch_model.bin",
            "*.json",
            "*.txt"
          ]
        },
        {
          "type": "hf.zero_shot_image_classification",
          "repo_id": "patricjohncyh/fashion-clip",
          "allow_patterns": [
            "README.md",
            "pytorch_model.bin",
            "*.json",
            "*.txt"
          ]
        },
        {
          "type": "hf.zero_shot_image_classification",
          "repo_id": "laion/CLIP-ViT-H-14-laion2B-s32B-b79K",
          "allow_patterns": [
            "README.md",
            "pytorch_model.bin",
            "*.json",
            "*.txt"
          ]
        },
        {
          "type": "hf.zero_shot_image_classification",
          "repo_id": "laion/CLIP-ViT-g-14-laion2B-s12B-b42K",
          "allow_patterns": [
            "README.md",
            "pytorch_model.bin",
            "*.json",
            "*.txt"
          ]
        }
      ],
      "basic_fields": [
        "model",
        "image",
        "candidate_labels"
      ]
    },
    {
      "title": "Hugging Face Pipeline",
      "description": "",
      "namespace": "huggingface.huggingface_pipeline",
      "node_type": "huggingface.huggingface_pipeline.HuggingFacePipeline",
      "outputs": [
        {
          "type": {
            "type": "any"
          },
          "name": "output"
        }
      ]
    },
    {
      "title": "Reranker",
      "description": "Reranks pairs of text based on their semantic similarity.\n    text, ranking, reranking, natural language processing\n\n    Use cases:\n    - Improve search results ranking\n    - Question-answer pair scoring\n    - Document relevance ranking",
      "namespace": "huggingface.ranking",
      "node_type": "huggingface.ranking.Reranker",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.reranker"
          },
          "default": {},
          "title": "Model ID on Huggingface",
          "description": "The model ID to use for reranking"
        },
        {
          "name": "query",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Query Text",
          "description": "The query text to compare against candidates"
        },
        {
          "name": "candidates",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Candidate Texts",
          "description": "List of candidate texts to rank"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "float"
              }
            ]
          },
          "name": "output"
        }
      ],
      "recommended_models": [
        {
          "type": "hf.reranker",
          "repo_id": "BAAI/bge-reranker-v2-m3",
          "allow_patterns": [
            "*.safetensors",
            "*.txt",
            "*.json"
          ]
        },
        {
          "type": "hf.reranker",
          "repo_id": "BAAI/bge-reranker-base",
          "allow_patterns": [
            "*.safetensors",
            "*.txt",
            "*.json"
          ]
        },
        {
          "type": "hf.reranker",
          "repo_id": "BAAI/bge-reranker-large",
          "allow_patterns": [
            "*.safetensors",
            "*.txt",
            "*.json"
          ]
        }
      ],
      "basic_fields": [
        "model",
        "query",
        "candidates"
      ]
    },
    {
      "title": "Text Classifier",
      "description": "Classifies text into predefined categories using a Hugging Face model.\n    text, classification, zero-shot, natural language processing",
      "namespace": "huggingface.text_classification",
      "node_type": "huggingface.text_classification.TextClassifier",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.text_classification"
          },
          "default": {},
          "title": "Model ID on Huggingface",
          "description": "The model ID to use for the classification"
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Inputs",
          "description": "The input text to the model"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "float"
              }
            ]
          },
          "name": "output"
        }
      ],
      "recommended_models": [
        {
          "type": "hf.text_classification",
          "repo_id": "cardiffnlp/twitter-roberta-base-sentiment-latest",
          "allow_patterns": [
            "*.json",
            "*.txt",
            "*.bin"
          ]
        },
        {
          "type": "hf.text_classification",
          "repo_id": "michellejieli/emotion_text_classifier",
          "allow_patterns": [
            "*.json",
            "*.txt",
            "*.bin"
          ]
        }
      ],
      "basic_fields": [
        "model",
        "prompt"
      ]
    },
    {
      "title": "Zero Shot Text Classifier",
      "description": "Performs zero-shot classification on text.\n    text, classification, zero-shot, natural language processing\n\n    Use cases:\n    - Classify text into custom categories without training\n    - Topic detection in documents\n    - Sentiment analysis with custom sentiment labels\n    - Intent classification in conversational AI",
      "namespace": "huggingface.text_classification",
      "node_type": "huggingface.text_classification.ZeroShotTextClassifier",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.zero_shot_classification"
          },
          "default": {},
          "title": "Model ID on Huggingface",
          "description": "The model ID to use for zero-shot classification"
        },
        {
          "name": "inputs",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Input Text",
          "description": "The text to classify"
        },
        {
          "name": "candidate_labels",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Candidate Labels",
          "description": "Comma-separated list of candidate labels for classification"
        },
        {
          "name": "multi_label",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Multi-label Classification",
          "description": "Whether to perform multi-label classification"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "float"
              }
            ]
          },
          "name": "output"
        }
      ],
      "recommended_models": [
        {
          "type": "hf.zero_shot_classification",
          "repo_id": "facebook/bart-large-mnli",
          "allow_patterns": [
            "*.json",
            "*.txt",
            "*.safetensors"
          ]
        },
        {
          "type": "hf.zero_shot_classification",
          "repo_id": "MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli",
          "allow_patterns": [
            "*.json",
            "*.txt",
            "*.safetensors"
          ]
        },
        {
          "type": "hf.zero_shot_classification",
          "repo_id": "MoritzLaurer/mDeBERTa-v3-base-mnli-xnli",
          "allow_patterns": [
            "*.json",
            "*.txt",
            "*.safetensors"
          ]
        },
        {
          "type": "hf.zero_shot_classification",
          "repo_id": "tasksource/ModernBERT-base-nli",
          "allow_patterns": [
            "*.json",
            "*.txt",
            "*.safetensors"
          ]
        },
        {
          "type": "hf.zero_shot_classification",
          "repo_id": "cross-encoder/nli-deberta-v3-base",
          "allow_patterns": [
            "*.json",
            "*.txt",
            "*.safetensors"
          ]
        },
        {
          "type": "hf.zero_shot_classification",
          "repo_id": "microsoft/deberta-v2-xlarge-mnli",
          "allow_patterns": [
            "*.json",
            "*.txt",
            "*.safetensors"
          ]
        },
        {
          "type": "hf.zero_shot_classification",
          "repo_id": "roberta-large-mnli",
          "allow_patterns": [
            "*.json",
            "*.txt",
            "*.safetensors"
          ]
        }
      ],
      "basic_fields": [
        "model",
        "inputs",
        "candidate_labels",
        "multi_label"
      ]
    },
    {
      "title": "Chunks To SRT",
      "description": "Convert audio chunks to SRT (SubRip Subtitle) format\n    subtitle, srt, whisper, transcription\n\n    **Use Cases:**\n    - Generate subtitles for videos\n    - Create closed captions from audio transcriptions\n    - Convert speech-to-text output to a standardized subtitle format\n\n    **Features:**\n    - Converts Whisper audio chunks to SRT format\n    - Supports customizable time offset\n    - Generates properly formatted SRT file content",
      "namespace": "huggingface.automatic_speech_recognition",
      "node_type": "huggingface.automatic_speech_recognition.ChunksToSRT",
      "properties": [
        {
          "name": "chunks",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "audio_chunk"
              }
            ]
          },
          "default": [],
          "title": "Audio Chunks",
          "description": "List of audio chunks from Whisper transcription"
        },
        {
          "name": "time_offset",
          "type": {
            "type": "float"
          },
          "default": 0.0,
          "title": "Time Offset",
          "description": "Time offset in seconds to apply to all timestamps"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "str"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "chunks",
        "time_offset"
      ]
    },
    {
      "title": "Whisper",
      "description": "Convert speech to text\n    asr, automatic-speech-recognition, speech-to-text, translate, transcribe, audio, huggingface\n\n    **Use Cases:**\n    - Voice input for a chatbot\n    - Transcribe or translate audio files\n    - Create subtitles for videos\n\n    **Features:**\n    - Multilingual speech recognition\n    - Speech translation\n    - Language identification\n\n    **Note**\n    - Language selection is sorted by word error rate in the FLEURS benchmark\n    - There are many variants of Whisper that are optimized for different use cases.\n\n    **Links:**\n    - https://github.com/openai/whisper\n    - https://platform.openai.com/docs/guides/speech-to-text/supported-languages",
      "namespace": "huggingface.automatic_speech_recognition",
      "node_type": "huggingface.automatic_speech_recognition.Whisper",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.automatic_speech_recognition"
          },
          "default": {},
          "title": "Model ID on Huggingface",
          "description": "The model ID to use for the speech recognition."
        },
        {
          "name": "audio",
          "type": {
            "type": "audio"
          },
          "default": {},
          "title": "Audio Input",
          "description": "The input audio to transcribe."
        },
        {
          "name": "task",
          "type": {
            "type": "enum",
            "values": [
              "transcribe",
              "translate"
            ],
            "type_name": "nodetool.nodes.huggingface.automatic_speech_recognition.Task"
          },
          "default": "transcribe",
          "title": "Task",
          "description": "The task to perform: 'transcribe' for speech-to-text or 'translate' for speech translation."
        },
        {
          "name": "language",
          "type": {
            "type": "enum",
            "values": [
              "auto_detect",
              "spanish",
              "italian",
              "korean",
              "portuguese",
              "english",
              "japanese",
              "german",
              "russian",
              "dutch",
              "polish",
              "catalan",
              "french",
              "indonesian",
              "ukrainian",
              "turkish",
              "malay",
              "swedish",
              "mandarin",
              "finnish",
              "norwegian",
              "romanian",
              "thai",
              "vietnamese",
              "slovak",
              "arabic",
              "czech",
              "croatian",
              "greek",
              "serbian",
              "danish",
              "bulgarian",
              "hungarian",
              "filipino",
              "bosnian",
              "galician",
              "macedonian",
              "hindi",
              "estonian",
              "slovenian",
              "tamil",
              "latvian",
              "azerbaijani",
              "urdu",
              "lithuanian",
              "hebrew",
              "welsh",
              "persian",
              "icelandic",
              "kazakh",
              "afrikaans",
              "kannada",
              "marathi",
              "swahili",
              "telugu",
              "maori",
              "nepali",
              "armenian",
              "belarusian",
              "gujarati",
              "punjabi",
              "bengali"
            ],
            "type_name": "nodetool.nodes.huggingface.automatic_speech_recognition.WhisperLanguage"
          },
          "default": "auto_detect",
          "title": "Language",
          "description": "The language of the input audio. If not specified, the model will attempt to detect it automatically."
        },
        {
          "name": "timestamps",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "word",
              "sentence"
            ],
            "type_name": "nodetool.nodes.huggingface.automatic_speech_recognition.Timestamps"
          },
          "default": "none",
          "title": "Timestamps",
          "description": "The type of timestamps to return for the generated text."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "str"
          },
          "name": "text"
        },
        {
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "audio_chunk"
              }
            ]
          },
          "name": "chunks"
        }
      ],
      "recommended_models": [
        {
          "type": "hf.automatic_speech_recognition",
          "repo_id": "openai/whisper-large-v3",
          "allow_patterns": [
            "model.safetensors",
            "*.json",
            "*.txt"
          ]
        },
        {
          "type": "hf.automatic_speech_recognition",
          "repo_id": "openai/whisper-large-v3-turbo",
          "allow_patterns": [
            "model.safetensors",
            "*.json",
            "*.txt"
          ]
        },
        {
          "type": "hf.automatic_speech_recognition",
          "repo_id": "openai/whisper-large-v2",
          "allow_patterns": [
            "model.safetensors",
            "*.json",
            "*.txt"
          ]
        },
        {
          "type": "hf.automatic_speech_recognition",
          "repo_id": "openai/whisper-medium",
          "allow_patterns": [
            "model.safetensors",
            "*.json",
            "*.txt"
          ]
        },
        {
          "type": "hf.automatic_speech_recognition",
          "repo_id": "openai/whisper-small",
          "allow_patterns": [
            "model.safetensors",
            "*.json",
            "*.txt"
          ]
        },
        {
          "type": "hf.automatic_speech_recognition",
          "repo_id": "Systran/faster-whisper-large-v3",
          "allow_patterns": [
            "model.bin",
            "*.json",
            "*.txt"
          ]
        }
      ],
      "basic_fields": [
        "model",
        "audio",
        "task"
      ]
    },
    {
      "title": "Bark",
      "description": "Bark is a text-to-audio model created by Suno. Bark can generate highly realistic, multilingual speech as well as other audio - including music, background noise and simple sound effects. The model can also produce nonverbal communications like laughing, sighing and crying.\n    tts, audio, speech, huggingface\n\n    Use cases:\n    - Create voice content for apps and websites\n    - Develop voice assistants with natural-sounding speech\n    - Generate automated announcements for public spaces",
      "namespace": "huggingface.text_to_speech",
      "node_type": "huggingface.text_to_speech.Bark",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.text_to_speech"
          },
          "default": {},
          "title": "Model ID on Huggingface",
          "description": "The model ID to use for the image generation"
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Inputs",
          "description": "The input text to the model"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "recommended_models": [
        {
          "type": "hf.text_to_speech",
          "repo_id": "suno/bark",
          "allow_patterns": [
            "*.bin",
            "*.json",
            "*.txt"
          ]
        },
        {
          "type": "hf.text_to_speech",
          "repo_id": "suno/bark-small",
          "allow_patterns": [
            "*.bin",
            "*.json",
            "*.txt"
          ]
        }
      ],
      "basic_fields": [
        "model",
        "prompt"
      ]
    },
    {
      "title": "Kokoro TTS",
      "description": "Kokoro is an open-weight, fast, and lightweight TTS model (~82M params) with Apache-2.0 weights.\n    It supports multiple languages via `misaki` and provides high-quality speech with selectable voices.\n    tts, audio, speech, huggingface, kokoro\n\n    Reference: https://huggingface.co/hexgrad/Kokoro-82M\n\n    Use cases:\n    - Natural-sounding speech synthesis for apps, assistants, and narration\n    - Low-latency TTS in production or local projects\n    - Multi-language TTS with configurable voices and speed",
      "namespace": "huggingface.text_to_speech",
      "node_type": "huggingface.text_to_speech.KokoroTTS",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.text_to_speech"
          },
          "default": {
            "repo_id": "hexgrad/Kokoro-82M"
          },
          "title": "Model ID on Hugging Face",
          "description": "The Kokoro repo to use (e.g., hexgrad/Kokoro-82M)"
        },
        {
          "name": "text",
          "type": {
            "type": "str"
          },
          "default": "Hello from Kokoro.",
          "title": "Text",
          "description": "Input text to synthesize"
        },
        {
          "name": "lang_code",
          "type": {
            "type": "enum",
            "values": [
              "a",
              "b",
              "e",
              "f",
              "h",
              "i",
              "p",
              "j",
              "z",
              "k",
              "r",
              "t",
              "v",
              "a",
              "g",
              "p",
              "r",
              "u"
            ],
            "type_name": "nodetool.nodes.huggingface.text_to_speech.LanguageCode"
          },
          "default": "a",
          "title": "Language Code",
          "description": "Language code for G2P. Examples: 'a' (American English), 'b' (British English), 'e' (es), 'f' (fr-fr), 'h' (hi), 'i' (it), 'p' (pt-br), 'j' (ja), 'z' (zh)."
        },
        {
          "name": "voice",
          "type": {
            "type": "enum",
            "values": [
              "af_alloy",
              "af_aoede",
              "af_bella",
              "af_heart",
              "af_jessica",
              "af_kore",
              "af_nicole",
              "af_nova",
              "af_river",
              "af_sarah",
              "af_sky",
              "am_adam",
              "am_echo",
              "am_eric",
              "am_fenrir",
              "am_liam",
              "am_michael",
              "am_onyx",
              "am_puck",
              "am_santa",
              "bf_alice",
              "bf_emma",
              "bf_isabella",
              "bf_lily",
              "bm_daniel",
              "bm_fable",
              "bm_george",
              "bm_lewis",
              "ef_dora",
              "em_alex",
              "em_santa",
              "ff_siwis",
              "hf_alpha",
              "hf_beta",
              "hm_omega",
              "hm_psi",
              "if_sara",
              "im_nicola",
              "jf_alpha",
              "jf_gongitsune",
              "jf_nezumi",
              "jf_tebukuro",
              "jm_kumo",
              "pf_dora",
              "pm_alex",
              "pm_santa",
              "zf_xiaobei",
              "zf_xiaoni",
              "zf_xiaoxiao",
              "zf_xiaoyi"
            ],
            "type_name": "nodetool.nodes.huggingface.text_to_speech.Voice"
          },
          "default": "af_heart",
          "title": "Voice",
          "description": "Voice name (see VOICES.md on the model page). Examples: af_heart, af_bella, af_jessica."
        },
        {
          "name": "speed",
          "type": {
            "type": "float"
          },
          "default": 1.0,
          "title": "Speed",
          "description": "Speech speed multiplier (0.5\u20132.0)",
          "min": 0.5,
          "max": 2.0
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "recommended_models": [
        {
          "type": "hf.text_to_speech",
          "repo_id": "hexgrad/Kokoro-82M",
          "allow_patterns": [
            "*.json",
            "*.pth",
            "voices/*.pt"
          ]
        }
      ],
      "basic_fields": [
        "model",
        "text",
        "lang_code",
        "voice",
        "speed"
      ]
    },
    {
      "title": "Text To Speech",
      "description": "A generic Text-to-Speech node that can work with various Hugging Face TTS models.\n    tts, audio, speech, huggingface, speak, voice\n\n    Use cases:\n    - Generate speech from text for various applications\n    - Create voice content for apps, websites, or virtual assistants\n    - Produce audio narrations for videos, presentations, or e-learning content",
      "namespace": "huggingface.text_to_speech",
      "node_type": "huggingface.text_to_speech.TextToSpeech",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.text_to_speech"
          },
          "default": {},
          "title": "Model ID on Huggingface",
          "description": "The model ID to use for text-to-speech generation"
        },
        {
          "name": "text",
          "type": {
            "type": "str"
          },
          "default": "Hello, this is a test of the text-to-speech system.",
          "title": "Input Text",
          "description": "The text to convert to speech"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "recommended_models": [
        {
          "type": "hf.text_to_speech",
          "repo_id": "facebook/mms-tts-eng",
          "allow_patterns": [
            "*.bin",
            "*.json",
            "*.txt"
          ]
        },
        {
          "type": "hf.text_to_speech",
          "repo_id": "facebook/mms-tts-kor",
          "allow_patterns": [
            "*.bin",
            "*.json",
            "*.txt"
          ]
        },
        {
          "type": "hf.text_to_speech",
          "repo_id": "facebook/mms-tts-fra",
          "allow_patterns": [
            "*.bin",
            "*.json",
            "*.txt"
          ]
        },
        {
          "type": "hf.text_to_speech",
          "repo_id": "facebook/mms-tts-deu",
          "allow_patterns": [
            "*.bin",
            "*.json",
            "*.txt"
          ]
        }
      ],
      "basic_fields": [
        "model",
        "text"
      ]
    },
    {
      "title": "Object Detection",
      "description": "Detects and localizes objects in images.\n    image, object detection, bounding boxes, huggingface\n\n    Use cases:\n    - Identify and count objects in images\n    - Locate specific items in complex scenes\n    - Assist in autonomous vehicle vision systems\n    - Enhance security camera footage analysis",
      "namespace": "huggingface.object_detection",
      "node_type": "huggingface.object_detection.ObjectDetection",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.object_detection"
          },
          "default": {
            "repo_id": "facebook/detr-resnet-50"
          },
          "title": "Model ID on Huggingface",
          "description": "The model ID to use for object detection"
        },
        {
          "name": "image",
          "type": {
            "type": "image"
          },
          "default": {},
          "title": "Inputs",
          "description": "The input image for object detection"
        },
        {
          "name": "threshold",
          "type": {
            "type": "float"
          },
          "default": 0.9,
          "title": "Confidence Threshold",
          "description": "Minimum confidence score for detected objects"
        },
        {
          "name": "top_k",
          "type": {
            "type": "int"
          },
          "default": 5,
          "title": "Top K",
          "description": "The number of top predictions to return"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "object_detection_result"
              }
            ]
          },
          "name": "output"
        }
      ],
      "recommended_models": [
        {
          "type": "hf.object_detection",
          "repo_id": "facebook/detr-resnet-50",
          "allow_patterns": [
            "README.md",
            "*.bin",
            "*.json",
            "**/*.json"
          ]
        },
        {
          "type": "hf.object_detection",
          "repo_id": "facebook/detr-resnet-101",
          "allow_patterns": [
            "README.md",
            "*.safetensors",
            "*.json",
            "**/*.json"
          ]
        },
        {
          "type": "hf.object_detection",
          "repo_id": "hustvl/yolos-tiny",
          "allow_patterns": [
            "README.md",
            "*.safetensors",
            "*.json",
            "**/*.json"
          ]
        },
        {
          "type": "hf.object_detection",
          "repo_id": "hustvl/yolos-small",
          "allow_patterns": [
            "README.md",
            "*.safetensors",
            "*.json",
            "**/*.json"
          ]
        },
        {
          "type": "hf.object_detection",
          "repo_id": "microsoft/table-transformer-detection",
          "allow_patterns": [
            "README.md",
            "*.bin",
            "*.json",
            "**/*.json"
          ]
        },
        {
          "type": "hf.object_detection",
          "repo_id": "microsoft/table-transformer-structure-recognition-v1.1-all",
          "allow_patterns": [
            "README.md",
            "*.bin",
            "*.json",
            "**/*.json"
          ]
        },
        {
          "type": "hf.object_detection",
          "repo_id": "valentinafeve/yolos-fashionpedia",
          "allow_patterns": [
            "README.md",
            "*.bin",
            "*.json",
            "**/*.json"
          ]
        }
      ],
      "basic_fields": [
        "model",
        "image"
      ]
    },
    {
      "title": "Visualize Object Detection",
      "description": "Visualizes object detection results on images.\n    image, object detection, bounding boxes, visualization, mask",
      "namespace": "huggingface.object_detection",
      "node_type": "huggingface.object_detection.VisualizeObjectDetection",
      "properties": [
        {
          "name": "image",
          "type": {
            "type": "image"
          },
          "default": {},
          "title": "Image",
          "description": "The input image to visualize"
        },
        {
          "name": "objects",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "object_detection_result"
              }
            ]
          },
          "default": {},
          "title": "Detected Objects",
          "description": "The detected objects to visualize"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "objects"
      ]
    },
    {
      "title": "Zero-Shot Object Detection",
      "description": "Detects objects in images without the need for training data.\n    image, object detection, bounding boxes, zero-shot, mask\n\n    Use cases:\n    - Quickly detect objects in images without training data\n    - Identify objects in images without predefined labels\n    - Automate object detection for large datasets",
      "namespace": "huggingface.object_detection",
      "node_type": "huggingface.object_detection.ZeroShotObjectDetection",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.zero_shot_object_detection"
          },
          "default": {
            "repo_id": "google/owlv2-base-patch16"
          },
          "title": "Model ID on Huggingface",
          "description": "The model ID to use for object detection"
        },
        {
          "name": "image",
          "type": {
            "type": "image"
          },
          "default": {},
          "title": "Inputs",
          "description": "The input image for object detection"
        },
        {
          "name": "threshold",
          "type": {
            "type": "float"
          },
          "default": 0.1,
          "title": "Confidence Threshold",
          "description": "Minimum confidence score for detected objects"
        },
        {
          "name": "top_k",
          "type": {
            "type": "int"
          },
          "default": 5,
          "title": "Top K",
          "description": "The number of top predictions to return"
        },
        {
          "name": "candidate_labels",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Candidate Labels",
          "description": "The candidate labels to detect in the image, separated by commas"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "object_detection_result"
              }
            ]
          },
          "name": "output"
        }
      ],
      "recommended_models": [
        {
          "type": "hf.zero_shot_object_detection",
          "repo_id": "google/owlvit-base-patch32",
          "allow_patterns": [
            "README.md",
            "*.bin",
            "*.json",
            "**/*.json",
            "txt"
          ]
        },
        {
          "type": "hf.zero_shot_object_detection",
          "repo_id": "google/owlvit-large-patch14",
          "allow_patterns": [
            "README.md",
            "*.bin",
            "*.json",
            "**/*.json",
            "txt"
          ]
        },
        {
          "type": "hf.zero_shot_object_detection",
          "repo_id": "google/owlvit-base-patch16",
          "allow_patterns": [
            "README.md",
            "*.bin",
            "*.json",
            "**/*.json",
            "txt"
          ]
        },
        {
          "type": "hf.zero_shot_object_detection",
          "repo_id": "google/owlv2-base-patch16",
          "allow_patterns": [
            "README.md",
            "*.bin",
            "*.json",
            "**/*.json",
            "txt"
          ]
        },
        {
          "type": "hf.zero_shot_object_detection",
          "repo_id": "google/owlv2-base-patch16-ensemble",
          "allow_patterns": [
            "README.md",
            "*.bin",
            "*.json",
            "**/*.json",
            "txt"
          ]
        },
        {
          "type": "hf.zero_shot_object_detection",
          "repo_id": "IDEA-Research/grounding-dino-tiny",
          "allow_patterns": [
            "README.md",
            "*.bin",
            "*.json",
            "**/*.json",
            "txt"
          ]
        }
      ],
      "basic_fields": [
        "model",
        "image",
        "threshold",
        "top_k",
        "candidate_labels"
      ]
    },
    {
      "title": "Audio Classifier",
      "description": "Classifies audio into predefined categories.\n    audio, classification, labeling, categorization\n\n    Use cases:\n    - Classify music genres\n    - Detect speech vs. non-speech audio\n    - Identify environmental sounds\n    - Emotion recognition in speech\n\n    Recommended models\n    - MIT/ast-finetuned-audioset-10-10-0.4593\n    - ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition",
      "namespace": "huggingface.audio_classification",
      "node_type": "huggingface.audio_classification.AudioClassifier",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.audio_classification"
          },
          "default": {},
          "title": "Model ID on Huggingface",
          "description": "The model ID to use for audio classification"
        },
        {
          "name": "audio",
          "type": {
            "type": "audio"
          },
          "default": {},
          "title": "Audio",
          "description": "The input audio to classify"
        },
        {
          "name": "top_k",
          "type": {
            "type": "int"
          },
          "default": 10,
          "title": "Top K",
          "description": "The number of top results to return"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "float"
              }
            ]
          },
          "name": "output"
        }
      ],
      "recommended_models": [
        {
          "type": "hf.audio_classification",
          "repo_id": "MIT/ast-finetuned-audioset-10-10-0.4593",
          "allow_patterns": [
            "*.safetensors",
            "*.json"
          ]
        },
        {
          "type": "hf.audio_classification",
          "repo_id": "ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition",
          "allow_patterns": [
            "pytorch_model.bin",
            "*.json"
          ]
        }
      ],
      "basic_fields": [
        "model",
        "audio",
        "top_k"
      ]
    },
    {
      "title": "Zero Shot Audio Classifier",
      "description": "Classifies audio into categories without the need for training data.\n    audio, classification, labeling, categorization, zero-shot\n\n    Use cases:\n    - Quickly categorize audio without training data\n    - Identify sounds or music genres without predefined labels\n    - Automate audio tagging for large datasets",
      "namespace": "huggingface.audio_classification",
      "node_type": "huggingface.audio_classification.ZeroShotAudioClassifier",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.zero_shot_audio_classification"
          },
          "default": {},
          "title": "Model ID on Huggingface",
          "description": "The model ID to use for the classification"
        },
        {
          "name": "audio",
          "type": {
            "type": "audio"
          },
          "default": {},
          "title": "Audio",
          "description": "The input audio to classify"
        },
        {
          "name": "candidate_labels",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Candidate Labels",
          "description": "The candidate labels to classify the audio against, separated by commas"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "float"
              }
            ]
          },
          "name": "output"
        }
      ],
      "recommended_models": [
        {
          "type": "hf.zero_shot_audio_classification",
          "repo_id": "laion/clap-htsat-unfused",
          "allow_patterns": [
            "model.safetensors",
            "*.json",
            "*.txt"
          ]
        }
      ],
      "basic_fields": [
        "model",
        "audio",
        "candidate_labels"
      ]
    },
    {
      "title": "Stable Diffusion Base",
      "description": "",
      "namespace": "huggingface.stable_diffusion_base",
      "node_type": "huggingface.stable_diffusion_base.StableDiffusionBase",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.stable_diffusion"
          },
          "default": {},
          "title": "Model",
          "description": "The model to use for image generation."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt for image generation."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to guide what should not appear in the generated image."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for the random number generator. Use -1 for a random seed.",
          "min": -1.0,
          "max": 4294967295.0
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 25,
          "title": "Num Inference Steps",
          "description": "Number of denoising steps.",
          "min": 1.0,
          "max": 100.0
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 7.5,
          "title": "Guidance Scale",
          "description": "Guidance scale for generation.",
          "min": 1.0,
          "max": 20.0
        },
        {
          "name": "scheduler",
          "type": {
            "type": "enum",
            "values": [
              "DPMSolverSDEScheduler",
              "EulerDiscreteScheduler",
              "LMSDiscreteScheduler",
              "DDIMScheduler",
              "DDPMScheduler",
              "HeunDiscreteScheduler",
              "DPMSolverMultistepScheduler",
              "DEISMultistepScheduler",
              "PNDMScheduler",
              "EulerAncestralDiscreteScheduler",
              "UniPCMultistepScheduler",
              "KDPM2DiscreteScheduler",
              "DPMSolverSinglestepScheduler",
              "KDPM2AncestralDiscreteScheduler"
            ],
            "type_name": "nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionScheduler"
          },
          "default": "EulerDiscreteScheduler",
          "title": "Scheduler",
          "description": "The scheduler to use for the diffusion process."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "hf.lora_sd_config"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRA models to use for image processing"
        },
        {
          "name": "ip_adapter_model",
          "type": {
            "type": "hf.ip_adapter"
          },
          "default": {},
          "title": "Ip Adapter Model",
          "description": "The IP adapter model to use for image processing"
        },
        {
          "name": "ip_adapter_image",
          "type": {
            "type": "image"
          },
          "default": {},
          "title": "Ip Adapter Image",
          "description": "When provided the image will be fed into the IP adapter"
        },
        {
          "name": "ip_adapter_scale",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Ip Adapter Scale",
          "description": "The strength of the IP adapter",
          "min": 0.0,
          "max": 1.0
        },
        {
          "name": "pag_scale",
          "type": {
            "type": "float"
          },
          "default": 3.0,
          "title": "Pag Scale",
          "description": "Scale of the Perturbed-Attention Guidance applied to the image.",
          "min": 0.0,
          "max": 10.0
        },
        {
          "name": "detail_level",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Detail Level",
          "description": "Level of detail for the hi-res pass. 0.0 is low detail, 1.0 is high detail.",
          "min": 0.0,
          "max": 1.0
        },
        {
          "name": "enable_tiling",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Tiling",
          "description": "Enable tiling for the VAE. This can reduce VRAM usage."
        },
        {
          "name": "enable_cpu_offload",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Cpu Offload",
          "description": "Enable CPU offload for the pipeline. This can reduce VRAM usage."
        },
        {
          "name": "upscaler",
          "type": {
            "type": "enum",
            "values": [
              "None",
              "Latent",
              "Bicubic"
            ],
            "type_name": "nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionUpscaler"
          },
          "default": "None",
          "title": "Upscaler",
          "description": "The upscaler to use for 2-pass generation."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "recommended_models": [
        {
          "type": "hf.ip_adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "models/ip-adapter_sd15.bin"
        },
        {
          "type": "hf.ip_adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "models/ip-adapter_sd15_light.bin"
        },
        {
          "type": "hf.ip_adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "models/ip-adapter_sd15_vit-G.bin"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "SG161222/Realistic_Vision_V5.1_noVAE",
          "path": "Realistic_Vision_V5.1_fp16-no-ema.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "digiplay/majicMIX_realistic_v7",
          "path": "majicmixRealistic_v7.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "philz1337x/epicrealism",
          "path": "epicrealism_naturalSinRC1VAE.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "Lykon/DreamShaper",
          "path": "DreamShaper_5_beta2_noVae_half_pruned.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "Lykon/DreamShaper",
          "path": "DreamShaper_4BakedVae_fp16.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "XpucT/Deliberate",
          "path": "Deliberate_v6.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "XpucT/Deliberate",
          "path": "Deliberate_v6-inpainting.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "Lykon/AbsoluteReality",
          "path": "AbsoluteReality_1.8.1_pruned.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "Lykon/AbsoluteReality",
          "path": "AbsoluteReality_1.8.1_INPAINTING.inpainting.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "gsdf/Counterfeit-V2.5",
          "path": "Counterfeit-V2.5_fp16.safetensors"
        },
        {
          "type": "hf.ip_adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "models/ip-adapter_sd15.bin"
        },
        {
          "type": "hf.ip_adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "models/ip-adapter_sd15_light.bin"
        },
        {
          "type": "hf.ip_adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "models/ip-adapter_sd15_vit-G.bin"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "stabilityai/sd-x2-latent-upscaler",
          "allow_patterns": [
            "README.md",
            "**/*.safetensors",
            "**/*.json",
            "**/*.txt",
            "*.json"
          ]
        }
      ],
      "basic_fields": [
        "model",
        "prompt"
      ]
    },
    {
      "title": "Stable Diffusion XLBase",
      "description": "",
      "namespace": "huggingface.stable_diffusion_base",
      "node_type": "huggingface.stable_diffusion_base.StableDiffusionXLBase",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.stable_diffusion_xl"
          },
          "default": {},
          "title": "Model",
          "description": "The Stable Diffusion XL model to use for generation."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt for image generation."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to guide what should not appear in the generated image."
        },
        {
          "name": "width",
          "type": {
            "type": "int"
          },
          "default": 1024,
          "title": "Width",
          "description": "Width of the generated image.",
          "min": 64.0,
          "max": 2048.0
        },
        {
          "name": "height",
          "type": {
            "type": "int"
          },
          "default": 1024,
          "title": "Height",
          "description": "Height of the generated image",
          "min": 64.0,
          "max": 2048.0
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for the random number generator.",
          "min": -1.0,
          "max": 1000000.0
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 25,
          "title": "Num Inference Steps",
          "description": "Number of inference steps.",
          "min": 1.0,
          "max": 100.0
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 7.0,
          "title": "Guidance Scale",
          "description": "Guidance scale for generation.",
          "min": 0.0,
          "max": 20.0
        },
        {
          "name": "scheduler",
          "type": {
            "type": "enum",
            "values": [
              "DPMSolverSDEScheduler",
              "EulerDiscreteScheduler",
              "LMSDiscreteScheduler",
              "DDIMScheduler",
              "DDPMScheduler",
              "HeunDiscreteScheduler",
              "DPMSolverMultistepScheduler",
              "DEISMultistepScheduler",
              "PNDMScheduler",
              "EulerAncestralDiscreteScheduler",
              "UniPCMultistepScheduler",
              "KDPM2DiscreteScheduler",
              "DPMSolverSinglestepScheduler",
              "KDPM2AncestralDiscreteScheduler"
            ],
            "type_name": "nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionScheduler"
          },
          "default": "EulerDiscreteScheduler",
          "title": "Scheduler",
          "description": "The scheduler to use for the diffusion process."
        },
        {
          "name": "pag_scale",
          "type": {
            "type": "float"
          },
          "default": 3.0,
          "title": "Pag Scale",
          "description": "Scale of the Perturbed-Attention Guidance applied to the image.",
          "min": 0.0,
          "max": 10.0
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "hf.lora_sdxl_config"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRA models to use for image processing"
        },
        {
          "name": "lora_scale",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Lora Scale",
          "description": "Strength of the LoRAs",
          "min": 0.0,
          "max": 1.0
        },
        {
          "name": "ip_adapter_model",
          "type": {
            "type": "hf.ip_adapter"
          },
          "default": {},
          "title": "Ip Adapter Model",
          "description": "The IP adapter model to use for image processing"
        },
        {
          "name": "ip_adapter_image",
          "type": {
            "type": "image"
          },
          "default": {},
          "title": "Ip Adapter Image",
          "description": "When provided the image will be fed into the IP adapter"
        },
        {
          "name": "ip_adapter_scale",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Ip Adapter Scale",
          "description": "Strength of the IP adapter image",
          "min": 0.0,
          "max": 1.0
        },
        {
          "name": "enable_tiling",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Tiling",
          "description": "Enable tiling for the VAE. This can reduce VRAM usage."
        },
        {
          "name": "enable_cpu_offload",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Cpu Offload",
          "description": "Enable CPU offload for the pipeline. This can reduce VRAM usage."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "recommended_models": [
        {
          "type": "hf.ip_adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "sdxl_models/ip-adapter_sdxl.bin"
        },
        {
          "type": "hf.ip_adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "sdxl_models/ip-adapter_sdxl_vit-h.bin"
        },
        {
          "type": "hf.ip_adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "sdxl_models/ip-adapter-plus_sdxl_vit-h.bin"
        },
        {
          "type": "hf.stable_diffusion_xl",
          "repo_id": "stabilityai/stable-diffusion-xl-base-1.0",
          "path": "sd_xl_base_1.0.safetensors"
        },
        {
          "type": "hf.stable_diffusion_xl",
          "repo_id": "stabilityai/stable-diffusion-xl-refiner-1.0",
          "path": "sd_xl_refiner_1.0.safetensors"
        },
        {
          "type": "hf.stable_diffusion_xl",
          "repo_id": "playgroundai/playground-v2.5-1024px-aesthetic",
          "path": "playground-v2.5-1024px-aesthetic.fp16.safetensors"
        },
        {
          "type": "hf.stable_diffusion_xl",
          "repo_id": "RunDiffusion/Juggernaut-XL-v9",
          "path": "Juggernaut-XL_v9_RunDiffusionPhoto_v2.safetensors"
        },
        {
          "type": "hf.stable_diffusion_xl",
          "repo_id": "dataautogpt3/ProteusV0.5",
          "path": "proteusV0.5.safetensors"
        },
        {
          "type": "hf.stable_diffusion_xl",
          "repo_id": "Lykon/dreamshaper-xl-lightning",
          "path": "DreamShaperXL_Lightning.safetensors"
        },
        {
          "type": "hf.stable_diffusion_xl",
          "repo_id": "Lykon/AAM_XL_AnimeMix",
          "path": "AAM_XL_Anime_Mix.safetensors"
        },
        {
          "type": "hf.stable_diffusion_xl",
          "repo_id": "stabilityai/sdxl-turbo",
          "path": "sd_xl_turbo_1.0_fp16.safetensors"
        },
        {
          "type": "hf.stable_diffusion_xl",
          "repo_id": "Lykon/dreamshaper-xl-v2-turbo",
          "path": "DreamShaperXL_Turbo_v2_1.safetensors"
        }
      ],
      "basic_fields": [
        "model",
        "prompt",
        "width",
        "height"
      ]
    },
    {
      "title": "Text To Text",
      "description": "Performs text-to-text generation tasks.\n    text, generation, translation, question-answering, summarization, nlp, natural-language-processing\n\n    Use cases:\n    - Text translation\n    - Text summarization\n    - Paraphrasing\n    - Text style transfer\n\n    Usage:\n    Start with a command like Translate, Summarize, or Q (for question)\n    Follow with the text you want to translate, summarize, or answer a question about.\n    Examples:\n    - Translate to German: Hello\n    - Summarize: The quick brown fox jumps over the lazy dog.\n    - Q: Who ate the cookie? followed by the text of the cookie monster.",
      "namespace": "huggingface.text_to_text",
      "node_type": "huggingface.text_to_text.TextToText",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.text2text_generation"
          },
          "default": {},
          "title": "Model ID on Huggingface",
          "description": "The model ID to use for the text-to-text generation"
        },
        {
          "name": "text",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Input Text",
          "description": "The input text for the text-to-text task"
        },
        {
          "name": "max_length",
          "type": {
            "type": "int"
          },
          "default": 50,
          "title": "Max Length",
          "description": "The maximum length of the generated text"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "str"
          },
          "name": "output"
        }
      ],
      "recommended_models": [
        {
          "type": "hf.text2text_generation",
          "repo_id": "google/flan-t5-small",
          "allow_patterns": [
            "*.json",
            "*.txt",
            "*.safetensors"
          ]
        },
        {
          "type": "hf.text2text_generation",
          "repo_id": "google/flan-t5-base",
          "allow_patterns": [
            "*.json",
            "*.txt",
            "*.safetensors"
          ]
        },
        {
          "type": "hf.text2text_generation",
          "repo_id": "google/flan-t5-large",
          "allow_patterns": [
            "*.json",
            "*.txt",
            "*.safetensors"
          ]
        },
        {
          "type": "hf.text2text_generation",
          "repo_id": "gokaygokay/Flux-Prompt-Enhance",
          "allow_patterns": [
            "*.json",
            "*.txt",
            "*.safetensors"
          ]
        }
      ],
      "basic_fields": [
        "model",
        "text"
      ]
    },
    {
      "title": "Audio LDM",
      "description": "Generates audio using the AudioLDM model based on text prompts.\n    audio, generation, AI, text-to-audio\n\n    Use cases:\n    - Create custom music or sound effects from text descriptions\n    - Generate background audio for videos, games, or other media\n    - Produce audio content for creative projects\n    - Explore AI-generated audio for music production or sound design",
      "namespace": "huggingface.text_to_audio",
      "node_type": "huggingface.text_to_audio.AudioLDM",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "Techno music with a strong, upbeat tempo and high melodic riffs",
          "title": "Prompt",
          "description": "A text prompt describing the desired audio."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 10,
          "title": "Num Inference Steps",
          "description": "Number of denoising steps. More steps generally improve quality but increase generation time.",
          "min": 1.0,
          "max": 100.0
        },
        {
          "name": "audio_length_in_s",
          "type": {
            "type": "float"
          },
          "default": 5.0,
          "title": "Audio Length In S",
          "description": "The desired duration of the generated audio in seconds.",
          "min": 1.0,
          "max": 30.0
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Seed",
          "description": "Seed for the random number generator. Use -1 for a random seed.",
          "min": -1.0
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "recommended_models": [
        {
          "type": "hf.text_to_audio",
          "repo_id": "cvssp/audioldm-s-full-v2",
          "allow_patterns": [
            "*.safetensors",
            "*.json",
            "*.txt"
          ]
        }
      ],
      "basic_fields": [
        "model",
        "prompt"
      ]
    },
    {
      "title": "Audio LDM 2",
      "description": "Generates audio using the AudioLDM2 model based on text prompts.\n    audio, generation, AI, text-to-audio\n\n    Use cases:\n    - Create custom sound effects based on textual descriptions\n    - Generate background audio for videos or games\n    - Produce audio content for multimedia projects\n    - Explore AI-generated audio for creative sound design",
      "namespace": "huggingface.text_to_audio",
      "node_type": "huggingface.text_to_audio.AudioLDM2",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "The sound of a hammer hitting a wooden surface.",
          "title": "Prompt",
          "description": "A text prompt describing the desired audio."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "Low quality.",
          "title": "Negative Prompt",
          "description": "A text prompt describing what you don't want in the audio."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 200,
          "title": "Num Inference Steps",
          "description": "Number of denoising steps. More steps generally improve quality but increase generation time.",
          "min": 50.0,
          "max": 500.0
        },
        {
          "name": "audio_length_in_s",
          "type": {
            "type": "float"
          },
          "default": 10.0,
          "title": "Audio Length In S",
          "description": "The desired duration of the generated audio in seconds.",
          "min": 1.0,
          "max": 30.0
        },
        {
          "name": "num_waveforms_per_prompt",
          "type": {
            "type": "int"
          },
          "default": 3,
          "title": "Num Waveforms Per Prompt",
          "description": "Number of audio samples to generate per prompt.",
          "min": 1.0,
          "max": 5.0
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Seed",
          "description": "Seed for the random number generator. Use -1 for a random seed.",
          "min": -1.0
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "recommended_models": [
        {
          "type": "hf.text_to_audio",
          "repo_id": "cvssp/audioldm2",
          "allow_patterns": [
            "*.safetensors",
            "*.json",
            "*.txt"
          ]
        }
      ],
      "basic_fields": [
        "model",
        "prompt"
      ]
    },
    {
      "title": "Dance Diffusion",
      "description": "Generates audio using the DanceDiffusion model.\n    audio, generation, AI, music, text-to-audio\n\n    Use cases:\n    - Create AI-generated music samples\n    - Produce background music for videos or games\n    - Generate audio content for creative projects\n    - Explore AI-composed musical ideas",
      "namespace": "huggingface.text_to_audio",
      "node_type": "huggingface.text_to_audio.DanceDiffusion",
      "properties": [
        {
          "name": "audio_length_in_s",
          "type": {
            "type": "float"
          },
          "default": 4.0,
          "title": "Audio Length In S",
          "description": "The desired duration of the generated audio in seconds.",
          "min": 1.0,
          "max": 30.0
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 50,
          "title": "Num Inference Steps",
          "description": "Number of denoising steps. More steps generally improve quality but increase generation time.",
          "min": 1.0,
          "max": 1000.0
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Seed",
          "description": "Seed for the random number generator. Use -1 for a random seed.",
          "min": -1.0
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "recommended_models": [
        {
          "type": "hf.text_to_audio",
          "repo_id": "harmonai/maestro-150k",
          "allow_patterns": [
            "*.bin",
            "*.json",
            "*.txt"
          ]
        }
      ],
      "basic_fields": [
        "model",
        "prompt"
      ]
    },
    {
      "title": "Music Gen",
      "description": "Generates audio (music or sound effects) from text descriptions.\n    audio, music, generation, huggingface, text-to-audio\n\n    Use cases:\n    - Create custom background music for videos or games\n    - Generate sound effects based on textual descriptions\n    - Prototype musical ideas quickly",
      "namespace": "huggingface.text_to_audio",
      "node_type": "huggingface.text_to_audio.MusicGen",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.text_to_audio"
          },
          "default": {},
          "title": "Model ID on Huggingface",
          "description": "The model ID to use for the audio generation"
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Inputs",
          "description": "The input text to the model"
        },
        {
          "name": "max_new_tokens",
          "type": {
            "type": "int"
          },
          "default": 1024,
          "title": "Max New Tokens",
          "description": "The maximum number of tokens to generate"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "recommended_models": [
        {
          "type": "hf.text_to_audio",
          "repo_id": "facebook/musicgen-small",
          "allow_patterns": [
            "*.safetensors",
            "*.json",
            "*.model"
          ]
        },
        {
          "type": "hf.text_to_audio",
          "repo_id": "facebook/musicgen-medium",
          "allow_patterns": [
            "*.safetensors",
            "*.json",
            "*.model"
          ]
        },
        {
          "type": "hf.text_to_audio",
          "repo_id": "facebook/musicgen-large",
          "allow_patterns": [
            "*.safetensors",
            "*.json",
            "*.model"
          ]
        },
        {
          "type": "hf.text_to_audio",
          "repo_id": "facebook/musicgen-melody",
          "allow_patterns": [
            "*.safetensors",
            "*.json",
            "*.model"
          ]
        },
        {
          "type": "hf.text_to_audio",
          "repo_id": "facebook/musicgen-stereo-small",
          "allow_patterns": [
            "*.safetensors",
            "*.json",
            "*.model"
          ]
        },
        {
          "type": "hf.text_to_audio",
          "repo_id": "facebook/musicgen-stereo-large",
          "allow_patterns": [
            "*.safetensors",
            "*.json",
            "*.model"
          ]
        }
      ],
      "basic_fields": [
        "model",
        "prompt"
      ]
    },
    {
      "title": "Music LDM",
      "description": "Generates audio (music or sound effects) from text descriptions.\n    audio, music, generation, huggingface, text-to-audio\n\n    Use cases:\n    - Create custom background music for videos or games\n    - Generate sound effects based on textual descriptions\n    - Prototype musical ideas quickly",
      "namespace": "huggingface.text_to_audio",
      "node_type": "huggingface.text_to_audio.MusicLDM",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.text_to_audio"
          },
          "default": {},
          "title": "Model ID on Huggingface",
          "description": "The model ID to use for the audio generation"
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Inputs",
          "description": "The input text to the model"
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 10,
          "title": "Number of Inference Steps",
          "description": "The number of inference steps to use for the generation"
        },
        {
          "name": "audio_length_in_s",
          "type": {
            "type": "float"
          },
          "default": 5.0,
          "title": "Audio Length",
          "description": "The length of the generated audio in seconds"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "recommended_models": [
        {
          "type": "hf.text_to_audio",
          "repo_id": "ucsd-reach/musicldm",
          "allow_patterns": [
            "*.safetensors",
            "*.json",
            "*.txt"
          ]
        }
      ],
      "basic_fields": [
        "model",
        "prompt"
      ]
    },
    {
      "title": "Stable Audio",
      "description": "Generate audio using Stable Audio model based on text prompts. Features high-quality audio synthesis with configurable parameters.\n    audio, generation, synthesis, text-to-audio, text-to-audio\n\n    Use cases:\n    - Create custom audio content from text\n    - Generate background music and sounds\n    - Produce audio for multimedia projects\n    - Create sound effects and ambience\n    - Generate experimental audio content",
      "namespace": "huggingface.text_to_audio",
      "node_type": "huggingface.text_to_audio.StableAudio",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "A peaceful piano melody.",
          "title": "Prompt",
          "description": "A text prompt describing the desired audio."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "Low quality.",
          "title": "Negative Prompt",
          "description": "A text prompt describing what you don't want in the audio."
        },
        {
          "name": "duration",
          "type": {
            "type": "float"
          },
          "default": 10.0,
          "title": "Duration",
          "description": "The desired duration of the generated audio in seconds.",
          "min": 1.0,
          "max": 300.0
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 200,
          "title": "Num Inference Steps",
          "description": "Number of denoising steps. More steps generally improve quality but increase generation time.",
          "min": 50.0,
          "max": 500.0
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Seed",
          "description": "Seed for the random number generator. Use -1 for a random seed.",
          "min": -1.0
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "recommended_models": [
        {
          "type": "hf.text_to_audio",
          "repo_id": "stabilityai/stable-audio-open-1.0",
          "allow_patterns": [
            "*.safetensors",
            "*.json",
            "*.txt"
          ]
        }
      ],
      "basic_fields": [
        "model",
        "prompt"
      ]
    },
    {
      "title": "Sentence Similarity",
      "description": "Compares the similarity between two sentences.\n    text, sentence similarity, embeddings, natural language processing\n\n    Use cases:\n    - Duplicate detection in text data\n    - Semantic search\n    - Sentiment analysis",
      "namespace": "huggingface.sentence_similarity",
      "node_type": "huggingface.sentence_similarity.SentenceSimilarity",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.sentence_similarity"
          },
          "default": {},
          "title": "Model ID on Huggingface",
          "description": "The model ID to use for sentence similarity"
        },
        {
          "name": "inputs",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Input Text",
          "description": "The text to compare"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "np_array"
          },
          "name": "output"
        }
      ],
      "recommended_models": [
        {
          "type": "hf.sentence_similarity",
          "repo_id": "sentence-transformers/all-mpnet-base-v2",
          "allow_patterns": [
            "*.safetensors",
            "*.txt",
            "*,json"
          ]
        },
        {
          "type": "hf.sentence_similarity",
          "repo_id": "sentence-transformers/all-MiniLM-L6-v2",
          "allow_patterns": [
            "*.safetensors",
            "*.txt",
            "*,json"
          ]
        },
        {
          "type": "hf.sentence_similarity",
          "repo_id": "BAAI/bge-m3",
          "allow_patterns": [
            "*.safetensors",
            "*.txt",
            "*,json"
          ]
        },
        {
          "type": "hf.sentence_similarity",
          "repo_id": "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
          "allow_patterns": [
            "*.safetensors",
            "*.txt",
            "*,json"
          ]
        }
      ],
      "basic_fields": [
        "model",
        "inputs"
      ]
    },
    {
      "title": "Base Image To Image",
      "description": "Base class for image-to-image transformation tasks.\n    image, transformation, generation, huggingface",
      "namespace": "huggingface.image_to_image",
      "node_type": "huggingface.image_to_image.BaseImageToImage",
      "properties": [
        {
          "name": "image",
          "type": {
            "type": "image"
          },
          "default": {},
          "title": "Input Image",
          "description": "The input image to transform"
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt to guide the image transformation (if applicable)"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Image to Image",
      "description": "Transforms existing images based on text prompts using AutoPipeline for Image-to-Image.\n    This node automatically detects the appropriate pipeline class based on the model used.\n    image, generation, image-to-image, autopipeline\n\n    Use cases:\n    - Transform existing images with any compatible model (Stable Diffusion, SDXL, Kandinsky, etc.)\n    - Apply specific styles or concepts to photographs or artwork\n    - Modify existing images based on text descriptions\n    - Create variations of existing visual content with automatic pipeline selection",
      "namespace": "huggingface.image_to_image",
      "node_type": "huggingface.image_to_image.ImageToImage",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.image_to_image"
          },
          "default": {},
          "title": "Model",
          "description": "The HuggingFace model to use for image-to-image generation."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "A beautiful landscape with mountains and a lake at sunset",
          "title": "Prompt",
          "description": "Text prompt describing the desired image transformation."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Text prompt describing what should not appear in the generated image."
        },
        {
          "name": "image",
          "type": {
            "type": "image"
          },
          "default": {},
          "title": "Input Image",
          "description": "The input image to transform"
        },
        {
          "name": "strength",
          "type": {
            "type": "float"
          },
          "default": 0.8,
          "title": "Strength",
          "description": "Strength of the transformation. Higher values allow for more deviation from the original image.",
          "min": 0.0,
          "max": 1.0
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 25,
          "title": "Num Inference Steps",
          "description": "Number of denoising steps.",
          "min": 1.0,
          "max": 100.0
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 7.5,
          "title": "Guidance Scale",
          "description": "Guidance scale for generation. Higher values follow the prompt more closely.",
          "min": 1.0,
          "max": 20.0
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for the random number generator. Use -1 for a random seed.",
          "min": -1.0
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "model",
        "image",
        "prompt",
        "negative_prompt",
        "strength"
      ]
    },
    {
      "title": "AutoPipeline Inpainting",
      "description": "Performs inpainting on images using AutoPipeline for Inpainting.\n    This node automatically detects the appropriate pipeline class based on the model used.\n    image, inpainting, autopipeline, stable-diffusion, SDXL, kandinsky\n\n    Use cases:\n    - Remove unwanted objects from images with any compatible model\n    - Fill in missing parts of images using various diffusion models\n    - Modify specific areas of images while preserving the rest\n    - Automatic pipeline selection for different model architectures",
      "namespace": "huggingface.image_to_image",
      "node_type": "huggingface.image_to_image.Inpaint",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.image_to_image"
          },
          "default": {},
          "title": "Model",
          "description": "The HuggingFace model to use for inpainting."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Text prompt describing what should be generated in the masked area."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Text prompt describing what should not appear in the generated content."
        },
        {
          "name": "image",
          "type": {
            "type": "image"
          },
          "default": {},
          "title": "Input Image",
          "description": "The input image to inpaint"
        },
        {
          "name": "mask_image",
          "type": {
            "type": "image"
          },
          "default": {},
          "title": "Mask Image",
          "description": "The mask image indicating areas to be inpainted (white areas will be inpainted)"
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 25,
          "title": "Num Inference Steps",
          "description": "Number of denoising steps.",
          "min": 1.0,
          "max": 100.0
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 7.5,
          "title": "Guidance Scale",
          "description": "Guidance scale for generation. Higher values follow the prompt more closely.",
          "min": 1.0,
          "max": 20.0
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for the random number generator. Use -1 for a random seed.",
          "min": -1.0
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "model",
        "image",
        "mask_image",
        "prompt",
        "negative_prompt"
      ]
    },
    {
      "title": "Load Image To Image Model",
      "description": "Load HuggingFace model for image-to-image generation from a repo_id.\n\n    Use cases:\n    - Loads a pipeline directly from a repo_id\n    - Used for ImageToImage node",
      "namespace": "huggingface.image_to_image",
      "node_type": "huggingface.image_to_image.LoadImageToImageModel",
      "properties": [
        {
          "name": "repo_id",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Repo Id",
          "description": "The repository ID of the model to use for image-to-image generation."
        },
        {
          "name": "variant",
          "type": {
            "type": "enum",
            "values": [
              "default",
              "fp16",
              "fp32",
              "bf16"
            ],
            "type_name": "nodetool.nodes.huggingface.image_to_image.ModelVariant"
          },
          "default": "fp16",
          "title": "Variant",
          "description": "The variant of the model to use for image-to-image generation."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "hf.image_to_image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "repo_id",
        "variant"
      ]
    },
    {
      "title": "OmniGen",
      "description": "Generates and edits images using the OmniGen model, supporting multimodal inputs.\n    image, generation, text-to-image, image-editing, multimodal, omnigen\n\n    Use cases:\n    - Generate images from text prompts\n    - Edit existing images with text instructions\n    - Controllable image generation with reference images\n    - Visual reasoning and image manipulation\n    - ID and object preserving generation",
      "namespace": "huggingface.image_to_image",
      "node_type": "huggingface.image_to_image.OmniGen",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "A realistic photo of a young woman sitting on a sofa, holding a book and facing the camera.",
          "title": "Prompt",
          "description": "The text prompt for image generation. Use <img><|image_1|></img> placeholders to reference input images."
        },
        {
          "name": "input_images",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "image"
              }
            ]
          },
          "default": [],
          "title": "Input Images",
          "description": "List of input images to use for editing or as reference. Referenced in prompt using <img><|image_1|></img>, <img><|image_2|></img>, etc."
        },
        {
          "name": "height",
          "type": {
            "type": "int"
          },
          "default": 1024,
          "title": "Height",
          "description": "Height of the generated image.",
          "min": 64.0,
          "max": 2048.0
        },
        {
          "name": "width",
          "type": {
            "type": "int"
          },
          "default": 1024,
          "title": "Width",
          "description": "Width of the generated image.",
          "min": 64.0,
          "max": 2048.0
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 2.5,
          "title": "Guidance Scale",
          "description": "Guidance scale for generation. Higher values follow the prompt more closely.",
          "min": 1.0,
          "max": 20.0
        },
        {
          "name": "img_guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 1.6,
          "title": "Img Guidance Scale",
          "description": "Image guidance scale when using input images.",
          "min": 1.0,
          "max": 20.0
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 25,
          "title": "Num Inference Steps",
          "description": "Number of denoising steps.",
          "min": 1.0,
          "max": 100.0
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for the random number generator. Use -1 for a random seed.",
          "min": -1.0
        },
        {
          "name": "use_input_image_size_as_output",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Use Input Image Size As Output",
          "description": "If True, use the input image size as output size. Recommended for image editing."
        },
        {
          "name": "max_input_image_size",
          "type": {
            "type": "int"
          },
          "default": 1024,
          "title": "Max Input Image Size",
          "description": "Maximum input image size. Smaller values reduce memory usage but may affect quality.",
          "min": 256.0,
          "max": 2048.0
        },
        {
          "name": "enable_model_cpu_offload",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Model Cpu Offload",
          "description": "Enable CPU offload to reduce memory usage when using multiple images."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "recommended_models": [
        {
          "repo_id": "Shitao/OmniGen-v1-diffusers",
          "allow_patterns": [
            "README.md",
            "*.safetensors",
            "*.json",
            "**/*.json"
          ]
        }
      ],
      "basic_fields": [
        "prompt",
        "input_images",
        "height",
        "width",
        "guidance_scale"
      ]
    },
    {
      "title": "Real ESRGAN",
      "description": "Performs image super-resolution using the RealESRGAN model.\n    image, super-resolution, enhancement, huggingface\n\n    Use cases:\n    - Enhance low-resolution images\n    - Improve image quality for printing or display\n    - Upscale images for better detail",
      "namespace": "huggingface.image_to_image",
      "node_type": "huggingface.image_to_image.RealESRGAN",
      "properties": [
        {
          "name": "image",
          "type": {
            "type": "image"
          },
          "default": {},
          "title": "Input Image",
          "description": "The input image to transform"
        },
        {
          "name": "model",
          "type": {
            "type": "hf.real_esrgan"
          },
          "default": {},
          "title": "RealESRGAN Model",
          "description": "The RealESRGAN model to use for image super-resolution"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "recommended_models": [
        {
          "type": "hf.real_esrgan",
          "repo_id": "ai-forever/Real-ESRGAN",
          "path": "RealESRGAN_x2.pth"
        },
        {
          "type": "hf.real_esrgan",
          "repo_id": "ai-forever/Real-ESRGAN",
          "path": "RealESRGAN_x4.pth"
        },
        {
          "type": "hf.real_esrgan",
          "repo_id": "ai-forever/Real-ESRGAN",
          "path": "RealESRGAN_x8.pth"
        },
        {
          "type": "hf.real_esrgan",
          "repo_id": "ximso/RealESRGAN_x4plus_anime_6B",
          "path": "RealESRGAN_x4plus_anime_6B.pth"
        }
      ],
      "basic_fields": [
        "image",
        "model"
      ]
    },
    {
      "title": "Stable Diffusion ControlNet (Img2Img)",
      "description": "Transforms existing images using Stable Diffusion with ControlNet guidance.\n    image, generation, image-to-image, controlnet, SD, style-transfer, ipadapter\n\n    Use cases:\n    - Modify existing images with precise control over composition and structure\n    - Apply specific styles or concepts to photographs or artwork with guided transformations\n    - Create variations of existing visual content while maintaining certain features\n    - Enhance image editing capabilities with AI-guided transformations",
      "namespace": "huggingface.image_to_image",
      "node_type": "huggingface.image_to_image.StableDiffusionControlNetImg2Img",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.stable_diffusion"
          },
          "default": {},
          "title": "Model",
          "description": "The model to use for image generation."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt for image generation."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to guide what should not appear in the generated image."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for the random number generator. Use -1 for a random seed.",
          "min": -1.0,
          "max": 4294967295.0
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 25,
          "title": "Num Inference Steps",
          "description": "Number of denoising steps.",
          "min": 1.0,
          "max": 100.0
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 7.5,
          "title": "Guidance Scale",
          "description": "Guidance scale for generation.",
          "min": 1.0,
          "max": 20.0
        },
        {
          "name": "scheduler",
          "type": {
            "type": "enum",
            "values": [
              "DPMSolverSDEScheduler",
              "EulerDiscreteScheduler",
              "LMSDiscreteScheduler",
              "DDIMScheduler",
              "DDPMScheduler",
              "HeunDiscreteScheduler",
              "DPMSolverMultistepScheduler",
              "DEISMultistepScheduler",
              "PNDMScheduler",
              "EulerAncestralDiscreteScheduler",
              "UniPCMultistepScheduler",
              "KDPM2DiscreteScheduler",
              "DPMSolverSinglestepScheduler",
              "KDPM2AncestralDiscreteScheduler"
            ],
            "type_name": "nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionScheduler"
          },
          "default": "EulerDiscreteScheduler",
          "title": "Scheduler",
          "description": "The scheduler to use for the diffusion process."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "hf.lora_sd_config"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRA models to use for image processing"
        },
        {
          "name": "ip_adapter_model",
          "type": {
            "type": "hf.ip_adapter"
          },
          "default": {},
          "title": "Ip Adapter Model",
          "description": "The IP adapter model to use for image processing"
        },
        {
          "name": "ip_adapter_image",
          "type": {
            "type": "image"
          },
          "default": {},
          "title": "Ip Adapter Image",
          "description": "When provided the image will be fed into the IP adapter"
        },
        {
          "name": "ip_adapter_scale",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Ip Adapter Scale",
          "description": "The strength of the IP adapter",
          "min": 0.0,
          "max": 1.0
        },
        {
          "name": "pag_scale",
          "type": {
            "type": "float"
          },
          "default": 3.0,
          "title": "Pag Scale",
          "description": "Scale of the Perturbed-Attention Guidance applied to the image.",
          "min": 0.0,
          "max": 10.0
        },
        {
          "name": "detail_level",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Detail Level",
          "description": "Level of detail for the hi-res pass. 0.0 is low detail, 1.0 is high detail.",
          "min": 0.0,
          "max": 1.0
        },
        {
          "name": "enable_tiling",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Tiling",
          "description": "Enable tiling for the VAE. This can reduce VRAM usage."
        },
        {
          "name": "enable_cpu_offload",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Cpu Offload",
          "description": "Enable CPU offload for the pipeline. This can reduce VRAM usage."
        },
        {
          "name": "upscaler",
          "type": {
            "type": "enum",
            "values": [
              "None",
              "Latent",
              "Bicubic"
            ],
            "type_name": "nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionUpscaler"
          },
          "default": "None",
          "title": "Upscaler",
          "description": "The upscaler to use for 2-pass generation."
        },
        {
          "name": "image",
          "type": {
            "type": "image"
          },
          "default": {},
          "title": "Image",
          "description": "The input image to be transformed."
        },
        {
          "name": "strength",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Strength",
          "description": "Similarity to the input image",
          "min": 0.0,
          "max": 1.0
        },
        {
          "name": "controlnet",
          "type": {
            "type": "hf.controlnet"
          },
          "default": {},
          "title": "Controlnet",
          "description": "The ControlNet model to use for guidance."
        },
        {
          "name": "control_image",
          "type": {
            "type": "image"
          },
          "default": {},
          "title": "Control Image",
          "description": "The control image to guide the transformation."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "recommended_models": [
        {
          "type": "hf.controlnet",
          "repo_id": "lllyasviel/control_v11p_sd15_canny",
          "path": "diffusion_pytorch_model.fp16.safetensors"
        },
        {
          "type": "hf.controlnet",
          "repo_id": "lllyasviel/control_v11p_sd15_inpaint",
          "path": "diffusion_pytorch_model.fp16.safetensors"
        },
        {
          "type": "hf.controlnet",
          "repo_id": "lllyasviel/control_v11p_sd15_mlsd",
          "path": "diffusion_pytorch_model.fp16.safetensors"
        },
        {
          "type": "hf.controlnet",
          "repo_id": "lllyasviel/control_v11p_sd15_tile",
          "path": "diffusion_pytorch_model.fp16.safetensors"
        },
        {
          "type": "hf.controlnet",
          "repo_id": "lllyasviel/control_v11p_sd15_shuffle",
          "path": "diffusion_pytorch_model.fp16.safetensors"
        },
        {
          "type": "hf.controlnet",
          "repo_id": "lllyasviel/control_v11p_sd15_ip2p",
          "path": "diffusion_pytorch_model.fp16.safetensors"
        },
        {
          "type": "hf.controlnet",
          "repo_id": "lllyasviel/control_v11p_sd15_lineart",
          "path": "diffusion_pytorch_model.fp16.safetensors"
        },
        {
          "type": "hf.controlnet",
          "repo_id": "lllyasviel/control_v11p_sd15_lineart_anime",
          "path": "diffusion_pytorch_model.fp16.safetensors"
        },
        {
          "type": "hf.controlnet",
          "repo_id": "lllyasviel/control_v11p_sd15_scribble",
          "path": "diffusion_pytorch_model.fp16.safetensors"
        },
        {
          "type": "hf.controlnet",
          "repo_id": "lllyasviel/control_v11p_sd15_openpose",
          "path": "diffusion_pytorch_model.fp16.safetensors"
        },
        {
          "type": "hf.controlnet",
          "repo_id": "lllyasviel/control_v11p_sd15_scribble",
          "path": "diffusion_pytorch_model.fp16.safetensors"
        },
        {
          "type": "hf.controlnet",
          "repo_id": "lllyasviel/control_v11p_sd15_seg",
          "path": "diffusion_pytorch_model.fp16.safetensors"
        },
        {
          "type": "hf.controlnet",
          "repo_id": "lllyasviel/control_v11p_sd15_hed",
          "path": "diffusion_pytorch_model.fp16.safetensors"
        },
        {
          "type": "hf.controlnet",
          "repo_id": "lllyasviel/control_v11p_sd15_normalbae",
          "path": "diffusion_pytorch_model.fp16.safetensors"
        },
        {
          "type": "hf.ip_adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "models/ip-adapter_sd15.bin"
        },
        {
          "type": "hf.ip_adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "models/ip-adapter_sd15_light.bin"
        },
        {
          "type": "hf.ip_adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "models/ip-adapter_sd15_vit-G.bin"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "SG161222/Realistic_Vision_V5.1_noVAE",
          "path": "Realistic_Vision_V5.1_fp16-no-ema.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "digiplay/majicMIX_realistic_v7",
          "path": "majicmixRealistic_v7.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "philz1337x/epicrealism",
          "path": "epicrealism_naturalSinRC1VAE.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "Lykon/DreamShaper",
          "path": "DreamShaper_5_beta2_noVae_half_pruned.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "Lykon/DreamShaper",
          "path": "DreamShaper_4BakedVae_fp16.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "XpucT/Deliberate",
          "path": "Deliberate_v6.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "XpucT/Deliberate",
          "path": "Deliberate_v6-inpainting.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "Lykon/AbsoluteReality",
          "path": "AbsoluteReality_1.8.1_pruned.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "Lykon/AbsoluteReality",
          "path": "AbsoluteReality_1.8.1_INPAINTING.inpainting.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "gsdf/Counterfeit-V2.5",
          "path": "Counterfeit-V2.5_fp16.safetensors"
        },
        {
          "type": "hf.ip_adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "models/ip-adapter_sd15.bin"
        },
        {
          "type": "hf.ip_adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "models/ip-adapter_sd15_light.bin"
        },
        {
          "type": "hf.ip_adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "models/ip-adapter_sd15_vit-G.bin"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "stabilityai/sd-x2-latent-upscaler",
          "allow_patterns": [
            "README.md",
            "**/*.safetensors",
            "**/*.json",
            "**/*.txt",
            "*.json"
          ]
        }
      ],
      "basic_fields": [
        "model",
        "prompt",
        "image",
        "controlnet",
        "control_image"
      ]
    },
    {
      "title": "Stable Diffusion ControlNet Inpaint",
      "description": "Performs inpainting on images using Stable Diffusion with ControlNet guidance.\n    image, inpainting, controlnet, SD, style-transfer, ipadapter\n\n    Use cases:\n    - Remove unwanted objects from images with precise control\n    - Fill in missing parts of images guided by control images\n    - Modify specific areas of images while preserving the rest and maintaining structure",
      "namespace": "huggingface.image_to_image",
      "node_type": "huggingface.image_to_image.StableDiffusionControlNetInpaint",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.stable_diffusion"
          },
          "default": {},
          "title": "Model",
          "description": "The model to use for image generation."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt for image generation."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to guide what should not appear in the generated image."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for the random number generator. Use -1 for a random seed.",
          "min": -1.0,
          "max": 4294967295.0
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 25,
          "title": "Num Inference Steps",
          "description": "Number of denoising steps.",
          "min": 1.0,
          "max": 100.0
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 7.5,
          "title": "Guidance Scale",
          "description": "Guidance scale for generation.",
          "min": 1.0,
          "max": 20.0
        },
        {
          "name": "scheduler",
          "type": {
            "type": "enum",
            "values": [
              "DPMSolverSDEScheduler",
              "EulerDiscreteScheduler",
              "LMSDiscreteScheduler",
              "DDIMScheduler",
              "DDPMScheduler",
              "HeunDiscreteScheduler",
              "DPMSolverMultistepScheduler",
              "DEISMultistepScheduler",
              "PNDMScheduler",
              "EulerAncestralDiscreteScheduler",
              "UniPCMultistepScheduler",
              "KDPM2DiscreteScheduler",
              "DPMSolverSinglestepScheduler",
              "KDPM2AncestralDiscreteScheduler"
            ],
            "type_name": "nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionScheduler"
          },
          "default": "EulerDiscreteScheduler",
          "title": "Scheduler",
          "description": "The scheduler to use for the diffusion process."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "hf.lora_sd_config"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRA models to use for image processing"
        },
        {
          "name": "ip_adapter_model",
          "type": {
            "type": "hf.ip_adapter"
          },
          "default": {},
          "title": "Ip Adapter Model",
          "description": "The IP adapter model to use for image processing"
        },
        {
          "name": "ip_adapter_image",
          "type": {
            "type": "image"
          },
          "default": {},
          "title": "Ip Adapter Image",
          "description": "When provided the image will be fed into the IP adapter"
        },
        {
          "name": "ip_adapter_scale",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Ip Adapter Scale",
          "description": "The strength of the IP adapter",
          "min": 0.0,
          "max": 1.0
        },
        {
          "name": "pag_scale",
          "type": {
            "type": "float"
          },
          "default": 3.0,
          "title": "Pag Scale",
          "description": "Scale of the Perturbed-Attention Guidance applied to the image.",
          "min": 0.0,
          "max": 10.0
        },
        {
          "name": "detail_level",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Detail Level",
          "description": "Level of detail for the hi-res pass. 0.0 is low detail, 1.0 is high detail.",
          "min": 0.0,
          "max": 1.0
        },
        {
          "name": "enable_tiling",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Tiling",
          "description": "Enable tiling for the VAE. This can reduce VRAM usage."
        },
        {
          "name": "enable_cpu_offload",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Cpu Offload",
          "description": "Enable CPU offload for the pipeline. This can reduce VRAM usage."
        },
        {
          "name": "upscaler",
          "type": {
            "type": "enum",
            "values": [
              "None",
              "Latent",
              "Bicubic"
            ],
            "type_name": "nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionUpscaler"
          },
          "default": "None",
          "title": "Upscaler",
          "description": "The upscaler to use for 2-pass generation."
        },
        {
          "name": "controlnet",
          "type": {
            "type": "enum",
            "values": [
              "lllyasviel/control_v11p_sd15_inpaint"
            ],
            "type_name": "nodetool.nodes.huggingface.image_to_image.StableDiffusionControlNetModel"
          },
          "default": "lllyasviel/control_v11p_sd15_inpaint",
          "title": "Controlnet",
          "description": "The ControlNet model to use for guidance."
        },
        {
          "name": "init_image",
          "type": {
            "type": "image"
          },
          "default": {},
          "title": "Init Image",
          "description": "The initial image to be inpainted."
        },
        {
          "name": "mask_image",
          "type": {
            "type": "image"
          },
          "default": {},
          "title": "Mask Image",
          "description": "The mask image indicating areas to be inpainted."
        },
        {
          "name": "control_image",
          "type": {
            "type": "image"
          },
          "default": {},
          "title": "Control Image",
          "description": "The control image to guide the inpainting process."
        },
        {
          "name": "controlnet_conditioning_scale",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Controlnet Conditioning Scale",
          "description": "The scale for ControlNet conditioning.",
          "min": 0.0,
          "max": 2.0
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "recommended_models": [
        {
          "type": "hf.ip_adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "models/ip-adapter_sd15.bin"
        },
        {
          "type": "hf.ip_adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "models/ip-adapter_sd15_light.bin"
        },
        {
          "type": "hf.ip_adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "models/ip-adapter_sd15_vit-G.bin"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "SG161222/Realistic_Vision_V5.1_noVAE",
          "path": "Realistic_Vision_V5.1_fp16-no-ema.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "digiplay/majicMIX_realistic_v7",
          "path": "majicmixRealistic_v7.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "philz1337x/epicrealism",
          "path": "epicrealism_naturalSinRC1VAE.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "Lykon/DreamShaper",
          "path": "DreamShaper_5_beta2_noVae_half_pruned.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "Lykon/DreamShaper",
          "path": "DreamShaper_4BakedVae_fp16.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "XpucT/Deliberate",
          "path": "Deliberate_v6.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "XpucT/Deliberate",
          "path": "Deliberate_v6-inpainting.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "Lykon/AbsoluteReality",
          "path": "AbsoluteReality_1.8.1_pruned.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "Lykon/AbsoluteReality",
          "path": "AbsoluteReality_1.8.1_INPAINTING.inpainting.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "gsdf/Counterfeit-V2.5",
          "path": "Counterfeit-V2.5_fp16.safetensors"
        },
        {
          "type": "hf.ip_adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "models/ip-adapter_sd15.bin"
        },
        {
          "type": "hf.ip_adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "models/ip-adapter_sd15_light.bin"
        },
        {
          "type": "hf.ip_adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "models/ip-adapter_sd15_vit-G.bin"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "stabilityai/sd-x2-latent-upscaler",
          "allow_patterns": [
            "README.md",
            "**/*.safetensors",
            "**/*.json",
            "**/*.txt",
            "*.json"
          ]
        }
      ],
      "basic_fields": [
        "model",
        "prompt",
        "init_image",
        "mask_image",
        "control_image",
        "controlnet_conditioning_scale"
      ]
    },
    {
      "title": "Stable Diffusion ControlNet",
      "description": "Generates images using Stable Diffusion with ControlNet guidance.\n    image, generation, text-to-image, controlnet, SD\n\n    Use cases:\n    - Generate images with precise control over composition and structure\n    - Create variations of existing images while maintaining specific features\n    - Artistic image generation with guided outputs",
      "namespace": "huggingface.image_to_image",
      "node_type": "huggingface.image_to_image.StableDiffusionControlNet",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.stable_diffusion"
          },
          "default": {},
          "title": "Model",
          "description": "The model to use for image generation."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt for image generation."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to guide what should not appear in the generated image."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for the random number generator. Use -1 for a random seed.",
          "min": -1.0,
          "max": 4294967295.0
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 25,
          "title": "Num Inference Steps",
          "description": "Number of denoising steps.",
          "min": 1.0,
          "max": 100.0
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 7.5,
          "title": "Guidance Scale",
          "description": "Guidance scale for generation.",
          "min": 1.0,
          "max": 20.0
        },
        {
          "name": "scheduler",
          "type": {
            "type": "enum",
            "values": [
              "DPMSolverSDEScheduler",
              "EulerDiscreteScheduler",
              "LMSDiscreteScheduler",
              "DDIMScheduler",
              "DDPMScheduler",
              "HeunDiscreteScheduler",
              "DPMSolverMultistepScheduler",
              "DEISMultistepScheduler",
              "PNDMScheduler",
              "EulerAncestralDiscreteScheduler",
              "UniPCMultistepScheduler",
              "KDPM2DiscreteScheduler",
              "DPMSolverSinglestepScheduler",
              "KDPM2AncestralDiscreteScheduler"
            ],
            "type_name": "nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionScheduler"
          },
          "default": "EulerDiscreteScheduler",
          "title": "Scheduler",
          "description": "The scheduler to use for the diffusion process."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "hf.lora_sd_config"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRA models to use for image processing"
        },
        {
          "name": "ip_adapter_model",
          "type": {
            "type": "hf.ip_adapter"
          },
          "default": {},
          "title": "Ip Adapter Model",
          "description": "The IP adapter model to use for image processing"
        },
        {
          "name": "ip_adapter_image",
          "type": {
            "type": "image"
          },
          "default": {},
          "title": "Ip Adapter Image",
          "description": "When provided the image will be fed into the IP adapter"
        },
        {
          "name": "ip_adapter_scale",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Ip Adapter Scale",
          "description": "The strength of the IP adapter",
          "min": 0.0,
          "max": 1.0
        },
        {
          "name": "pag_scale",
          "type": {
            "type": "float"
          },
          "default": 3.0,
          "title": "Pag Scale",
          "description": "Scale of the Perturbed-Attention Guidance applied to the image.",
          "min": 0.0,
          "max": 10.0
        },
        {
          "name": "detail_level",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Detail Level",
          "description": "Level of detail for the hi-res pass. 0.0 is low detail, 1.0 is high detail.",
          "min": 0.0,
          "max": 1.0
        },
        {
          "name": "enable_tiling",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Tiling",
          "description": "Enable tiling for the VAE. This can reduce VRAM usage."
        },
        {
          "name": "enable_cpu_offload",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Cpu Offload",
          "description": "Enable CPU offload for the pipeline. This can reduce VRAM usage."
        },
        {
          "name": "upscaler",
          "type": {
            "type": "enum",
            "values": [
              "None",
              "Latent",
              "Bicubic"
            ],
            "type_name": "nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionUpscaler"
          },
          "default": "None",
          "title": "Upscaler",
          "description": "The upscaler to use for 2-pass generation."
        },
        {
          "name": "controlnet",
          "type": {
            "type": "hf.controlnet"
          },
          "default": {},
          "title": "Controlnet",
          "description": "The ControlNet model to use for guidance."
        },
        {
          "name": "control_image",
          "type": {
            "type": "image"
          },
          "default": {},
          "title": "Control Image",
          "description": "The control image to guide the generation process."
        },
        {
          "name": "controlnet_conditioning_scale",
          "type": {
            "type": "float"
          },
          "default": 1.0,
          "title": "Controlnet Conditioning Scale",
          "description": "The scale for ControlNet conditioning.",
          "min": 0.0,
          "max": 2.0
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "recommended_models": [
        {
          "type": "hf.controlnet",
          "repo_id": "lllyasviel/control_v11p_sd15_canny",
          "path": "diffusion_pytorch_model.fp16.safetensors"
        },
        {
          "type": "hf.controlnet",
          "repo_id": "lllyasviel/control_v11p_sd15_inpaint",
          "path": "diffusion_pytorch_model.fp16.safetensors"
        },
        {
          "type": "hf.controlnet",
          "repo_id": "lllyasviel/control_v11p_sd15_mlsd",
          "path": "diffusion_pytorch_model.fp16.safetensors"
        },
        {
          "type": "hf.controlnet",
          "repo_id": "lllyasviel/control_v11p_sd15_tile",
          "path": "diffusion_pytorch_model.fp16.safetensors"
        },
        {
          "type": "hf.controlnet",
          "repo_id": "lllyasviel/control_v11p_sd15_shuffle",
          "path": "diffusion_pytorch_model.fp16.safetensors"
        },
        {
          "type": "hf.controlnet",
          "repo_id": "lllyasviel/control_v11p_sd15_ip2p",
          "path": "diffusion_pytorch_model.fp16.safetensors"
        },
        {
          "type": "hf.controlnet",
          "repo_id": "lllyasviel/control_v11p_sd15_lineart",
          "path": "diffusion_pytorch_model.fp16.safetensors"
        },
        {
          "type": "hf.controlnet",
          "repo_id": "lllyasviel/control_v11p_sd15_lineart_anime",
          "path": "diffusion_pytorch_model.fp16.safetensors"
        },
        {
          "type": "hf.controlnet",
          "repo_id": "lllyasviel/control_v11p_sd15_scribble",
          "path": "diffusion_pytorch_model.fp16.safetensors"
        },
        {
          "type": "hf.controlnet",
          "repo_id": "lllyasviel/control_v11p_sd15_openpose",
          "path": "diffusion_pytorch_model.fp16.safetensors"
        },
        {
          "type": "hf.controlnet",
          "repo_id": "lllyasviel/control_v11p_sd15_scribble",
          "path": "diffusion_pytorch_model.fp16.safetensors"
        },
        {
          "type": "hf.controlnet",
          "repo_id": "lllyasviel/control_v11p_sd15_seg",
          "path": "diffusion_pytorch_model.fp16.safetensors"
        },
        {
          "type": "hf.controlnet",
          "repo_id": "lllyasviel/control_v11p_sd15_hed",
          "path": "diffusion_pytorch_model.fp16.safetensors"
        },
        {
          "type": "hf.controlnet",
          "repo_id": "lllyasviel/control_v11p_sd15_normalbae",
          "path": "diffusion_pytorch_model.fp16.safetensors"
        },
        {
          "type": "hf.ip_adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "models/ip-adapter_sd15.bin"
        },
        {
          "type": "hf.ip_adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "models/ip-adapter_sd15_light.bin"
        },
        {
          "type": "hf.ip_adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "models/ip-adapter_sd15_vit-G.bin"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "SG161222/Realistic_Vision_V5.1_noVAE",
          "path": "Realistic_Vision_V5.1_fp16-no-ema.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "digiplay/majicMIX_realistic_v7",
          "path": "majicmixRealistic_v7.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "philz1337x/epicrealism",
          "path": "epicrealism_naturalSinRC1VAE.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "Lykon/DreamShaper",
          "path": "DreamShaper_5_beta2_noVae_half_pruned.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "Lykon/DreamShaper",
          "path": "DreamShaper_4BakedVae_fp16.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "XpucT/Deliberate",
          "path": "Deliberate_v6.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "XpucT/Deliberate",
          "path": "Deliberate_v6-inpainting.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "Lykon/AbsoluteReality",
          "path": "AbsoluteReality_1.8.1_pruned.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "Lykon/AbsoluteReality",
          "path": "AbsoluteReality_1.8.1_INPAINTING.inpainting.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "gsdf/Counterfeit-V2.5",
          "path": "Counterfeit-V2.5_fp16.safetensors"
        },
        {
          "type": "hf.ip_adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "models/ip-adapter_sd15.bin"
        },
        {
          "type": "hf.ip_adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "models/ip-adapter_sd15_light.bin"
        },
        {
          "type": "hf.ip_adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "models/ip-adapter_sd15_vit-G.bin"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "stabilityai/sd-x2-latent-upscaler",
          "allow_patterns": [
            "README.md",
            "**/*.safetensors",
            "**/*.json",
            "**/*.txt",
            "*.json"
          ]
        }
      ],
      "basic_fields": [
        "model",
        "prompt",
        "controlnet",
        "control_image",
        "controlnet_conditioning_scale"
      ]
    },
    {
      "title": "Stable Diffusion (Img2Img)",
      "description": "Transforms existing images based on text prompts using Stable Diffusion.\n    image, generation, image-to-image, SD, img2img, style-transfer, ipadapter\n\n    Use cases:\n    - Modifying existing images to fit a specific style or theme\n    - Enhancing or altering photographs\n    - Creating variations of existing artwork\n    - Applying text-guided edits to images",
      "namespace": "huggingface.image_to_image",
      "node_type": "huggingface.image_to_image.StableDiffusionImg2Img",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.stable_diffusion"
          },
          "default": {},
          "title": "Model",
          "description": "The model to use for image generation."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt for image generation."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to guide what should not appear in the generated image."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for the random number generator. Use -1 for a random seed.",
          "min": -1.0,
          "max": 4294967295.0
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 25,
          "title": "Num Inference Steps",
          "description": "Number of denoising steps.",
          "min": 1.0,
          "max": 100.0
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 7.5,
          "title": "Guidance Scale",
          "description": "Guidance scale for generation.",
          "min": 1.0,
          "max": 20.0
        },
        {
          "name": "scheduler",
          "type": {
            "type": "enum",
            "values": [
              "DPMSolverSDEScheduler",
              "EulerDiscreteScheduler",
              "LMSDiscreteScheduler",
              "DDIMScheduler",
              "DDPMScheduler",
              "HeunDiscreteScheduler",
              "DPMSolverMultistepScheduler",
              "DEISMultistepScheduler",
              "PNDMScheduler",
              "EulerAncestralDiscreteScheduler",
              "UniPCMultistepScheduler",
              "KDPM2DiscreteScheduler",
              "DPMSolverSinglestepScheduler",
              "KDPM2AncestralDiscreteScheduler"
            ],
            "type_name": "nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionScheduler"
          },
          "default": "EulerDiscreteScheduler",
          "title": "Scheduler",
          "description": "The scheduler to use for the diffusion process."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "hf.lora_sd_config"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRA models to use for image processing"
        },
        {
          "name": "ip_adapter_model",
          "type": {
            "type": "hf.ip_adapter"
          },
          "default": {},
          "title": "Ip Adapter Model",
          "description": "The IP adapter model to use for image processing"
        },
        {
          "name": "ip_adapter_image",
          "type": {
            "type": "image"
          },
          "default": {},
          "title": "Ip Adapter Image",
          "description": "When provided the image will be fed into the IP adapter"
        },
        {
          "name": "ip_adapter_scale",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Ip Adapter Scale",
          "description": "The strength of the IP adapter",
          "min": 0.0,
          "max": 1.0
        },
        {
          "name": "pag_scale",
          "type": {
            "type": "float"
          },
          "default": 3.0,
          "title": "Pag Scale",
          "description": "Scale of the Perturbed-Attention Guidance applied to the image.",
          "min": 0.0,
          "max": 10.0
        },
        {
          "name": "detail_level",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Detail Level",
          "description": "Level of detail for the hi-res pass. 0.0 is low detail, 1.0 is high detail.",
          "min": 0.0,
          "max": 1.0
        },
        {
          "name": "enable_tiling",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Tiling",
          "description": "Enable tiling for the VAE. This can reduce VRAM usage."
        },
        {
          "name": "enable_cpu_offload",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Cpu Offload",
          "description": "Enable CPU offload for the pipeline. This can reduce VRAM usage."
        },
        {
          "name": "upscaler",
          "type": {
            "type": "enum",
            "values": [
              "None",
              "Latent",
              "Bicubic"
            ],
            "type_name": "nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionUpscaler"
          },
          "default": "None",
          "title": "Upscaler",
          "description": "The upscaler to use for 2-pass generation."
        },
        {
          "name": "init_image",
          "type": {
            "type": "image"
          },
          "default": {},
          "title": "Init Image",
          "description": "The initial image for Image-to-Image generation."
        },
        {
          "name": "strength",
          "type": {
            "type": "float"
          },
          "default": 0.8,
          "title": "Strength",
          "description": "Strength for Image-to-Image generation. Higher values allow for more deviation from the original image.",
          "min": 0.0,
          "max": 1.0
        },
        {
          "name": "variant",
          "type": {
            "type": "enum",
            "values": [
              "default",
              "fp16",
              "fp32",
              "bf16"
            ],
            "type_name": "nodetool.nodes.huggingface.image_to_image.ModelVariant"
          },
          "default": "fp16",
          "title": "Variant",
          "description": "The variant of the model to use for Image-to-Image generation."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "recommended_models": [
        {
          "type": "hf.ip_adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "models/ip-adapter_sd15.bin"
        },
        {
          "type": "hf.ip_adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "models/ip-adapter_sd15_light.bin"
        },
        {
          "type": "hf.ip_adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "models/ip-adapter_sd15_vit-G.bin"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "SG161222/Realistic_Vision_V5.1_noVAE",
          "path": "Realistic_Vision_V5.1_fp16-no-ema.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "digiplay/majicMIX_realistic_v7",
          "path": "majicmixRealistic_v7.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "philz1337x/epicrealism",
          "path": "epicrealism_naturalSinRC1VAE.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "Lykon/DreamShaper",
          "path": "DreamShaper_5_beta2_noVae_half_pruned.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "Lykon/DreamShaper",
          "path": "DreamShaper_4BakedVae_fp16.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "XpucT/Deliberate",
          "path": "Deliberate_v6.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "XpucT/Deliberate",
          "path": "Deliberate_v6-inpainting.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "Lykon/AbsoluteReality",
          "path": "AbsoluteReality_1.8.1_pruned.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "Lykon/AbsoluteReality",
          "path": "AbsoluteReality_1.8.1_INPAINTING.inpainting.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "gsdf/Counterfeit-V2.5",
          "path": "Counterfeit-V2.5_fp16.safetensors"
        },
        {
          "type": "hf.ip_adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "models/ip-adapter_sd15.bin"
        },
        {
          "type": "hf.ip_adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "models/ip-adapter_sd15_light.bin"
        },
        {
          "type": "hf.ip_adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "models/ip-adapter_sd15_vit-G.bin"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "stabilityai/sd-x2-latent-upscaler",
          "allow_patterns": [
            "README.md",
            "**/*.safetensors",
            "**/*.json",
            "**/*.txt",
            "*.json"
          ]
        }
      ],
      "basic_fields": [
        "model",
        "prompt",
        "init_image",
        "strength"
      ]
    },
    {
      "title": "Stable Diffusion (Inpaint)",
      "description": "Performs inpainting on images using Stable Diffusion.\n    image, inpainting, SD\n\n    Use cases:\n    - Remove unwanted objects from images\n    - Fill in missing parts of images\n    - Modify specific areas of images while preserving the rest",
      "namespace": "huggingface.image_to_image",
      "node_type": "huggingface.image_to_image.StableDiffusionInpaint",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.stable_diffusion"
          },
          "default": {},
          "title": "Model",
          "description": "The model to use for image generation."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt for image generation."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to guide what should not appear in the generated image."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for the random number generator. Use -1 for a random seed.",
          "min": -1.0,
          "max": 4294967295.0
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 25,
          "title": "Num Inference Steps",
          "description": "Number of denoising steps.",
          "min": 1.0,
          "max": 100.0
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 7.5,
          "title": "Guidance Scale",
          "description": "Guidance scale for generation.",
          "min": 1.0,
          "max": 20.0
        },
        {
          "name": "scheduler",
          "type": {
            "type": "enum",
            "values": [
              "DPMSolverSDEScheduler",
              "EulerDiscreteScheduler",
              "LMSDiscreteScheduler",
              "DDIMScheduler",
              "DDPMScheduler",
              "HeunDiscreteScheduler",
              "DPMSolverMultistepScheduler",
              "DEISMultistepScheduler",
              "PNDMScheduler",
              "EulerAncestralDiscreteScheduler",
              "UniPCMultistepScheduler",
              "KDPM2DiscreteScheduler",
              "DPMSolverSinglestepScheduler",
              "KDPM2AncestralDiscreteScheduler"
            ],
            "type_name": "nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionScheduler"
          },
          "default": "EulerDiscreteScheduler",
          "title": "Scheduler",
          "description": "The scheduler to use for the diffusion process."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "hf.lora_sd_config"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRA models to use for image processing"
        },
        {
          "name": "ip_adapter_model",
          "type": {
            "type": "hf.ip_adapter"
          },
          "default": {},
          "title": "Ip Adapter Model",
          "description": "The IP adapter model to use for image processing"
        },
        {
          "name": "ip_adapter_image",
          "type": {
            "type": "image"
          },
          "default": {},
          "title": "Ip Adapter Image",
          "description": "When provided the image will be fed into the IP adapter"
        },
        {
          "name": "ip_adapter_scale",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Ip Adapter Scale",
          "description": "The strength of the IP adapter",
          "min": 0.0,
          "max": 1.0
        },
        {
          "name": "pag_scale",
          "type": {
            "type": "float"
          },
          "default": 3.0,
          "title": "Pag Scale",
          "description": "Scale of the Perturbed-Attention Guidance applied to the image.",
          "min": 0.0,
          "max": 10.0
        },
        {
          "name": "detail_level",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Detail Level",
          "description": "Level of detail for the hi-res pass. 0.0 is low detail, 1.0 is high detail.",
          "min": 0.0,
          "max": 1.0
        },
        {
          "name": "enable_tiling",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Tiling",
          "description": "Enable tiling for the VAE. This can reduce VRAM usage."
        },
        {
          "name": "enable_cpu_offload",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Cpu Offload",
          "description": "Enable CPU offload for the pipeline. This can reduce VRAM usage."
        },
        {
          "name": "upscaler",
          "type": {
            "type": "enum",
            "values": [
              "None",
              "Latent",
              "Bicubic"
            ],
            "type_name": "nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionUpscaler"
          },
          "default": "None",
          "title": "Upscaler",
          "description": "The upscaler to use for 2-pass generation."
        },
        {
          "name": "init_image",
          "type": {
            "type": "image"
          },
          "default": {},
          "title": "Init Image",
          "description": "The initial image to be inpainted."
        },
        {
          "name": "mask_image",
          "type": {
            "type": "image"
          },
          "default": {},
          "title": "Mask Image",
          "description": "The mask image indicating areas to be inpainted."
        },
        {
          "name": "strength",
          "type": {
            "type": "float"
          },
          "default": 0.8,
          "title": "Strength",
          "description": "Strength for inpainting. Higher values allow for more deviation from the original image.",
          "min": 0.0,
          "max": 1.0
        },
        {
          "name": "variant",
          "type": {
            "type": "enum",
            "values": [
              "default",
              "fp16",
              "fp32",
              "bf16"
            ],
            "type_name": "nodetool.nodes.huggingface.image_to_image.ModelVariant"
          },
          "default": "fp16",
          "title": "Variant",
          "description": "The variant of the model to use for Image-to-Image generation."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "recommended_models": [
        {
          "type": "hf.ip_adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "models/ip-adapter_sd15.bin"
        },
        {
          "type": "hf.ip_adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "models/ip-adapter_sd15_light.bin"
        },
        {
          "type": "hf.ip_adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "models/ip-adapter_sd15_vit-G.bin"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "SG161222/Realistic_Vision_V5.1_noVAE",
          "path": "Realistic_Vision_V5.1_fp16-no-ema.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "digiplay/majicMIX_realistic_v7",
          "path": "majicmixRealistic_v7.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "philz1337x/epicrealism",
          "path": "epicrealism_naturalSinRC1VAE.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "Lykon/DreamShaper",
          "path": "DreamShaper_5_beta2_noVae_half_pruned.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "Lykon/DreamShaper",
          "path": "DreamShaper_4BakedVae_fp16.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "XpucT/Deliberate",
          "path": "Deliberate_v6.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "XpucT/Deliberate",
          "path": "Deliberate_v6-inpainting.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "Lykon/AbsoluteReality",
          "path": "AbsoluteReality_1.8.1_pruned.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "Lykon/AbsoluteReality",
          "path": "AbsoluteReality_1.8.1_INPAINTING.inpainting.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "gsdf/Counterfeit-V2.5",
          "path": "Counterfeit-V2.5_fp16.safetensors"
        },
        {
          "type": "hf.ip_adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "models/ip-adapter_sd15.bin"
        },
        {
          "type": "hf.ip_adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "models/ip-adapter_sd15_light.bin"
        },
        {
          "type": "hf.ip_adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "models/ip-adapter_sd15_vit-G.bin"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "stabilityai/sd-x2-latent-upscaler",
          "allow_patterns": [
            "README.md",
            "**/*.safetensors",
            "**/*.json",
            "**/*.txt",
            "*.json"
          ]
        }
      ],
      "basic_fields": [
        "model",
        "prompt",
        "init_image",
        "mask_image",
        "strength"
      ]
    },
    {
      "title": "Stable Diffusion 4x Upscale",
      "description": "Upscales an image using Stable Diffusion 4x upscaler.\n    image, upscaling, stable-diffusion, SD\n\n    Use cases:\n    - Enhance low-resolution images\n    - Improve image quality for printing or display\n    - Create high-resolution versions of small images",
      "namespace": "huggingface.image_to_image",
      "node_type": "huggingface.image_to_image.StableDiffusionUpscale",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt for image generation."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to guide what should not appear in the generated image."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 25,
          "title": "Num Inference Steps",
          "description": "Number of upscaling steps.",
          "min": 1.0,
          "max": 100.0
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 7.5,
          "title": "Guidance Scale",
          "description": "Guidance scale for generation.",
          "min": 1.0,
          "max": 20.0
        },
        {
          "name": "image",
          "type": {
            "type": "image"
          },
          "default": {},
          "title": "Image",
          "description": "The initial image for Image-to-Image generation."
        },
        {
          "name": "scheduler",
          "type": {
            "type": "enum",
            "values": [
              "DPMSolverSDEScheduler",
              "EulerDiscreteScheduler",
              "LMSDiscreteScheduler",
              "DDIMScheduler",
              "DDPMScheduler",
              "HeunDiscreteScheduler",
              "DPMSolverMultistepScheduler",
              "DEISMultistepScheduler",
              "PNDMScheduler",
              "EulerAncestralDiscreteScheduler",
              "UniPCMultistepScheduler",
              "KDPM2DiscreteScheduler",
              "DPMSolverSinglestepScheduler",
              "KDPM2AncestralDiscreteScheduler"
            ],
            "type_name": "nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionScheduler"
          },
          "default": "HeunDiscreteScheduler",
          "title": "Scheduler",
          "description": "The scheduler to use for the diffusion process."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for the random number generator. Use -1 for a random seed.",
          "min": -1.0,
          "max": 4294967295.0
        },
        {
          "name": "enable_tiling",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Tiling",
          "description": "Enable tiling to save VRAM"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "recommended_models": [
        {
          "type": "hf.stable_diffusion_upscale",
          "repo_id": "stabilityai/stable-diffusion-x4-upscaler",
          "allow_patterns": [
            "README.md",
            "**/*.fp16.safetensors",
            "**/*.json",
            "**/*.txt",
            "*.json"
          ]
        }
      ],
      "basic_fields": [
        "prompt",
        "negative_prompt",
        "image"
      ]
    },
    {
      "title": "Stable Diffusion XL ControlNet",
      "description": "Transforms existing images using Stable Diffusion XL with ControlNet guidance.\n    image, generation, image-to-image, controlnet, SDXL\n\n    Use cases:\n    - Modify existing images with precise control over composition and structure\n    - Apply specific styles or concepts to photographs or artwork with guided transformations\n    - Create variations of existing visual content while maintaining certain features\n    - Enhance image editing capabilities with AI-guided transformations",
      "namespace": "huggingface.image_to_image",
      "node_type": "huggingface.image_to_image.StableDiffusionXLControlNet",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.stable_diffusion_xl"
          },
          "default": {},
          "title": "Model",
          "description": "The Stable Diffusion XL model to use for generation."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt for image generation."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to guide what should not appear in the generated image."
        },
        {
          "name": "width",
          "type": {
            "type": "int"
          },
          "default": 1024,
          "title": "Width",
          "description": "Width of the generated image.",
          "min": 64.0,
          "max": 2048.0
        },
        {
          "name": "height",
          "type": {
            "type": "int"
          },
          "default": 1024,
          "title": "Height",
          "description": "Height of the generated image",
          "min": 64.0,
          "max": 2048.0
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for the random number generator.",
          "min": -1.0,
          "max": 1000000.0
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 25,
          "title": "Num Inference Steps",
          "description": "Number of inference steps.",
          "min": 1.0,
          "max": 100.0
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 7.0,
          "title": "Guidance Scale",
          "description": "Guidance scale for generation.",
          "min": 0.0,
          "max": 20.0
        },
        {
          "name": "scheduler",
          "type": {
            "type": "enum",
            "values": [
              "DPMSolverSDEScheduler",
              "EulerDiscreteScheduler",
              "LMSDiscreteScheduler",
              "DDIMScheduler",
              "DDPMScheduler",
              "HeunDiscreteScheduler",
              "DPMSolverMultistepScheduler",
              "DEISMultistepScheduler",
              "PNDMScheduler",
              "EulerAncestralDiscreteScheduler",
              "UniPCMultistepScheduler",
              "KDPM2DiscreteScheduler",
              "DPMSolverSinglestepScheduler",
              "KDPM2AncestralDiscreteScheduler"
            ],
            "type_name": "nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionScheduler"
          },
          "default": "EulerDiscreteScheduler",
          "title": "Scheduler",
          "description": "The scheduler to use for the diffusion process."
        },
        {
          "name": "pag_scale",
          "type": {
            "type": "float"
          },
          "default": 3.0,
          "title": "Pag Scale",
          "description": "Scale of the Perturbed-Attention Guidance applied to the image.",
          "min": 0.0,
          "max": 10.0
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "hf.lora_sdxl_config"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRA models to use for image processing"
        },
        {
          "name": "lora_scale",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Lora Scale",
          "description": "Strength of the LoRAs",
          "min": 0.0,
          "max": 1.0
        },
        {
          "name": "ip_adapter_model",
          "type": {
            "type": "hf.ip_adapter"
          },
          "default": {},
          "title": "Ip Adapter Model",
          "description": "The IP adapter model to use for image processing"
        },
        {
          "name": "ip_adapter_image",
          "type": {
            "type": "image"
          },
          "default": {},
          "title": "Ip Adapter Image",
          "description": "When provided the image will be fed into the IP adapter"
        },
        {
          "name": "ip_adapter_scale",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Ip Adapter Scale",
          "description": "Strength of the IP adapter image",
          "min": 0.0,
          "max": 1.0
        },
        {
          "name": "enable_tiling",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Tiling",
          "description": "Enable tiling for the VAE. This can reduce VRAM usage."
        },
        {
          "name": "enable_cpu_offload",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Cpu Offload",
          "description": "Enable CPU offload for the pipeline. This can reduce VRAM usage."
        },
        {
          "name": "init_image",
          "type": {
            "type": "image"
          },
          "default": {},
          "title": "Init Image",
          "description": "The initial image for Image-to-Image generation."
        },
        {
          "name": "strength",
          "type": {
            "type": "float"
          },
          "default": 0.8,
          "title": "Strength",
          "description": "Strength for Image-to-Image generation. Higher values allow for more deviation from the original image.",
          "min": 0.0,
          "max": 1.0
        },
        {
          "name": "variant",
          "type": {
            "type": "enum",
            "values": [
              "default",
              "fp16",
              "fp32",
              "bf16"
            ],
            "type_name": "nodetool.nodes.huggingface.image_to_image.ModelVariant"
          },
          "default": "fp16",
          "title": "Variant",
          "description": "The variant of the model to use for Image-to-Image generation."
        },
        {
          "name": "controlnet",
          "type": {
            "type": "hf.controlnet"
          },
          "default": {},
          "title": "Controlnet",
          "description": "The ControlNet model to use for guidance."
        },
        {
          "name": "control_image",
          "type": {
            "type": "image"
          },
          "default": {},
          "title": "Control Image",
          "description": "The control image to guide the transformation."
        },
        {
          "name": "controlnet_conditioning_scale",
          "type": {
            "type": "float"
          },
          "default": 1.0,
          "title": "Controlnet Conditioning Scale",
          "description": "The scale for ControlNet conditioning.",
          "min": 0.0,
          "max": 2.0
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "recommended_models": [
        {
          "type": "hf.ip_adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "sdxl_models/ip-adapter_sdxl.bin"
        },
        {
          "type": "hf.ip_adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "sdxl_models/ip-adapter_sdxl_vit-h.bin"
        },
        {
          "type": "hf.ip_adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "sdxl_models/ip-adapter-plus_sdxl_vit-h.bin"
        },
        {
          "type": "hf.stable_diffusion_xl",
          "repo_id": "stabilityai/stable-diffusion-xl-base-1.0",
          "path": "sd_xl_base_1.0.safetensors"
        },
        {
          "type": "hf.stable_diffusion_xl",
          "repo_id": "stabilityai/stable-diffusion-xl-refiner-1.0",
          "path": "sd_xl_refiner_1.0.safetensors"
        },
        {
          "type": "hf.stable_diffusion_xl",
          "repo_id": "playgroundai/playground-v2.5-1024px-aesthetic",
          "path": "playground-v2.5-1024px-aesthetic.fp16.safetensors"
        },
        {
          "type": "hf.stable_diffusion_xl",
          "repo_id": "RunDiffusion/Juggernaut-XL-v9",
          "path": "Juggernaut-XL_v9_RunDiffusionPhoto_v2.safetensors"
        },
        {
          "type": "hf.stable_diffusion_xl",
          "repo_id": "dataautogpt3/ProteusV0.5",
          "path": "proteusV0.5.safetensors"
        },
        {
          "type": "hf.stable_diffusion_xl",
          "repo_id": "Lykon/dreamshaper-xl-lightning",
          "path": "DreamShaperXL_Lightning.safetensors"
        },
        {
          "type": "hf.stable_diffusion_xl",
          "repo_id": "Lykon/AAM_XL_AnimeMix",
          "path": "AAM_XL_Anime_Mix.safetensors"
        },
        {
          "type": "hf.stable_diffusion_xl",
          "repo_id": "stabilityai/sdxl-turbo",
          "path": "sd_xl_turbo_1.0_fp16.safetensors"
        },
        {
          "type": "hf.stable_diffusion_xl",
          "repo_id": "Lykon/dreamshaper-xl-v2-turbo",
          "path": "DreamShaperXL_Turbo_v2_1.safetensors"
        }
      ],
      "basic_fields": [
        "model",
        "prompt",
        "width",
        "height",
        "init_image",
        "strength",
        "controlnet",
        "control_image",
        "controlnet_conditioning_scale"
      ]
    },
    {
      "title": "Stable Diffusion XL (Img2Img)",
      "description": "Transforms existing images based on text prompts using Stable Diffusion XL.\n    image, generation, image-to-image, SDXL, style-transfer, ipadapter\n\n    Use cases:\n    - Modifying existing images to fit a specific style or theme\n    - Enhancing or altering photographs\n    - Creating variations of existing artwork\n    - Applying text-guided edits to images",
      "namespace": "huggingface.image_to_image",
      "node_type": "huggingface.image_to_image.StableDiffusionXLImg2Img",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.stable_diffusion_xl"
          },
          "default": {},
          "title": "Model",
          "description": "The Stable Diffusion XL model to use for generation."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt for image generation."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to guide what should not appear in the generated image."
        },
        {
          "name": "width",
          "type": {
            "type": "int"
          },
          "default": 1024,
          "title": "Width",
          "description": "Width of the generated image.",
          "min": 64.0,
          "max": 2048.0
        },
        {
          "name": "height",
          "type": {
            "type": "int"
          },
          "default": 1024,
          "title": "Height",
          "description": "Height of the generated image",
          "min": 64.0,
          "max": 2048.0
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for the random number generator.",
          "min": -1.0,
          "max": 1000000.0
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 25,
          "title": "Num Inference Steps",
          "description": "Number of inference steps.",
          "min": 1.0,
          "max": 100.0
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 7.0,
          "title": "Guidance Scale",
          "description": "Guidance scale for generation.",
          "min": 0.0,
          "max": 20.0
        },
        {
          "name": "scheduler",
          "type": {
            "type": "enum",
            "values": [
              "DPMSolverSDEScheduler",
              "EulerDiscreteScheduler",
              "LMSDiscreteScheduler",
              "DDIMScheduler",
              "DDPMScheduler",
              "HeunDiscreteScheduler",
              "DPMSolverMultistepScheduler",
              "DEISMultistepScheduler",
              "PNDMScheduler",
              "EulerAncestralDiscreteScheduler",
              "UniPCMultistepScheduler",
              "KDPM2DiscreteScheduler",
              "DPMSolverSinglestepScheduler",
              "KDPM2AncestralDiscreteScheduler"
            ],
            "type_name": "nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionScheduler"
          },
          "default": "EulerDiscreteScheduler",
          "title": "Scheduler",
          "description": "The scheduler to use for the diffusion process."
        },
        {
          "name": "pag_scale",
          "type": {
            "type": "float"
          },
          "default": 3.0,
          "title": "Pag Scale",
          "description": "Scale of the Perturbed-Attention Guidance applied to the image.",
          "min": 0.0,
          "max": 10.0
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "hf.lora_sdxl_config"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRA models to use for image processing"
        },
        {
          "name": "lora_scale",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Lora Scale",
          "description": "Strength of the LoRAs",
          "min": 0.0,
          "max": 1.0
        },
        {
          "name": "ip_adapter_model",
          "type": {
            "type": "hf.ip_adapter"
          },
          "default": {},
          "title": "Ip Adapter Model",
          "description": "The IP adapter model to use for image processing"
        },
        {
          "name": "ip_adapter_image",
          "type": {
            "type": "image"
          },
          "default": {},
          "title": "Ip Adapter Image",
          "description": "When provided the image will be fed into the IP adapter"
        },
        {
          "name": "ip_adapter_scale",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Ip Adapter Scale",
          "description": "Strength of the IP adapter image",
          "min": 0.0,
          "max": 1.0
        },
        {
          "name": "enable_tiling",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Tiling",
          "description": "Enable tiling for the VAE. This can reduce VRAM usage."
        },
        {
          "name": "enable_cpu_offload",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Cpu Offload",
          "description": "Enable CPU offload for the pipeline. This can reduce VRAM usage."
        },
        {
          "name": "init_image",
          "type": {
            "type": "image"
          },
          "default": {},
          "title": "Init Image",
          "description": "The initial image for Image-to-Image generation."
        },
        {
          "name": "strength",
          "type": {
            "type": "float"
          },
          "default": 0.8,
          "title": "Strength",
          "description": "Strength for Image-to-Image generation. Higher values allow for more deviation from the original image.",
          "min": 0.0,
          "max": 1.0
        },
        {
          "name": "variant",
          "type": {
            "type": "enum",
            "values": [
              "default",
              "fp16",
              "fp32",
              "bf16"
            ],
            "type_name": "nodetool.nodes.huggingface.image_to_image.ModelVariant"
          },
          "default": "fp16",
          "title": "Variant",
          "description": "The variant of the model to use for Image-to-Image generation."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "recommended_models": [
        {
          "type": "hf.ip_adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "sdxl_models/ip-adapter_sdxl.bin"
        },
        {
          "type": "hf.ip_adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "sdxl_models/ip-adapter_sdxl_vit-h.bin"
        },
        {
          "type": "hf.ip_adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "sdxl_models/ip-adapter-plus_sdxl_vit-h.bin"
        },
        {
          "type": "hf.stable_diffusion_xl",
          "repo_id": "stabilityai/stable-diffusion-xl-base-1.0",
          "path": "sd_xl_base_1.0.safetensors"
        },
        {
          "type": "hf.stable_diffusion_xl",
          "repo_id": "stabilityai/stable-diffusion-xl-refiner-1.0",
          "path": "sd_xl_refiner_1.0.safetensors"
        },
        {
          "type": "hf.stable_diffusion_xl",
          "repo_id": "playgroundai/playground-v2.5-1024px-aesthetic",
          "path": "playground-v2.5-1024px-aesthetic.fp16.safetensors"
        },
        {
          "type": "hf.stable_diffusion_xl",
          "repo_id": "RunDiffusion/Juggernaut-XL-v9",
          "path": "Juggernaut-XL_v9_RunDiffusionPhoto_v2.safetensors"
        },
        {
          "type": "hf.stable_diffusion_xl",
          "repo_id": "dataautogpt3/ProteusV0.5",
          "path": "proteusV0.5.safetensors"
        },
        {
          "type": "hf.stable_diffusion_xl",
          "repo_id": "Lykon/dreamshaper-xl-lightning",
          "path": "DreamShaperXL_Lightning.safetensors"
        },
        {
          "type": "hf.stable_diffusion_xl",
          "repo_id": "Lykon/AAM_XL_AnimeMix",
          "path": "AAM_XL_Anime_Mix.safetensors"
        },
        {
          "type": "hf.stable_diffusion_xl",
          "repo_id": "stabilityai/sdxl-turbo",
          "path": "sd_xl_turbo_1.0_fp16.safetensors"
        },
        {
          "type": "hf.stable_diffusion_xl",
          "repo_id": "Lykon/dreamshaper-xl-v2-turbo",
          "path": "DreamShaperXL_Turbo_v2_1.safetensors"
        }
      ],
      "basic_fields": [
        "model",
        "prompt",
        "width",
        "height",
        "init_image",
        "strength"
      ]
    },
    {
      "title": "Stable Diffusion XL (Inpaint)",
      "description": "Performs inpainting on images using Stable Diffusion XL.\n    image, inpainting, SDXL\n\n    Use cases:\n    - Remove unwanted objects from images\n    - Fill in missing parts of images\n    - Modify specific areas of images while preserving the rest",
      "namespace": "huggingface.image_to_image",
      "node_type": "huggingface.image_to_image.StableDiffusionXLInpainting",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.stable_diffusion_xl"
          },
          "default": {},
          "title": "Model",
          "description": "The Stable Diffusion XL model to use for generation."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt for image generation."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to guide what should not appear in the generated image."
        },
        {
          "name": "width",
          "type": {
            "type": "int"
          },
          "default": 1024,
          "title": "Width",
          "description": "Width of the generated image.",
          "min": 64.0,
          "max": 2048.0
        },
        {
          "name": "height",
          "type": {
            "type": "int"
          },
          "default": 1024,
          "title": "Height",
          "description": "Height of the generated image",
          "min": 64.0,
          "max": 2048.0
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for the random number generator.",
          "min": -1.0,
          "max": 1000000.0
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 25,
          "title": "Num Inference Steps",
          "description": "Number of inference steps.",
          "min": 1.0,
          "max": 100.0
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 7.0,
          "title": "Guidance Scale",
          "description": "Guidance scale for generation.",
          "min": 0.0,
          "max": 20.0
        },
        {
          "name": "scheduler",
          "type": {
            "type": "enum",
            "values": [
              "DPMSolverSDEScheduler",
              "EulerDiscreteScheduler",
              "LMSDiscreteScheduler",
              "DDIMScheduler",
              "DDPMScheduler",
              "HeunDiscreteScheduler",
              "DPMSolverMultistepScheduler",
              "DEISMultistepScheduler",
              "PNDMScheduler",
              "EulerAncestralDiscreteScheduler",
              "UniPCMultistepScheduler",
              "KDPM2DiscreteScheduler",
              "DPMSolverSinglestepScheduler",
              "KDPM2AncestralDiscreteScheduler"
            ],
            "type_name": "nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionScheduler"
          },
          "default": "EulerDiscreteScheduler",
          "title": "Scheduler",
          "description": "The scheduler to use for the diffusion process."
        },
        {
          "name": "pag_scale",
          "type": {
            "type": "float"
          },
          "default": 3.0,
          "title": "Pag Scale",
          "description": "Scale of the Perturbed-Attention Guidance applied to the image.",
          "min": 0.0,
          "max": 10.0
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "hf.lora_sdxl_config"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRA models to use for image processing"
        },
        {
          "name": "lora_scale",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Lora Scale",
          "description": "Strength of the LoRAs",
          "min": 0.0,
          "max": 1.0
        },
        {
          "name": "ip_adapter_model",
          "type": {
            "type": "hf.ip_adapter"
          },
          "default": {},
          "title": "Ip Adapter Model",
          "description": "The IP adapter model to use for image processing"
        },
        {
          "name": "ip_adapter_image",
          "type": {
            "type": "image"
          },
          "default": {},
          "title": "Ip Adapter Image",
          "description": "When provided the image will be fed into the IP adapter"
        },
        {
          "name": "ip_adapter_scale",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Ip Adapter Scale",
          "description": "Strength of the IP adapter image",
          "min": 0.0,
          "max": 1.0
        },
        {
          "name": "enable_tiling",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Tiling",
          "description": "Enable tiling for the VAE. This can reduce VRAM usage."
        },
        {
          "name": "enable_cpu_offload",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Cpu Offload",
          "description": "Enable CPU offload for the pipeline. This can reduce VRAM usage."
        },
        {
          "name": "image",
          "type": {
            "type": "image"
          },
          "default": {},
          "title": "Image",
          "description": "The initial image to be inpainted."
        },
        {
          "name": "mask_image",
          "type": {
            "type": "image"
          },
          "default": {},
          "title": "Mask Image",
          "description": "The mask image indicating areas to be inpainted."
        },
        {
          "name": "strength",
          "type": {
            "type": "float"
          },
          "default": 0.8,
          "title": "Strength",
          "description": "Strength for inpainting. Higher values allow for more deviation from the original image.",
          "min": 0.0,
          "max": 1.0
        },
        {
          "name": "variant",
          "type": {
            "type": "enum",
            "values": [
              "default",
              "fp16",
              "fp32",
              "bf16"
            ],
            "type_name": "nodetool.nodes.huggingface.image_to_image.ModelVariant"
          },
          "default": "fp16",
          "title": "Variant",
          "description": "The variant of the model to use for Image-to-Image generation."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "recommended_models": [
        {
          "type": "hf.ip_adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "sdxl_models/ip-adapter_sdxl.bin"
        },
        {
          "type": "hf.ip_adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "sdxl_models/ip-adapter_sdxl_vit-h.bin"
        },
        {
          "type": "hf.ip_adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "sdxl_models/ip-adapter-plus_sdxl_vit-h.bin"
        },
        {
          "type": "hf.stable_diffusion_xl",
          "repo_id": "stabilityai/stable-diffusion-xl-base-1.0",
          "path": "sd_xl_base_1.0.safetensors"
        },
        {
          "type": "hf.stable_diffusion_xl",
          "repo_id": "stabilityai/stable-diffusion-xl-refiner-1.0",
          "path": "sd_xl_refiner_1.0.safetensors"
        },
        {
          "type": "hf.stable_diffusion_xl",
          "repo_id": "playgroundai/playground-v2.5-1024px-aesthetic",
          "path": "playground-v2.5-1024px-aesthetic.fp16.safetensors"
        },
        {
          "type": "hf.stable_diffusion_xl",
          "repo_id": "RunDiffusion/Juggernaut-XL-v9",
          "path": "Juggernaut-XL_v9_RunDiffusionPhoto_v2.safetensors"
        },
        {
          "type": "hf.stable_diffusion_xl",
          "repo_id": "dataautogpt3/ProteusV0.5",
          "path": "proteusV0.5.safetensors"
        },
        {
          "type": "hf.stable_diffusion_xl",
          "repo_id": "Lykon/dreamshaper-xl-lightning",
          "path": "DreamShaperXL_Lightning.safetensors"
        },
        {
          "type": "hf.stable_diffusion_xl",
          "repo_id": "Lykon/AAM_XL_AnimeMix",
          "path": "AAM_XL_Anime_Mix.safetensors"
        },
        {
          "type": "hf.stable_diffusion_xl",
          "repo_id": "stabilityai/sdxl-turbo",
          "path": "sd_xl_turbo_1.0_fp16.safetensors"
        },
        {
          "type": "hf.stable_diffusion_xl",
          "repo_id": "Lykon/dreamshaper-xl-v2-turbo",
          "path": "DreamShaperXL_Turbo_v2_1.safetensors"
        }
      ],
      "basic_fields": [
        "model",
        "prompt",
        "width",
        "height",
        "image",
        "mask_image",
        "strength"
      ]
    },
    {
      "title": "Swin2SR",
      "description": "Performs image super-resolution using the Swin2SR model.\n    image, super-resolution, enhancement, huggingface\n\n    Use cases:\n    - Enhance low-resolution images\n    - Improve image quality for printing or display\n    - Upscale images for better detail",
      "namespace": "huggingface.image_to_image",
      "node_type": "huggingface.image_to_image.Swin2SR",
      "properties": [
        {
          "name": "image",
          "type": {
            "type": "image"
          },
          "default": {},
          "title": "Input Image",
          "description": "The input image to transform"
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt to guide the image transformation (if applicable)"
        },
        {
          "name": "model",
          "type": {
            "type": "hf.image_to_image"
          },
          "default": {},
          "title": "Model ID on Huggingface",
          "description": "The model ID to use for image super-resolution"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "recommended_models": [
        {
          "type": "hf.image_to_image",
          "repo_id": "caidas/swin2SR-classical-sr-x2-64",
          "allow_patterns": [
            "README.md",
            "*.safetensors",
            "*.json",
            "**/*.json"
          ]
        },
        {
          "type": "hf.image_to_image",
          "repo_id": "caidas/swin2SR-classical-sr-x4-48",
          "allow_patterns": [
            "README.md",
            "*.safetensors",
            "*.json",
            "**/*.json"
          ]
        },
        {
          "type": "hf.image_to_image",
          "repo_id": "caidas/swin2SR-lightweight-sr-x2-64",
          "allow_patterns": [
            "README.md",
            "*.bin",
            "*.json",
            "**/*.json"
          ]
        },
        {
          "type": "hf.image_to_image",
          "repo_id": "caidas/swin2SR-realworld-sr-x4-64-bsrgan-psnr",
          "allow_patterns": [
            "README.md",
            "*.bin",
            "*.json",
            "**/*.json"
          ]
        }
      ],
      "basic_fields": [
        "image",
        "prompt",
        "model"
      ]
    },
    {
      "title": "Feature Extraction",
      "description": "Extracts features from text using pre-trained models.\n    text, feature extraction, embeddings, natural language processing\n\n    Use cases:\n    - Text similarity comparison\n    - Clustering text documents\n    - Input for machine learning models\n    - Semantic search applications",
      "namespace": "huggingface.feature_extraction",
      "node_type": "huggingface.feature_extraction.FeatureExtraction",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.feature_extraction"
          },
          "default": {},
          "title": "Model ID on Huggingface",
          "description": "The model ID to use for feature extraction"
        },
        {
          "name": "inputs",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Input Text",
          "description": "The text to extract features from"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "np_array"
          },
          "name": "output"
        }
      ],
      "recommended_models": [
        {
          "type": "hf.feature_extraction",
          "repo_id": "mixedbread-ai/mxbai-embed-large-v1",
          "allow_patterns": [
            "*.safetensors",
            "*.txt",
            "*,json"
          ]
        },
        {
          "type": "hf.feature_extraction",
          "repo_id": "BAAI/bge-base-en-v1.5",
          "allow_patterns": [
            "*.safetensors",
            "*.txt",
            "*,json"
          ]
        },
        {
          "type": "hf.feature_extraction",
          "repo_id": "BAAI/bge-large-en-v1.5",
          "allow_patterns": [
            "*.safetensors",
            "*.txt",
            "*,json"
          ]
        }
      ],
      "basic_fields": [
        "model",
        "inputs"
      ]
    },
    {
      "title": "Text Generation",
      "description": "Generates text based on a given prompt.\n    text, generation, natural language processing\n\n    Use cases:\n    - Creative writing assistance\n    - Automated content generation\n    - Chatbots and conversational AI\n    - Code generation and completion",
      "namespace": "huggingface.text_generation",
      "node_type": "huggingface.text_generation.TextGeneration",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.text_generation"
          },
          "default": {},
          "title": "Model ID on Huggingface",
          "description": "The model ID to use for the text generation"
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The input text prompt for generation"
        },
        {
          "name": "max_new_tokens",
          "type": {
            "type": "int"
          },
          "default": 50,
          "title": "Max New Tokens",
          "description": "The maximum number of new tokens to generate"
        },
        {
          "name": "temperature",
          "type": {
            "type": "float"
          },
          "default": 1.0,
          "title": "Temperature",
          "description": "Controls randomness in generation. Lower values make it more deterministic.",
          "min": 0.0,
          "max": 2.0
        },
        {
          "name": "top_p",
          "type": {
            "type": "float"
          },
          "default": 1.0,
          "title": "Top P",
          "description": "Controls diversity of generated text. Lower values make it more focused.",
          "min": 0.0,
          "max": 1.0
        },
        {
          "name": "do_sample",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Do Sample",
          "description": "Whether to use sampling or greedy decoding"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "str"
          },
          "name": "output"
        }
      ],
      "recommended_models": [
        {
          "type": "hf.text_generation",
          "repo_id": "gpt2",
          "allow_patterns": [
            "*.json",
            "*.txt",
            "*.safetensors"
          ]
        },
        {
          "type": "hf.text_generation",
          "repo_id": "distilgpt2",
          "allow_patterns": [
            "*.json",
            "*.txt",
            "*.safetensors"
          ]
        },
        {
          "type": "hf.text_generation",
          "repo_id": "Qwen/Qwen2-0.5B-Instruct",
          "allow_patterns": [
            "*.json",
            "*.txt",
            "*.safetensors"
          ]
        },
        {
          "type": "hf.text_generation",
          "repo_id": "bigcode/starcoder",
          "allow_patterns": [
            "*.json",
            "*.txt",
            "*.safetensors"
          ]
        }
      ],
      "basic_fields": [
        "model",
        "prompt"
      ]
    },
    {
      "title": "Chroma",
      "description": "Generates images from text prompts using Chroma, a text-to-image model based on Flux.\n    image, generation, AI, text-to-image, flux, chroma, transformer\n\n    Use cases:\n    - Generate high-quality images with Flux-based architecture\n    - Create images with advanced attention masking for enhanced fidelity\n    - Produce images with IP adapter support for style control\n    - Generate images with optimized memory usage\n    - Create professional-quality images with precise color control",
      "namespace": "huggingface.text_to_image",
      "node_type": "huggingface.text_to_image.Chroma",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "A high-fashion close-up portrait of a blonde woman in clear sunglasses. The image uses a bold teal and red color split for dramatic lighting. The background is a simple teal-green. The photo is sharp and well-composed, and is designed for viewing with anaglyph 3D glasses for optimal effect. It looks professionally done.",
          "title": "Prompt",
          "description": "A text prompt describing the desired image."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "low quality, ugly, unfinished, out of focus, deformed, disfigure, blurry, smudged, restricted palette, flat colors",
          "title": "Negative Prompt",
          "description": "A text prompt describing what to avoid in the image."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.0,
          "title": "Guidance Scale",
          "description": "The scale for classifier-free guidance.",
          "min": 0.0,
          "max": 30.0
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 40,
          "title": "Num Inference Steps",
          "description": "The number of denoising steps.",
          "min": 1.0,
          "max": 100.0
        },
        {
          "name": "height",
          "type": {
            "type": "int"
          },
          "default": 1024,
          "title": "Height",
          "description": "The height of the generated image.",
          "min": 256.0,
          "max": 2048.0
        },
        {
          "name": "width",
          "type": {
            "type": "int"
          },
          "default": 1024,
          "title": "Width",
          "description": "The width of the generated image.",
          "min": 256.0,
          "max": 2048.0
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for the random number generator. Use -1 for a random seed.",
          "min": -1.0
        },
        {
          "name": "max_sequence_length",
          "type": {
            "type": "int"
          },
          "default": 512,
          "title": "Max Sequence Length",
          "description": "Maximum sequence length to use with the prompt.",
          "min": 1.0,
          "max": 512.0
        },
        {
          "name": "ip_adapter_image",
          "type": {
            "type": "union",
            "type_args": [
              {
                "type": "image"
              },
              {
                "type": "none"
              }
            ]
          },
          "title": "Ip Adapter Image",
          "description": "Optional image input for IP Adapter style control."
        },
        {
          "name": "enable_cpu_offload",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Cpu Offload",
          "description": "Enable CPU offload to reduce VRAM usage."
        },
        {
          "name": "enable_vae_slicing",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Vae Slicing",
          "description": "Enable VAE slicing to reduce VRAM usage."
        },
        {
          "name": "enable_vae_tiling",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Vae Tiling",
          "description": "Enable VAE tiling to reduce VRAM usage for large images."
        },
        {
          "name": "enable_attention_slicing",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Attention Slicing",
          "description": "Enable attention slicing to reduce memory usage."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "recommended_models": [
        {
          "repo_id": "lodestones/Chroma",
          "allow_patterns": [
            "**/*.safetensors",
            "**/*.json",
            "**/*.txt",
            "*.json"
          ]
        }
      ],
      "basic_fields": [
        "prompt",
        "negative_prompt",
        "height",
        "width"
      ]
    },
    {
      "title": "FLUX Text2Image",
      "description": "Generates images using FLUX models with quantization support for memory efficiency.\n    image, generation, AI, text-to-image, flux, quantization\n\n    Use cases:\n    - High-quality image generation with FLUX models\n    - Memory-efficient generation using quantization\n    - Fast generation with FLUX.1-schnell\n    - High-fidelity generation with FLUX.1-dev\n    - Controlled generation with Fill, Canny, or Depth variants",
      "namespace": "huggingface.text_to_image",
      "node_type": "huggingface.text_to_image.FluxText2Image",
      "properties": [
        {
          "name": "variant",
          "type": {
            "type": "enum",
            "values": [
              "schnell",
              "dev",
              "fill-dev",
              "canny-dev",
              "depth-dev"
            ],
            "type_name": "nodetool.nodes.huggingface.text_to_image.FluxVariant"
          },
          "default": "schnell",
          "title": "Variant",
          "description": "The FLUX model variant to use."
        },
        {
          "name": "quantization",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "bitsandbytes-8bit",
              "bitsandbytes-4bit"
            ],
            "type_name": "nodetool.nodes.huggingface.text_to_image.QuantizationMethod"
          },
          "default": "none",
          "title": "Quantization",
          "description": "Quantization method to reduce memory usage."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "A cat holding a sign that says hello world",
          "title": "Prompt",
          "description": "A text prompt describing the desired image."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 0.0,
          "title": "Guidance Scale",
          "description": "The scale for classifier-free guidance. Use 0.0 for schnell, 3.5 for dev.",
          "min": 0.0,
          "max": 30.0
        },
        {
          "name": "width",
          "type": {
            "type": "int"
          },
          "default": 1360,
          "title": "Width",
          "description": "The width of the generated image.",
          "min": 64.0,
          "max": 2048.0
        },
        {
          "name": "height",
          "type": {
            "type": "int"
          },
          "default": 768,
          "title": "Height",
          "description": "The height of the generated image.",
          "min": 64.0,
          "max": 2048.0
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 4,
          "title": "Num Inference Steps",
          "description": "The number of denoising steps. Use 4 for schnell, 50 for dev.",
          "min": 1.0,
          "max": 100.0
        },
        {
          "name": "max_sequence_length",
          "type": {
            "type": "int"
          },
          "default": 512,
          "title": "Max Sequence Length",
          "description": "Maximum sequence length for the prompt. Use 256 for schnell, 512 for dev.",
          "min": 1.0,
          "max": 512.0
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for the random number generator. Use -1 for a random seed.",
          "min": -1.0
        },
        {
          "name": "enable_memory_efficient_attention",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Memory Efficient Attention",
          "description": "Enable memory efficient attention to reduce VRAM usage."
        },
        {
          "name": "enable_vae_tiling",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Vae Tiling",
          "description": "Enable VAE tiling to reduce VRAM usage for large images."
        },
        {
          "name": "enable_vae_slicing",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Vae Slicing",
          "description": "Enable VAE slicing to reduce VRAM usage."
        },
        {
          "name": "enable_cpu_offload",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Cpu Offload",
          "description": "Enable CPU offload to reduce VRAM usage."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "recommended_models": [
        {
          "repo_id": "black-forest-labs/FLUX.1-schnell",
          "allow_patterns": [
            "**/*.safetensors",
            "**/*.json",
            "**/*.txt",
            "*.json"
          ]
        },
        {
          "repo_id": "black-forest-labs/FLUX.1-dev",
          "allow_patterns": [
            "**/*.safetensors",
            "**/*.json",
            "**/*.txt",
            "*.json"
          ]
        },
        {
          "repo_id": "black-forest-labs/FLUX.1-Fill-dev",
          "allow_patterns": [
            "**/*.safetensors",
            "**/*.json",
            "**/*.txt",
            "*.json"
          ]
        },
        {
          "repo_id": "black-forest-labs/FLUX.1-Canny-dev",
          "allow_patterns": [
            "**/*.safetensors",
            "**/*.json",
            "**/*.txt",
            "*.json"
          ]
        },
        {
          "repo_id": "black-forest-labs/FLUX.1-Depth-dev",
          "allow_patterns": [
            "**/*.safetensors",
            "**/*.json",
            "**/*.txt",
            "*.json"
          ]
        }
      ],
      "basic_fields": [
        "variant",
        "quantization",
        "prompt",
        "guidance_scale",
        "width",
        "height",
        "num_inference_steps",
        "max_sequence_length",
        "seed",
        "enable_memory_efficient_attention",
        "enable_vae_tiling",
        "enable_vae_slicing",
        "enable_cpu_offload"
      ]
    },
    {
      "title": "Hunyuan-DiT",
      "description": "Generates images from text prompts using Hunyuan-DiT, a powerful multi-resolution diffusion transformer.\n    image, generation, AI, text-to-image, chinese, english, diffusion, transformer\n\n    Use cases:\n    - Generate high-quality images from Chinese and English text descriptions\n    - Create images with fine-grained language understanding\n    - Produce multi-resolution images with optimal aspect ratios\n    - Generate images with both Chinese and English text support\n    - Create detailed images with strong semantic accuracy",
      "namespace": "huggingface.text_to_image",
      "node_type": "huggingface.text_to_image.HunyuanDiT",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "\u4e00\u4e2a\u5b87\u822a\u5458\u5728\u9a91\u9a6c",
          "title": "Prompt",
          "description": "A text prompt describing the desired image. Supports both Chinese and English."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "A text prompt describing what to avoid in the image."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 7.5,
          "title": "Guidance Scale",
          "description": "The scale for classifier-free guidance.",
          "min": 1.0,
          "max": 20.0
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 50,
          "title": "Num Inference Steps",
          "description": "The number of denoising steps.",
          "min": 1.0,
          "max": 100.0
        },
        {
          "name": "width",
          "type": {
            "type": "int"
          },
          "default": 1024,
          "title": "Width",
          "description": "The width of the generated image.",
          "min": 512.0,
          "max": 2048.0
        },
        {
          "name": "height",
          "type": {
            "type": "int"
          },
          "default": 1024,
          "title": "Height",
          "description": "The height of the generated image.",
          "min": 512.0,
          "max": 2048.0
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for the random number generator. Use -1 for a random seed.",
          "min": -1.0
        },
        {
          "name": "use_resolution_binning",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Use Resolution Binning",
          "description": "Whether to use resolution binning. Maps input resolution to closest standard resolution."
        },
        {
          "name": "original_size",
          "type": {
            "type": "tuple",
            "type_args": [
              {
                "type": "int"
              },
              {
                "type": "int"
              }
            ]
          },
          "default": [
            1024,
            1024
          ],
          "title": "Original Size",
          "description": "The original size of the image used to calculate time IDs."
        },
        {
          "name": "target_size",
          "type": {
            "type": "union",
            "type_args": [
              {
                "type": "tuple",
                "type_args": [
                  {
                    "type": "int"
                  },
                  {
                    "type": "int"
                  }
                ]
              },
              {
                "type": "none"
              }
            ]
          },
          "title": "Target Size",
          "description": "The target size of the image used to calculate time IDs. If None, uses (width, height)."
        },
        {
          "name": "crops_coords_top_left",
          "type": {
            "type": "tuple",
            "type_args": [
              {
                "type": "int"
              },
              {
                "type": "int"
              }
            ]
          },
          "default": [
            0,
            0
          ],
          "title": "Crops Coords Top Left",
          "description": "The top-left coordinates of the crop used to calculate time IDs."
        },
        {
          "name": "guidance_rescale",
          "type": {
            "type": "float"
          },
          "default": 0.0,
          "title": "Guidance Rescale",
          "description": "Rescale the noise according to guidance_rescale.",
          "min": 0.0,
          "max": 1.0
        },
        {
          "name": "enable_memory_optimization",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Memory Optimization",
          "description": "Enable memory optimization with T5 encoder quantization."
        },
        {
          "name": "enable_forward_chunking",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Forward Chunking",
          "description": "Enable forward chunking to reduce memory usage at the cost of inference speed."
        },
        {
          "name": "forward_chunk_size",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Forward Chunk Size",
          "description": "Chunk size for forward chunking.",
          "min": 1.0,
          "max": 4.0
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "recommended_models": [
        {
          "repo_id": "Tencent-Hunyuan/HunyuanDiT-Diffusers",
          "allow_patterns": [
            "**/*.safetensors",
            "**/*.json",
            "**/*.txt",
            "*.json"
          ]
        }
      ],
      "basic_fields": [
        "prompt",
        "width",
        "height",
        "use_resolution_binning"
      ]
    },
    {
      "title": "Kolors Text2Image",
      "description": "Generates images from text prompts using Kolors, a large-scale text-to-image generation model.\n    image, generation, AI, text-to-image, kolors, chinese, english\n\n    Use cases:\n    - Generate high-quality photorealistic images from text descriptions\n    - Create images with Chinese text understanding and rendering\n    - Produce images with complex semantic accuracy\n    - Generate images with both Chinese and English text support\n    - Create detailed images with strong text rendering capabilities",
      "namespace": "huggingface.text_to_image",
      "node_type": "huggingface.text_to_image.Kolors",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "A ladybug photo, macro, zoom, high quality, film, holding a sign that says \"\u53ef\u56fe\"",
          "title": "Prompt",
          "description": "A text prompt describing the desired image. Supports both Chinese and English."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "A text prompt describing what to avoid in the image."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 6.5,
          "title": "Guidance Scale",
          "description": "The scale for classifier-free guidance.",
          "min": 1.0,
          "max": 20.0
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 25,
          "title": "Num Inference Steps",
          "description": "The number of denoising steps.",
          "min": 1.0,
          "max": 100.0
        },
        {
          "name": "width",
          "type": {
            "type": "int"
          },
          "default": 1024,
          "title": "Width",
          "description": "The width of the generated image.",
          "min": 64.0,
          "max": 2048.0
        },
        {
          "name": "height",
          "type": {
            "type": "int"
          },
          "default": 1024,
          "title": "Height",
          "description": "The height of the generated image.",
          "min": 64.0,
          "max": 2048.0
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for the random number generator. Use -1 for a random seed.",
          "min": -1.0
        },
        {
          "name": "max_sequence_length",
          "type": {
            "type": "int"
          },
          "default": 256,
          "title": "Max Sequence Length",
          "description": "Maximum sequence length for the prompt.",
          "min": 1.0,
          "max": 512.0
        },
        {
          "name": "use_dpm_solver",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Use Dpm Solver",
          "description": "Whether to use DPMSolverMultistepScheduler with Karras sigmas for better quality."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "recommended_models": [
        {
          "repo_id": "Kwai-Kolors/Kolors-diffusers",
          "allow_patterns": [
            "**/*.safetensors",
            "**/*.json",
            "**/*.txt",
            "*.json"
          ]
        }
      ],
      "basic_fields": [
        "prompt",
        "negative_prompt",
        "guidance_scale",
        "num_inference_steps",
        "width",
        "height",
        "seed",
        "max_sequence_length",
        "use_dpm_solver"
      ]
    },
    {
      "title": "Load Text To Image Model",
      "description": "Load HuggingFace model for image-to-image generation from a repo_id.\n\n    Use cases:\n    - Loads a pipeline directly from a repo_id\n    - Used for AutoPipelineForImage2Image",
      "namespace": "huggingface.text_to_image",
      "node_type": "huggingface.text_to_image.LoadTextToImageModel",
      "properties": [
        {
          "name": "repo_id",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Repo Id",
          "description": "The repository ID of the model to use for image-to-image generation."
        },
        {
          "name": "variant",
          "type": {
            "type": "enum",
            "values": [
              "fp16",
              "fp32",
              "bf16",
              "default"
            ],
            "type_name": "nodetool.nodes.huggingface.text_to_image.ModelVariant"
          },
          "default": "default",
          "title": "Variant",
          "description": "The variant of the model to use for text-to-image generation."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "hf.text_to_image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "repo_id",
        "variant"
      ]
    },
    {
      "title": "Lumina-T2X",
      "description": "Generates images from text prompts using Lumina-T2X, a Flow-based Large Diffusion Transformer.\n    image, generation, AI, text-to-image, diffusion, transformer, flow, quantization\n\n    Use cases:\n    - Generate high-quality images with improved sampling efficiency\n    - Create images with Next-DiT architecture and 3D RoPE\n    - Produce images with better resolution extrapolation capabilities\n    - Generate images with multilingual support using decoder-based LLMs\n    - Create images with advanced frequency and time-aware scaling",
      "namespace": "huggingface.text_to_image",
      "node_type": "huggingface.text_to_image.LuminaT2X",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "Upper body of a young woman in a Victorian-era outfit with brass goggles and leather straps. Background shows an industrial revolution cityscape with smoky skies and tall, metal structures",
          "title": "Prompt",
          "description": "A text prompt describing the desired image."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "A text prompt describing what to avoid in the image. For Lumina-T2X, this should typically be empty."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 4.0,
          "title": "Guidance Scale",
          "description": "The scale for classifier-free guidance.",
          "min": 1.0,
          "max": 20.0
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Num Inference Steps",
          "description": "The number of denoising steps. Lumina-T2X uses fewer steps for efficient generation.",
          "min": 1.0,
          "max": 100.0
        },
        {
          "name": "height",
          "type": {
            "type": "int"
          },
          "default": 1024,
          "title": "Height",
          "description": "The height of the generated image.",
          "min": 256.0,
          "max": 2048.0
        },
        {
          "name": "width",
          "type": {
            "type": "int"
          },
          "default": 1024,
          "title": "Width",
          "description": "The width of the generated image.",
          "min": 256.0,
          "max": 2048.0
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for the random number generator. Use -1 for a random seed.",
          "min": -1.0
        },
        {
          "name": "clean_caption",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Clean Caption",
          "description": "Whether to clean the caption before creating embeddings. Requires beautifulsoup4 and ftfy."
        },
        {
          "name": "max_sequence_length",
          "type": {
            "type": "int"
          },
          "default": 256,
          "title": "Max Sequence Length",
          "description": "Maximum sequence length to use with the prompt.",
          "min": 1.0,
          "max": 512.0
        },
        {
          "name": "scaling_watershed",
          "type": {
            "type": "float"
          },
          "default": 1.0,
          "title": "Scaling Watershed",
          "description": "Scaling watershed parameter for improved generation quality.",
          "min": 0.1,
          "max": 2.0
        },
        {
          "name": "proportional_attn",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Proportional Attn",
          "description": "Whether to use proportional attention for better quality."
        },
        {
          "name": "enable_quantization",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Quantization",
          "description": "Enable quantization for memory efficiency."
        },
        {
          "name": "enable_cpu_offload",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Cpu Offload",
          "description": "Enable CPU offload to reduce VRAM usage."
        },
        {
          "name": "enable_vae_slicing",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Vae Slicing",
          "description": "Enable VAE slicing to reduce VRAM usage."
        },
        {
          "name": "enable_vae_tiling",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Vae Tiling",
          "description": "Enable VAE tiling to reduce VRAM usage for large images."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "recommended_models": [
        {
          "repo_id": "Alpha-VLLM/Lumina-Next-SFT-diffusers",
          "allow_patterns": [
            "**/*.safetensors",
            "**/*.json",
            "**/*.txt",
            "*.json"
          ]
        }
      ],
      "basic_fields": [
        "prompt",
        "height",
        "width",
        "num_inference_steps"
      ]
    },
    {
      "title": "Quanto FLUX FP8",
      "description": "Generates images using FLUX models with Optimum Quanto FP8 quantization for extreme memory efficiency.\n    image, generation, AI, text-to-image, flux, quantization, fp8, quanto\n\n    Use cases:\n    - Ultra memory-efficient FLUX image generation using FP8 quantization\n    - High-quality image generation on lower-end hardware\n    - Faster inference with reduced memory footprint\n    - Professional image generation with optimized resource usage",
      "namespace": "huggingface.text_to_image",
      "node_type": "huggingface.text_to_image.QuantoFlux",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "A cat holding a sign that says hello world",
          "title": "Prompt",
          "description": "A text prompt describing the desired image."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "The scale for classifier-free guidance.",
          "min": 0.0,
          "max": 30.0
        },
        {
          "name": "width",
          "type": {
            "type": "int"
          },
          "default": 1024,
          "title": "Width",
          "description": "The width of the generated image.",
          "min": 64.0,
          "max": 2048.0
        },
        {
          "name": "height",
          "type": {
            "type": "int"
          },
          "default": 1024,
          "title": "Height",
          "description": "The height of the generated image.",
          "min": 64.0,
          "max": 2048.0
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 20,
          "title": "Num Inference Steps",
          "description": "The number of denoising steps.",
          "min": 1.0,
          "max": 100.0
        },
        {
          "name": "max_sequence_length",
          "type": {
            "type": "int"
          },
          "default": 512,
          "title": "Max Sequence Length",
          "description": "Maximum sequence length for the prompt.",
          "min": 1.0,
          "max": 512.0
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for the random number generator. Use -1 for a random seed.",
          "min": -1.0
        },
        {
          "name": "enable_cpu_offload",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Cpu Offload",
          "description": "Enable CPU offload to reduce VRAM usage."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "recommended_models": [
        {
          "repo_id": "Kijai/flux-fp8",
          "path": "flux1-dev-fp8.safetensors"
        }
      ],
      "basic_fields": [
        "prompt",
        "guidance_scale",
        "width",
        "height",
        "num_inference_steps",
        "max_sequence_length",
        "seed",
        "enable_cpu_offload"
      ]
    },
    {
      "title": "Stable Diffusion",
      "description": "Generates images from text prompts using Stable Diffusion.\n    image, generation, AI, text-to-image, SD\n\n    Use cases:\n    - Creating custom illustrations for various projects\n    - Generating concept art for creative endeavors\n    - Producing unique visual content for marketing materials\n    - Exploring AI-generated art for personal or professional use",
      "namespace": "huggingface.text_to_image",
      "node_type": "huggingface.text_to_image.StableDiffusion",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.stable_diffusion"
          },
          "default": {},
          "title": "Model",
          "description": "The model to use for image generation."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt for image generation."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to guide what should not appear in the generated image."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for the random number generator. Use -1 for a random seed.",
          "min": -1.0,
          "max": 4294967295.0
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 25,
          "title": "Num Inference Steps",
          "description": "Number of denoising steps.",
          "min": 1.0,
          "max": 100.0
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 7.5,
          "title": "Guidance Scale",
          "description": "Guidance scale for generation.",
          "min": 1.0,
          "max": 20.0
        },
        {
          "name": "scheduler",
          "type": {
            "type": "enum",
            "values": [
              "DPMSolverSDEScheduler",
              "EulerDiscreteScheduler",
              "LMSDiscreteScheduler",
              "DDIMScheduler",
              "DDPMScheduler",
              "HeunDiscreteScheduler",
              "DPMSolverMultistepScheduler",
              "DEISMultistepScheduler",
              "PNDMScheduler",
              "EulerAncestralDiscreteScheduler",
              "UniPCMultistepScheduler",
              "KDPM2DiscreteScheduler",
              "DPMSolverSinglestepScheduler",
              "KDPM2AncestralDiscreteScheduler"
            ],
            "type_name": "nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionScheduler"
          },
          "default": "EulerDiscreteScheduler",
          "title": "Scheduler",
          "description": "The scheduler to use for the diffusion process."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "hf.lora_sd_config"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRA models to use for image processing"
        },
        {
          "name": "ip_adapter_model",
          "type": {
            "type": "hf.ip_adapter"
          },
          "default": {},
          "title": "Ip Adapter Model",
          "description": "The IP adapter model to use for image processing"
        },
        {
          "name": "ip_adapter_image",
          "type": {
            "type": "image"
          },
          "default": {},
          "title": "Ip Adapter Image",
          "description": "When provided the image will be fed into the IP adapter"
        },
        {
          "name": "ip_adapter_scale",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Ip Adapter Scale",
          "description": "The strength of the IP adapter",
          "min": 0.0,
          "max": 1.0
        },
        {
          "name": "pag_scale",
          "type": {
            "type": "float"
          },
          "default": 3.0,
          "title": "Pag Scale",
          "description": "Scale of the Perturbed-Attention Guidance applied to the image.",
          "min": 0.0,
          "max": 10.0
        },
        {
          "name": "detail_level",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Detail Level",
          "description": "Level of detail for the hi-res pass. 0.0 is low detail, 1.0 is high detail.",
          "min": 0.0,
          "max": 1.0
        },
        {
          "name": "enable_tiling",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Tiling",
          "description": "Enable tiling for the VAE. This can reduce VRAM usage."
        },
        {
          "name": "enable_cpu_offload",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Cpu Offload",
          "description": "Enable CPU offload for the pipeline. This can reduce VRAM usage."
        },
        {
          "name": "upscaler",
          "type": {
            "type": "enum",
            "values": [
              "None",
              "Latent",
              "Bicubic"
            ],
            "type_name": "nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionUpscaler"
          },
          "default": "None",
          "title": "Upscaler",
          "description": "The upscaler to use for 2-pass generation."
        },
        {
          "name": "width",
          "type": {
            "type": "int"
          },
          "default": 512,
          "title": "Width",
          "description": "Width of the generated image.",
          "min": 256.0,
          "max": 1024.0
        },
        {
          "name": "height",
          "type": {
            "type": "int"
          },
          "default": 512,
          "title": "Height",
          "description": "Height of the generated image",
          "min": 256.0,
          "max": 1024.0
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "recommended_models": [
        {
          "type": "hf.ip_adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "models/ip-adapter_sd15.bin"
        },
        {
          "type": "hf.ip_adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "models/ip-adapter_sd15_light.bin"
        },
        {
          "type": "hf.ip_adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "models/ip-adapter_sd15_vit-G.bin"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "SG161222/Realistic_Vision_V5.1_noVAE",
          "path": "Realistic_Vision_V5.1_fp16-no-ema.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "digiplay/majicMIX_realistic_v7",
          "path": "majicmixRealistic_v7.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "philz1337x/epicrealism",
          "path": "epicrealism_naturalSinRC1VAE.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "Lykon/DreamShaper",
          "path": "DreamShaper_5_beta2_noVae_half_pruned.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "Lykon/DreamShaper",
          "path": "DreamShaper_4BakedVae_fp16.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "XpucT/Deliberate",
          "path": "Deliberate_v6.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "XpucT/Deliberate",
          "path": "Deliberate_v6-inpainting.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "Lykon/AbsoluteReality",
          "path": "AbsoluteReality_1.8.1_pruned.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "Lykon/AbsoluteReality",
          "path": "AbsoluteReality_1.8.1_INPAINTING.inpainting.safetensors"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "gsdf/Counterfeit-V2.5",
          "path": "Counterfeit-V2.5_fp16.safetensors"
        },
        {
          "type": "hf.ip_adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "models/ip-adapter_sd15.bin"
        },
        {
          "type": "hf.ip_adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "models/ip-adapter_sd15_light.bin"
        },
        {
          "type": "hf.ip_adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "models/ip-adapter_sd15_vit-G.bin"
        },
        {
          "type": "hf.stable_diffusion",
          "repo_id": "stabilityai/sd-x2-latent-upscaler",
          "allow_patterns": [
            "README.md",
            "**/*.safetensors",
            "**/*.json",
            "**/*.txt",
            "*.json"
          ]
        }
      ],
      "basic_fields": [
        "model",
        "prompt",
        "width",
        "height"
      ]
    },
    {
      "title": "Stable Diffusion XL",
      "description": "Generates images from text prompts using Stable Diffusion XL.\n    image, generation, AI, text-to-image, SDXL\n\n    Use cases:\n    - Creating custom illustrations for marketing materials\n    - Generating concept art for game and film development\n    - Producing unique stock imagery for websites and publications\n    - Visualizing interior design concepts for clients",
      "namespace": "huggingface.text_to_image",
      "node_type": "huggingface.text_to_image.StableDiffusionXL",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.stable_diffusion_xl"
          },
          "default": {},
          "title": "Model",
          "description": "The Stable Diffusion XL model to use for generation."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt for image generation."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to guide what should not appear in the generated image."
        },
        {
          "name": "width",
          "type": {
            "type": "int"
          },
          "default": 1024,
          "title": "Width",
          "description": "Width of the generated image.",
          "min": 64.0,
          "max": 2048.0
        },
        {
          "name": "height",
          "type": {
            "type": "int"
          },
          "default": 1024,
          "title": "Height",
          "description": "Height of the generated image",
          "min": 64.0,
          "max": 2048.0
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for the random number generator.",
          "min": -1.0,
          "max": 1000000.0
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 25,
          "title": "Num Inference Steps",
          "description": "Number of inference steps.",
          "min": 1.0,
          "max": 100.0
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 7.0,
          "title": "Guidance Scale",
          "description": "Guidance scale for generation.",
          "min": 0.0,
          "max": 20.0
        },
        {
          "name": "scheduler",
          "type": {
            "type": "enum",
            "values": [
              "DPMSolverSDEScheduler",
              "EulerDiscreteScheduler",
              "LMSDiscreteScheduler",
              "DDIMScheduler",
              "DDPMScheduler",
              "HeunDiscreteScheduler",
              "DPMSolverMultistepScheduler",
              "DEISMultistepScheduler",
              "PNDMScheduler",
              "EulerAncestralDiscreteScheduler",
              "UniPCMultistepScheduler",
              "KDPM2DiscreteScheduler",
              "DPMSolverSinglestepScheduler",
              "KDPM2AncestralDiscreteScheduler"
            ],
            "type_name": "nodetool.nodes.huggingface.stable_diffusion_base.StableDiffusionScheduler"
          },
          "default": "EulerDiscreteScheduler",
          "title": "Scheduler",
          "description": "The scheduler to use for the diffusion process."
        },
        {
          "name": "pag_scale",
          "type": {
            "type": "float"
          },
          "default": 3.0,
          "title": "Pag Scale",
          "description": "Scale of the Perturbed-Attention Guidance applied to the image.",
          "min": 0.0,
          "max": 10.0
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "hf.lora_sdxl_config"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRA models to use for image processing"
        },
        {
          "name": "lora_scale",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Lora Scale",
          "description": "Strength of the LoRAs",
          "min": 0.0,
          "max": 1.0
        },
        {
          "name": "ip_adapter_model",
          "type": {
            "type": "hf.ip_adapter"
          },
          "default": {},
          "title": "Ip Adapter Model",
          "description": "The IP adapter model to use for image processing"
        },
        {
          "name": "ip_adapter_image",
          "type": {
            "type": "image"
          },
          "default": {},
          "title": "Ip Adapter Image",
          "description": "When provided the image will be fed into the IP adapter"
        },
        {
          "name": "ip_adapter_scale",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Ip Adapter Scale",
          "description": "Strength of the IP adapter image",
          "min": 0.0,
          "max": 1.0
        },
        {
          "name": "enable_tiling",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Tiling",
          "description": "Enable tiling for the VAE. This can reduce VRAM usage."
        },
        {
          "name": "enable_cpu_offload",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Cpu Offload",
          "description": "Enable CPU offload for the pipeline. This can reduce VRAM usage."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "recommended_models": [
        {
          "type": "hf.ip_adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "sdxl_models/ip-adapter_sdxl.bin"
        },
        {
          "type": "hf.ip_adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "sdxl_models/ip-adapter_sdxl_vit-h.bin"
        },
        {
          "type": "hf.ip_adapter",
          "repo_id": "h94/IP-Adapter",
          "path": "sdxl_models/ip-adapter-plus_sdxl_vit-h.bin"
        },
        {
          "type": "hf.stable_diffusion_xl",
          "repo_id": "stabilityai/stable-diffusion-xl-base-1.0",
          "path": "sd_xl_base_1.0.safetensors"
        },
        {
          "type": "hf.stable_diffusion_xl",
          "repo_id": "stabilityai/stable-diffusion-xl-refiner-1.0",
          "path": "sd_xl_refiner_1.0.safetensors"
        },
        {
          "type": "hf.stable_diffusion_xl",
          "repo_id": "playgroundai/playground-v2.5-1024px-aesthetic",
          "path": "playground-v2.5-1024px-aesthetic.fp16.safetensors"
        },
        {
          "type": "hf.stable_diffusion_xl",
          "repo_id": "RunDiffusion/Juggernaut-XL-v9",
          "path": "Juggernaut-XL_v9_RunDiffusionPhoto_v2.safetensors"
        },
        {
          "type": "hf.stable_diffusion_xl",
          "repo_id": "dataautogpt3/ProteusV0.5",
          "path": "proteusV0.5.safetensors"
        },
        {
          "type": "hf.stable_diffusion_xl",
          "repo_id": "Lykon/dreamshaper-xl-lightning",
          "path": "DreamShaperXL_Lightning.safetensors"
        },
        {
          "type": "hf.stable_diffusion_xl",
          "repo_id": "Lykon/AAM_XL_AnimeMix",
          "path": "AAM_XL_Anime_Mix.safetensors"
        },
        {
          "type": "hf.stable_diffusion_xl",
          "repo_id": "stabilityai/sdxl-turbo",
          "path": "sd_xl_turbo_1.0_fp16.safetensors"
        },
        {
          "type": "hf.stable_diffusion_xl",
          "repo_id": "Lykon/dreamshaper-xl-v2-turbo",
          "path": "DreamShaperXL_Turbo_v2_1.safetensors"
        }
      ],
      "basic_fields": [
        "model",
        "prompt",
        "width",
        "height",
        "width",
        "height"
      ]
    },
    {
      "title": "Text to Image",
      "description": "Generates images from text prompts using AutoPipeline for automatic pipeline selection.\n    image, generation, AI, text-to-image, auto\n\n    Use cases:\n    - Automatic selection of the best pipeline for a given model\n    - Flexible image generation without pipeline-specific knowledge\n    - Quick prototyping with various text-to-image models\n    - Streamlined workflow for different model architectures",
      "namespace": "huggingface.text_to_image",
      "node_type": "huggingface.text_to_image.Text2Image",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "hf.text_to_image"
          },
          "default": {},
          "title": "Model",
          "description": "The model to use for text-to-image generation."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "A cat holding a sign that says hello world",
          "title": "Prompt",
          "description": "A text prompt describing the desired image."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "A text prompt describing what to avoid in the image."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 50,
          "title": "Num Inference Steps",
          "description": "The number of denoising steps.",
          "min": 1.0,
          "max": 100.0
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 7.5,
          "title": "Guidance Scale",
          "description": "The scale for classifier-free guidance.",
          "min": 1.0,
          "max": 20.0
        },
        {
          "name": "width",
          "type": {
            "type": "int"
          },
          "default": 512,
          "title": "Width",
          "description": "The width of the generated image.",
          "min": 64.0,
          "max": 2048.0
        },
        {
          "name": "height",
          "type": {
            "type": "int"
          },
          "default": 512,
          "title": "Height",
          "description": "The height of the generated image.",
          "min": 64.0,
          "max": 2048.0
        },
        {
          "name": "pag_scale",
          "type": {
            "type": "float"
          },
          "default": 3.0,
          "title": "Pag Scale",
          "description": "Scale of the Perturbed-Attention Guidance applied to the image.",
          "min": 0.0,
          "max": 10.0
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for the random number generator. Use -1 for a random seed.",
          "min": -1.0
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "model",
        "prompt",
        "negative_prompt",
        "num_inference_steps",
        "guidance_scale",
        "width",
        "height",
        "pag_scale",
        "seed"
      ]
    }
  ],
  "assets": [
    {
      "package_name": "nodetool-huggingface",
      "name": "Image to Image.jpg",
      "path": ""
    },
    {
      "package_name": "nodetool-huggingface",
      "name": "Stable Diffusion.jpg",
      "path": ""
    },
    {
      "package_name": "nodetool-huggingface",
      "name": "Image Enhance.jpg",
      "path": ""
    },
    {
      "package_name": "nodetool-huggingface",
      "name": "Segmentation.jpg",
      "path": ""
    },
    {
      "package_name": "nodetool-huggingface",
      "name": "Pokemon Maker.jpg",
      "path": ""
    },
    {
      "package_name": "nodetool-huggingface",
      "name": "stable_diffusion_xl.jpg",
      "path": ""
    },
    {
      "package_name": "nodetool-huggingface",
      "name": "Audio To Image.jpg",
      "path": ""
    },
    {
      "package_name": "nodetool-huggingface",
      "name": "Re-Imagine.jpg",
      "path": ""
    },
    {
      "package_name": "nodetool-huggingface",
      "name": "Audio To Spectrogram.jpg",
      "path": ""
    },
    {
      "package_name": "nodetool-huggingface",
      "name": "Object Detection.jpg",
      "path": ""
    },
    {
      "package_name": "nodetool-huggingface",
      "name": "Add Subtitles To Video.jpg",
      "path": ""
    },
    {
      "package_name": "nodetool-huggingface",
      "name": "Upscaling.jpg",
      "path": ""
    },
    {
      "package_name": "nodetool-huggingface",
      "name": "Depth Estimation.jpg",
      "path": ""
    },
    {
      "package_name": "nodetool-huggingface",
      "name": "Movie Posters.jpg",
      "path": ""
    },
    {
      "package_name": "nodetool-huggingface",
      "name": "Piano Track.jpg",
      "path": ""
    },
    {
      "package_name": "nodetool-huggingface",
      "name": "Controlnet.jpg",
      "path": ""
    },
    {
      "package_name": "nodetool-huggingface",
      "name": "Summarize Audio.jpg",
      "path": ""
    },
    {
      "package_name": "nodetool-huggingface",
      "name": "Transcribe Audio.jpg",
      "path": ""
    },
    {
      "package_name": "nodetool-huggingface",
      "name": "Style Transfer.jpg",
      "path": ""
    }
  ],
  "examples": [
    {
      "id": "dfff77a8f38911ef919400004a056799",
      "name": "Upscaling",
      "description": "Upscale low-resolution images to higher quality using RealESRGAN, a powerful AI model that enhances details and clarity without artifacts.",
      "tags": [
        "image",
        "start",
        "huggingface"
      ]
    },
    {
      "id": "style_transfer",
      "name": "Style Transfer",
      "description": "Transform your images by applying artistic styles from reference images. This workflow uses IP-Adapter to transfer visual styles while ControlNet preserves the original structure. Perfect for creating artistic variations of portraits or other images.",
      "tags": [
        "huggingface",
        "image",
        "start"
      ]
    },
    {
      "id": "01ddcf16e35711ef80af000038478aae",
      "name": "Audio To Image",
      "description": "Transform spoken descriptions into images with this workflow. Record or upload audio, which is transcribed by Whisper and then visualized by Stable Diffusion. Perfect for quickly generating images from verbal ideas without typing.",
      "tags": [
        "huggingface",
        "multimodal",
        "start"
      ]
    },
    {
      "id": "controlnet",
      "name": "Controlnet",
      "description": "",
      "tags": [
        "start",
        "image"
      ]
    },
    {
      "id": "6e96807232a211f0a8870000194fbf00",
      "name": "Movie Posters",
      "description": "Create cinematic movie posters using AI image generation",
      "tags": [
        "start",
        "image",
        "huggingface"
      ]
    },
    {
      "id": "3dc7a22e12f311f0a84600004c6eb2d5",
      "name": "Audio To Spectrogram",
      "description": "Create a spectrogram from an audio file and use creative upscaling to transform it into wall-worthy art.",
      "tags": [
        "audio",
        "multimodal",
        "start",
        "huggingface"
      ]
    },
    {
      "id": "d6d4ffd859da11f094f9000001d6cfc2",
      "name": "Image to Image",
      "description": "",
      "tags": [
        "start",
        "image"
      ]
    },
    {
      "id": "transcribe_audio",
      "name": "Transcribe Audio",
      "description": "Convert speech to text using Whisper model with word-level timestamps",
      "tags": [
        "start",
        "audio",
        "huggingface"
      ]
    },
    {
      "id": "object_detection",
      "name": "Object Detection",
      "description": "Detect objects in an image and visualize the detections",
      "tags": [
        "huggingface",
        "start"
      ]
    },
    {
      "id": "depth_estimation",
      "name": "Depth Estimation",
      "description": "Estimate the depth of an image",
      "tags": [
        "image",
        "huggingface"
      ]
    },
    {
      "id": "add_subtitles_to_video",
      "name": "Add Subtitles To Video",
      "description": "This workflow automatically transcribes speech in videos and adds subtitles. It extracts audio from the input video, uses OpenAI's Whisper model to generate word-level timestamps and transcriptions, and then renders the subtitles back onto the original video. Perfect for creating accessible content, adding captions to social media videos, or transcribing presentations.",
      "tags": [
        "start",
        "video",
        "huggingface"
      ]
    },
    {
      "id": "segmentation",
      "name": "Segmentation",
      "description": "Segment images and visualize the segments",
      "tags": [
        "huggingface",
        "image"
      ]
    },
    {
      "id": "f1d42e6a12fb11f0901100001aeb0d2f",
      "name": "Summarize Audio",
      "description": "Transcribe an audio file and summarize the text.",
      "tags": [
        "audio",
        "start",
        "huggingface"
      ]
    },
    {
      "id": "001b40e05a6d11f0aea400001cbe54c8",
      "name": "Re-Imagine",
      "description": "",
      "tags": [
        "start",
        "image"
      ]
    },
    {
      "id": "8675bdaa388a11f0951800006f96a7c6",
      "name": "Pokemon Maker",
      "description": "",
      "tags": []
    }
  ]
}