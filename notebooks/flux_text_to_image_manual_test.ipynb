{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Flux Text-to-Image Manual Test\n",
        "\n",
        "This notebook mirrors the logic in `nodetool-huggingface/src/nodetool/nodes/huggingface/text_to_image.py` so you can interactively validate FLUX pipelines (standard or GGUF-quantized). Run all cells after activating the `nodetool` conda environment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup Checklist\n",
        "\n",
        "1. `conda activate nodetool`\n",
        "2. `pip install -U torch diffusers transformers accelerate huggingface_hub` if the deps are missing.\n",
        "3. `huggingface-cli login` (or set `HF_TOKEN`) before downloading gated models.\n",
        "4. If you plan to test GGUF quantized weights, download them first via `huggingface-cli download <repo> <file>`.\n",
        "5. Restart the kernel after changing low-level libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import platform\n",
        "import torch\n",
        "import diffusers\n",
        "import huggingface_hub\n",
        "\n",
        "def _default_device() -> str:\n",
        "    if torch.cuda.is_available():\n",
        "        return \"cuda\"\n",
        "    if getattr(torch.backends, 'mps', None) and torch.backends.mps.is_available():\n",
        "        return \"mps\"\n",
        "    return \"cpu\"\n",
        "\n",
        "device = _default_device()\n",
        "print(f'Python: {platform.python_version()}')\n",
        "print(f'Torch: {torch.__version__}')\n",
        "print(f'Diffusers: {diffusers.__version__}')\n",
        "print(f'Hugging Face Hub: {huggingface_hub.__version__}')\n",
        "print(f'Active device: {device}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configure A Test Run\n",
        "\n",
        "Adjust the dataclass below to match the FLUX variant you want to exercise. Set `use_gguf=True` plus `gguf_repo_id` and `gguf_filename` to load quantized weights; otherwise the notebook pulls the full diffusion weights from Hugging Face."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dataclasses import dataclass, field\n",
        "from enum import Enum\n",
        "from typing import Optional\n",
        "\n",
        "class FluxVariant(Enum):\n",
        "    SCHNELL = 'schnell'\n",
        "    DEV = 'dev'\n",
        "    FILL_DEV = 'fill-dev'\n",
        "    CANNY_DEV = 'canny-dev'\n",
        "    DEPTH_DEV = 'depth-dev'\n",
        "\n",
        "@dataclass\n",
        "class FluxConfig:\n",
        "    prompt: str = \"A cozy living room lit by warm neon strips, rendered in a cinematic style\"\n",
        "    guidance_scale: float = 3.5\n",
        "    num_inference_steps: int = 20\n",
        "    width: int = 1024\n",
        "    height: int = 1024\n",
        "    max_sequence_length: int = 512\n",
        "    seed: int = 0\n",
        "    model_repo_id: Optional[str] = 'black-forest-labs/FLUX.1-dev'\n",
        "    model_path: Optional[str] = None\n",
        "    use_gguf: bool = False\n",
        "    gguf_repo_id: Optional[str] = None\n",
        "    gguf_filename: Optional[str] = None\n",
        "    custom_gguf_path: Optional[str] = None\n",
        "    enable_cpu_offload: bool = False\n",
        "    enable_vae_tiling: bool = False\n",
        "    enable_vae_slicing: bool = False\n",
        "    output_path: Optional[str] = 'flux_test.png'\n",
        "    device: str = field(default_factory=lambda: 'cuda' if torch.cuda.is_available() else ('mps' if getattr(torch.backends, 'mps', None) and torch.backends.mps.is_available() else 'cpu'))\n",
        "\n",
        "cfg = FluxConfig()\n",
        "cfg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load The Pipeline\n",
        "\n",
        "The helpers below reproduce the variant detection, GGUF handling, and memory optimizations from the production node."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "from huggingface_hub.file_download import try_to_load_from_cache\n",
        "from diffusers import FluxPipeline\n",
        "from diffusers.models.transformers.transformer_flux import FluxTransformer2DModel\n",
        "from diffusers.quantizers.quantization_config import GGUFQuantizationConfig\n",
        "\n",
        "def detect_variant(model_repo_id: Optional[str], model_hint: Optional[str]) -> FluxVariant:\n",
        "    target = f\"{model_repo_id or ''} {model_hint or ''}\".lower()\n",
        "    if 'schnell' in target:\n",
        "        return FluxVariant.SCHNELL\n",
        "    if 'fill' in target:\n",
        "        return FluxVariant.FILL_DEV\n",
        "    if 'canny' in target:\n",
        "        return FluxVariant.CANNY_DEV\n",
        "    if 'depth' in target:\n",
        "        return FluxVariant.DEPTH_DEV\n",
        "    if 'dev' in target:\n",
        "        return FluxVariant.DEV\n",
        "    return FluxVariant.DEV\n",
        "\n",
        "def _is_gguf_path(path: Optional[str]) -> bool:\n",
        "    return bool(path and path.lower().endswith('.gguf'))\n",
        "\n",
        "def _resolve_dtype(variant: FluxVariant):\n",
        "    return torch.bfloat16 if variant in {FluxVariant.SCHNELL, FluxVariant.DEV} else torch.float16\n",
        "\n",
        "def load_flux_pipeline(cfg: FluxConfig):\n",
        "    variant = detect_variant(cfg.model_repo_id, cfg.model_path or cfg.gguf_filename or cfg.custom_gguf_path)\n",
        "    torch_dtype = _resolve_dtype(variant)\n",
        "    if cfg.use_gguf or _is_gguf_path(cfg.model_path):\n",
        "        gguf_path = cfg.custom_gguf_path\n",
        "        if gguf_path is None:\n",
        "            if not cfg.gguf_repo_id or not cfg.gguf_filename:\n",
        "                raise ValueError('Set `gguf_repo_id` and `gguf_filename` (or `custom_gguf_path`) to test GGUF weights.')\n",
        "            gguf_path = try_to_load_from_cache(cfg.gguf_repo_id, cfg.gguf_filename)\n",
        "            if gguf_path is None:\n",
        "                gguf_path = hf_hub_download(repo_id=cfg.gguf_repo_id, filename=cfg.gguf_filename)\n",
        "        transformer = FluxTransformer2DModel.from_single_file(\n",
        "            gguf_path,\n",
        "            quantization_config=GGUFQuantizationConfig(compute_dtype=torch_dtype),\n",
        "            torch_dtype=torch_dtype,\n",
        "        )\n",
        "        base_model_id = cfg.model_repo_id or ('black-forest-labs/FLUX.1-schnell' if variant == FluxVariant.SCHNELL else 'black-forest-labs/FLUX.1-dev')\n",
        "        pipeline = FluxPipeline.from_pretrained(base_model_id, transformer=transformer, torch_dtype=torch_dtype)\n",
        "    else:\n",
        "        if not cfg.model_repo_id:\n",
        "            raise ValueError('Configure `model_repo_id` when `use_gguf` is False.')\n",
        "        pipeline = FluxPipeline.from_pretrained(cfg.model_repo_id, torch_dtype=torch_dtype)\n",
        "    if cfg.enable_cpu_offload:\n",
        "        pipeline.enable_sequential_cpu_offload()\n",
        "    else:\n",
        "        pipeline.to(cfg.device)\n",
        "        if cfg.enable_vae_slicing:\n",
        "            pipeline.enable_vae_slicing()\n",
        "        if cfg.enable_vae_tiling:\n",
        "            pipeline.enable_vae_tiling()\n",
        "    return pipeline, variant\n",
        "\n",
        "flux_pipeline, selected_variant = load_flux_pipeline(cfg)\n",
        "selected_variant"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate An Image\n",
        "\n",
        "This cell mirrors the production sampling logic, including variant-specific overrides and the same callback signature used for streaming progress updates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "from time import perf_counter\n",
        "\n",
        "def _derive_sampling_params(cfg: FluxConfig, variant: FluxVariant):\n",
        "    guidance = cfg.guidance_scale\n",
        "    steps = cfg.num_inference_steps\n",
        "    max_seq = cfg.max_sequence_length\n",
        "    if variant == FluxVariant.SCHNELL:\n",
        "        guidance = 0.0\n",
        "        steps = 4\n",
        "        max_seq = min(256, cfg.max_sequence_length)\n",
        "    return guidance, steps, max_seq\n",
        "\n",
        "guidance_scale, num_steps, max_seq_len = _derive_sampling_params(cfg, selected_variant)\n",
        "generator = None if cfg.seed < 0 else torch.Generator(device='cpu').manual_seed(cfg.seed)\n",
        "\n",
        "def progress_callback(_, step: int, timestep: int, callback_kwargs: dict):\n",
        "    if step == 0 or step == num_steps - 1 or step % max(1, num_steps // 5) == 0:\n",
        "        print(f'[Flux] step {step + 1}/{num_steps} | timestep {timestep}')\n",
        "    return callback_kwargs\n",
        "\n",
        "start = perf_counter()\n",
        "try:\n",
        "    flux_output = flux_pipeline(\n",
        "        prompt=cfg.prompt,\n",
        "        guidance_scale=guidance_scale,\n",
        "        height=cfg.height,\n",
        "        width=cfg.width,\n",
        "        num_inference_steps=num_steps,\n",
        "        max_sequence_length=max_seq_len,\n",
        "        generator=generator,\n",
        "        callback_on_step_end=progress_callback,\n",
        "        callback_on_step_end_tensor_inputs=['latents'],\n",
        "    )\n",
        "except torch.OutOfMemoryError as exc:\n",
        "    raise RuntimeError('VRAM out of memory while running Flux. Reduce the resolution/steps or enable cfg.enable_cpu_offload.') from exc\n",
        "finally:\n",
        "    print(f'Elapsed: {perf_counter() - start:.2f}s')\n",
        "\n",
        "flux_image = flux_output.images[0]\n",
        "flux_image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preview & Persist\n",
        "\n",
        "Display the generated image inline and optionally save it for quick comparisons."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "\n",
        "display(flux_image)\n",
        "if cfg.output_path:\n",
        "    output_path = Path(cfg.output_path)\n",
        "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    flux_image.save(output_path)\n",
        "    print(f'Saved image to {output_path.resolve()}')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
